{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af5b471-cea9-4194-bc23-da5cdc2f9c67",
   "metadata": {},
   "source": [
    "Cell 1 — Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64aaa897-ab61-4eb4-a558-4c078acf751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Base path set to:\n",
      "C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\n",
      "[INFO] Found 18 folders:\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Selected test subject (LOSO): Mina 1\n",
      "[INFO] Training subjects (17):\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Sampling rate (SFREQ) = 128.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 1: Setup & discovery\n",
    "# =========================\n",
    "\n",
    "import os  # file ops\n",
    "import glob  # file matching\n",
    "import numpy as np  # arrays\n",
    "import random  # seeds\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data path\n",
    "BASE_PATH = r\"C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\"\n",
    "print(f\"[INFO] Base path set to:\\n{BASE_PATH}\")\n",
    "\n",
    "# List subject folders\n",
    "all_folders = [p for p in sorted(os.listdir(BASE_PATH)) if os.path.isdir(os.path.join(BASE_PATH, p))]\n",
    "print(f\"[INFO] Found {len(all_folders)} folders:\\n{all_folders}\")\n",
    "\n",
    "# Expected count (change if needed)\n",
    "assert len(all_folders) == 18, f\"Expected 18 folders, found {len(all_folders)}.\"\n",
    "\n",
    "# LOSO test subject\n",
    "TEST_SUBJECT = \"Mina 1\"\n",
    "print(f\"[INFO] Selected test subject (LOSO): {TEST_SUBJECT}\")\n",
    "\n",
    "# Train subjects = remaining\n",
    "train_subjects = [s for s in all_folders if s.lower() != TEST_SUBJECT.lower()]\n",
    "print(f\"[INFO] Training subjects ({len(train_subjects)}):\\n{train_subjects}\")\n",
    "\n",
    "# Sampling rate\n",
    "SFREQ = 128.0\n",
    "print(f\"[INFO] Sampling rate (SFREQ) = {SFREQ} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e649681-2861-4ce2-bbd1-0d9dd3359f42",
   "metadata": {},
   "source": [
    "Cell 2 — Load EEG + labels per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943c5c6e-a7aa-4618-899c-daf341c25358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 2] Loading data from folders...\n",
      "[STEP 2]       Amin | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Amin | EEG=          Amin.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Amin1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]      Amin1 | EEG=         Amin1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Cole | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Cole | EEG=          Cole.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Daniel | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]     Daniel | EEG=        Daniel.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]    Ismayil | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]    Ismayil | EEG=       Ismayil.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Jack | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Jack | EEG=          Jack.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]      James | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      James | EEG=         James.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Josh | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Josh | EEG=          Josh.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Marjan | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Marjan | EEG=        Marjan.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]        Max | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Max | EEG=           Max.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Mina | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mina | EEG=          Mina.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 1 | EEG=        Mina 1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 3 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 3 | EEG=        Mina 3.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]   Mohammad | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]   Mohammad | EEG=      Mohammad.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Mona | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mona | EEG=          Mona.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Roddy | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      Roddy | EEG=         Roddy.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]        Sam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Sam | EEG=           Sam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       adam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       adam | EEG=          adam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2] Loading complete ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 2: Load data (EMPTY LABEL => CLASS 0)\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path  # helper\n",
    "\n",
    "def load_subject(subj_dir: str, subj_name: str, expected_channels: int = 14):\n",
    "    \"\"\"\n",
    "    EEG file: {subj_name}.npy\n",
    "    Label file: {subj_name}_target.npy\n",
    "    Label rule:\n",
    "      - empty array -> class 0\n",
    "      - [0] -> class 0\n",
    "      - [1] -> class 1\n",
    "    \"\"\"\n",
    "\n",
    "    # EEG file path\n",
    "    eeg_file = os.path.join(subj_dir, f\"{subj_name}.npy\")\n",
    "    assert os.path.exists(eeg_file), f\"[ERROR] Missing EEG file: {eeg_file}\"\n",
    "\n",
    "    # Label file path\n",
    "    target_file = os.path.join(subj_dir, f\"{subj_name}_target.npy\")\n",
    "    assert os.path.exists(target_file), f\"[ERROR] Missing label file: {target_file}\"\n",
    "\n",
    "    # Load EEG\n",
    "    X_raw = np.load(eeg_file, allow_pickle=True)\n",
    "    X_raw = np.asarray(X_raw)\n",
    "\n",
    "    # Standardize EEG to (n_trials, 14, T)\n",
    "    if X_raw.ndim == 2:\n",
    "        X_trials = X_raw[None, :, :]\n",
    "    elif X_raw.ndim == 3:\n",
    "        dims = list(X_raw.shape)\n",
    "        chan_axes = [i for i, d in enumerate(dims) if d == expected_channels]\n",
    "        chan_axis = chan_axes[0] if len(chan_axes) == 1 else 1\n",
    "        time_axis = int(np.argmax(dims))\n",
    "        trial_axis = [i for i in range(3) if i not in (chan_axis, time_axis)][0]\n",
    "        X_trials = np.moveaxis(X_raw, (trial_axis, chan_axis, time_axis), (0, 1, 2))\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] Unsupported EEG shape {X_raw.shape} in {eeg_file}\")\n",
    "\n",
    "    assert X_trials.shape[1] == expected_channels, f\"[ERROR] Expected {expected_channels} channels, got {X_trials.shape[1]} in {eeg_file}\"\n",
    "\n",
    "    # Load label\n",
    "    y_raw = np.load(target_file, allow_pickle=True)\n",
    "    y_arr = np.asarray(y_raw).ravel()\n",
    "\n",
    "    # Apply your rule\n",
    "    if y_arr.size == 0:\n",
    "        label = 0\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\")\n",
    "    else:\n",
    "        label = int(y_arr[0])\n",
    "        if label not in (0, 1):\n",
    "            raise ValueError(f\"[ERROR] Invalid label {label} in {target_file} (must be 0 or 1)\")\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape={y_arr.shape}, value={y_arr} -> class={label}\")\n",
    "\n",
    "    # Repeat label for all trials\n",
    "    y_trials = np.full((X_trials.shape[0],), label, dtype=np.int32)\n",
    "\n",
    "    return X_trials.astype(np.float32), y_trials.astype(np.int32), eeg_file, target_file\n",
    "\n",
    "\n",
    "# Load all subjects\n",
    "subject_data = {}\n",
    "print(\"[STEP 2] Loading data from folders...\")\n",
    "\n",
    "for subj in all_folders:\n",
    "    subj_dir = os.path.join(BASE_PATH, subj)\n",
    "    X_trials, y_trials, eeg_file, target_file = load_subject(subj_dir, subj)\n",
    "\n",
    "    subject_data[subj] = (X_trials, y_trials)\n",
    "\n",
    "    print(f\"[STEP 2] {subj:>10} | EEG={Path(eeg_file).name:>18} | X={X_trials.shape} | y={y_trials.shape} | class={y_trials[0]}\")\n",
    "\n",
    "print(\"[STEP 2] Loading complete ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd93ca8-a94c-485c-8a61-e6e7441f6299",
   "metadata": {},
   "source": [
    "Cell 3 — Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733e592c-98e1-4cbd-afd2-6fdc8cf407f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 3] Preprocessing functions ready ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3: Preprocessing functions\n",
    "# =========================\n",
    "\n",
    "import mne  # EEG processing\n",
    "import pywt  # wavelets\n",
    "from sklearn.decomposition import FastICA  # ICA\n",
    "from typing import Optional, Tuple, Union, Sequence, Dict, List  # typing\n",
    "\n",
    "def wavelet_enhanced_ica(\n",
    "    data: np.ndarray,\n",
    "    n_components: int = 10,\n",
    "    wavelet: str = \"db4\",\n",
    "    level: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> np.ndarray:\n",
    "    n_ch, n_t = data.shape\n",
    "    n_components = min(n_components, n_ch)\n",
    "\n",
    "    coeffs = pywt.wavedec(data, wavelet=wavelet, level=level, axis=1)\n",
    "    A = coeffs[0]\n",
    "\n",
    "    ica = FastICA(n_components=n_components, random_state=random_state)\n",
    "    S = ica.fit_transform(A.T).T\n",
    "    A_denoised = ica.inverse_transform(S.T).T\n",
    "\n",
    "    coeffs[0] = A_denoised\n",
    "    cleaned = pywt.waverec(coeffs, wavelet=wavelet, axis=1)\n",
    "\n",
    "    if cleaned.shape[1] != n_t:\n",
    "        if cleaned.shape[1] > n_t:\n",
    "            cleaned = cleaned[:, :n_t]\n",
    "        else:\n",
    "            cleaned = np.pad(cleaned, ((0, 0), (0, n_t - cleaned.shape[1])), mode=\"constant\")\n",
    "    return cleaned\n",
    "\n",
    "def _names_from_index_mapping(n_channels: int, index_to_name: Optional[Dict[int, str]]) -> List[str]:\n",
    "    if index_to_name is None:\n",
    "        return [f\"EEG{i+1}\" for i in range(n_channels)]\n",
    "    keys = list(index_to_name.keys())\n",
    "    is_zero_based = (0 in keys) and (1 not in keys)\n",
    "    names = []\n",
    "    for i in range(n_channels):\n",
    "        key = i if is_zero_based else (i + 1)\n",
    "        names.append(index_to_name.get(key, f\"EEG{i+1}\"))\n",
    "    return names\n",
    "\n",
    "def preprocess_eeg(\n",
    "    eeg: np.ndarray,\n",
    "    sfreq: float,\n",
    "    *,\n",
    "    index_to_name: Optional[Dict[int, str]] = None,\n",
    "    use_standard_1010: bool = True,\n",
    "    resample_to: Optional[float] = None,\n",
    "    notch_freqs: Union[None, float, Sequence[float]] = 50.0,\n",
    "    highpass: Optional[float] = 0.05,\n",
    "    bad_point_z: float = 6.0,\n",
    "    bad_channel_z: float = 5.0,\n",
    "    interpolate_bad_channels: bool = True,\n",
    "    car: bool = True,\n",
    "    use_wica: bool = True,\n",
    "    wica_components: int = 10,\n",
    "    wica_wavelet: str = \"db4\",\n",
    "    wica_level: int = 3,\n",
    "    wica_random_state: int = 42,\n",
    "    return_raw: bool = False,\n",
    "):\n",
    "\n",
    "    eeg = np.asarray(eeg, dtype=np.float32)\n",
    "    assert eeg.ndim == 2, \"eeg must be 2D: (n_channels, n_times)\"\n",
    "\n",
    "    n_channels, _ = eeg.shape\n",
    "    ch_names = _names_from_index_mapping(n_channels, index_to_name)\n",
    "    ch_types = ['eog' if str(n).upper().startswith(\"EOG\") else 'eeg' for n in ch_names]\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    raw = mne.io.RawArray(eeg, info, verbose=False)\n",
    "\n",
    "    montage_applied = False\n",
    "    if use_standard_1010:\n",
    "        try:\n",
    "            mont = mne.channels.make_standard_montage(\"standard_1010\")\n",
    "            raw.set_montage(mont, match_case=False, on_missing=\"ignore\")\n",
    "            montage_applied = True\n",
    "        except Exception:\n",
    "            montage_applied = False\n",
    "\n",
    "    if resample_to is not None and float(resample_to) != float(sfreq):\n",
    "        raw.resample(sfreq=resample_to, npad=\"auto\")\n",
    "    sfreq_out = float(raw.info[\"sfreq\"])\n",
    "\n",
    "    if notch_freqs is not None:\n",
    "        raw.notch_filter(freqs=notch_freqs, verbose=False)\n",
    "    if highpass is not None:\n",
    "        raw.filter(l_freq=highpass, h_freq=None, verbose=False)\n",
    "\n",
    "    X = raw.get_data()\n",
    "    mu = np.mean(X, axis=1, keepdims=True)\n",
    "    sd = np.std(X, axis=1, keepdims=True) + 1e-12\n",
    "    hi = mu + bad_point_z * sd\n",
    "    lo = mu - bad_point_z * sd\n",
    "    bad_idx = (X > hi) | (X < lo)\n",
    "\n",
    "    if np.any(bad_idx):\n",
    "        X_fixed = X.copy()\n",
    "        t = np.arange(X.shape[1], dtype=float)\n",
    "        for ch in range(n_channels):\n",
    "            mask = bad_idx[ch]\n",
    "            if mask.any():\n",
    "                good = ~mask\n",
    "                if good.sum() >= 2:\n",
    "                    X_fixed[ch, mask] = np.interp(t[mask], t[good], X_fixed[ch, good])\n",
    "        raw._data = X_fixed\n",
    "\n",
    "    if interpolate_bad_channels and montage_applied:\n",
    "        X = raw.get_data(picks=\"eeg\")\n",
    "        if X.size > 0:\n",
    "            ch_std = X.std(axis=1)\n",
    "            med = np.median(ch_std)\n",
    "            mad = np.median(np.abs(ch_std - med)) + 1e-12\n",
    "            z = 0.6745 * (ch_std - med) / mad\n",
    "            eeg_names = mne.pick_info(raw.info, mne.pick_types(raw.info, eeg=True)).ch_names\n",
    "            bads = [eeg_names[i] for i in np.where(np.abs(z) > bad_channel_z)[0]]\n",
    "            raw.info[\"bads\"] = bads\n",
    "            if bads:\n",
    "                raw.interpolate_bads(reset_bads=True, verbose=False)\n",
    "\n",
    "    if car:\n",
    "        raw.set_eeg_reference(\"average\", projection=True)\n",
    "        raw.apply_proj()\n",
    "\n",
    "    if use_wica:\n",
    "        cleaned = wavelet_enhanced_ica(\n",
    "            raw.get_data(),\n",
    "            n_components=wica_components,\n",
    "            wavelet=wica_wavelet,\n",
    "            level=wica_level,\n",
    "            random_state=wica_random_state\n",
    "        )\n",
    "        raw = mne.io.RawArray(cleaned, raw.info, verbose=False)\n",
    "\n",
    "    cleaned = raw.get_data()\n",
    "    return cleaned, sfreq_out\n",
    "\n",
    "print(\"[STEP 3] Preprocessing functions ready ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a9d20-889d-45c4-bbc2-de62216908cd",
   "metadata": {},
   "source": [
    "Cell 4 — Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bfcbde0-1cc2-409c-8627-93da46606661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 4] Starting preprocessing...\n",
      "[INFO] USE_WICA = True\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 01/17: Amin | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 02/17: Amin1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 03/17: Cole | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 04/17: Daniel | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 05/17: Ismayil | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 06/17: Jack | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 07/17: James | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 08/17: Josh | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 09/17: Marjan | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 10/17: Max | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 11/17: Mina | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 12/17: Mina 3 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 13/17: Mohammad | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 14/17: Mona | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 15/17: Roddy | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 16/17: Sam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 17/17: adam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Test subject: Mina 1 | trials=1 | fs_out=128.0\n",
      "[STEP 4] Preprocessing done ✅\n",
      "  X_train_trials_all: (17, 14, 38400)\n",
      "  y_train_trials_all: (17,) (array([0, 1], dtype=int32), array([8, 9]))\n",
      "  X_test_trials_all : (1, 14, 38400)\n",
      "  y_test_trials_all : (1,) (array([0], dtype=int32), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 4: Apply preprocessing (LOSO)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 4] Starting preprocessing...\")\n",
    "\n",
    "USE_WICA = True\n",
    "print(f\"[INFO] USE_WICA = {USE_WICA}\")\n",
    "\n",
    "X_train_trials_all, y_train_trials_all = [], []\n",
    "X_test_trials_all, y_test_trials_all = [], []\n",
    "\n",
    "def preprocess_trials(X_trials, y_trials):\n",
    "    cleaned_list = []\n",
    "    label_list = []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        trial = X_trials[i]\n",
    "        label = int(y_trials[i])\n",
    "\n",
    "        cleaned, fs_out = preprocess_eeg(\n",
    "            eeg=trial,\n",
    "            sfreq=SFREQ,\n",
    "            use_wica=USE_WICA\n",
    "        )\n",
    "\n",
    "        cleaned_list.append(cleaned.astype(np.float32, copy=False))\n",
    "        label_list.append(label)\n",
    "\n",
    "    return cleaned_list, np.array(label_list, dtype=np.int32), fs_out\n",
    "\n",
    "# Train subjects\n",
    "for idx, subj in enumerate(train_subjects, start=1):\n",
    "    X_trials, y_trials = subject_data[subj]\n",
    "    cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "    X_train_trials_all.extend(cleaned_trials)\n",
    "    y_train_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "    print(f\"[STEP 4] Train {idx:02d}/{len(train_subjects)}: {subj} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Test subject\n",
    "X_trials, y_trials = subject_data[TEST_SUBJECT]\n",
    "cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "X_test_trials_all.extend(cleaned_trials)\n",
    "y_test_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "print(f\"[STEP 4] Test subject: {TEST_SUBJECT} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Convert to arrays\n",
    "X_train_trials_all = np.array(X_train_trials_all, dtype=np.float32)\n",
    "y_train_trials_all = np.array(y_train_trials_all, dtype=np.int32)\n",
    "\n",
    "X_test_trials_all = np.array(X_test_trials_all, dtype=np.float32)\n",
    "y_test_trials_all = np.array(y_test_trials_all, dtype=np.int32)\n",
    "\n",
    "print(\"[STEP 4] Preprocessing done ✅\")\n",
    "print(\"  X_train_trials_all:\", X_train_trials_all.shape)\n",
    "print(\"  y_train_trials_all:\", y_train_trials_all.shape, np.unique(y_train_trials_all, return_counts=True))\n",
    "print(\"  X_test_trials_all :\", X_test_trials_all.shape)\n",
    "print(\"  y_test_trials_all :\", y_test_trials_all.shape, np.unique(y_test_trials_all, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc3da1-2967-4ee3-84cd-bfcf0f98c0b1",
   "metadata": {},
   "source": [
    "Cell 5 — Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c21ad54-7001-4039-9360-29dfa2905127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 5] Starting segmentation...\n",
      "[STEP 5] Segmented 5/17 trials...\n",
      "[STEP 5] Segmented 10/17 trials...\n",
      "[STEP 5] Segmented 15/17 trials...\n",
      "[STEP 5] Segmented 17/17 trials...\n",
      "[STEP 5] Segmented 1/1 trials...\n",
      "[STEP 5] Segmentation done ✅\n",
      "  X_train: (5100, 14, 128, 1)\n",
      "  y_train: (5100,) (array([0, 1], dtype=int32), array([2400, 2700]))\n",
      "  X_test : (300, 14, 128, 1)\n",
      "  y_test : (300,) (array([0], dtype=int32), array([300]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 5: Segmentation\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 5] Starting segmentation...\")\n",
    "\n",
    "def segment_trial(trial_2d: np.ndarray, label: int, segment_len: int = 128):\n",
    "    C, T = trial_2d.shape\n",
    "    n_segs = T // segment_len\n",
    "    use_T = n_segs * segment_len\n",
    "\n",
    "    if n_segs == 0:\n",
    "        return np.empty((0, C, segment_len), dtype=np.float32), np.empty((0,), dtype=np.int32)\n",
    "\n",
    "    trial_trim = trial_2d[:, :use_T]\n",
    "    Xsegs = trial_trim.reshape(C, n_segs, segment_len).transpose(1, 0, 2)\n",
    "    ysegs = np.full((n_segs,), int(label), dtype=np.int32)\n",
    "    return Xsegs.astype(np.float32, copy=False), ysegs\n",
    "\n",
    "def segment_dataset(X_trials: np.ndarray, y_trials: np.ndarray, segment_len: int = 128):\n",
    "    X_all, y_all = [], []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        Xsegs, ysegs = segment_trial(X_trials[i], int(y_trials[i]), segment_len=segment_len)\n",
    "        X_all.append(Xsegs)\n",
    "        y_all.append(ysegs)\n",
    "\n",
    "        if (i + 1) % 5 == 0 or (i + 1) == X_trials.shape[0]:\n",
    "            print(f\"[STEP 5] Segmented {i+1}/{X_trials.shape[0]} trials...\")\n",
    "\n",
    "    X_out = np.concatenate(X_all, axis=0)\n",
    "    y_out = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    X_out = np.expand_dims(X_out, axis=-1)  # (N,14,128,1)\n",
    "    return X_out.astype(np.float32), y_out.astype(np.int32)\n",
    "\n",
    "X_train, y_train = segment_dataset(X_train_trials_all, y_train_trials_all, segment_len=128)\n",
    "X_test,  y_test  = segment_dataset(X_test_trials_all,  y_test_trials_all,  segment_len=128)\n",
    "\n",
    "print(\"[STEP 5] Segmentation done ✅\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  y_train:\", y_train.shape, np.unique(y_train, return_counts=True))\n",
    "print(\"  X_test :\", X_test.shape)\n",
    "print(\"  y_test :\", y_test.shape, np.unique(y_test, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b4ffd5-bf2f-490e-8854-0a2fc747f1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e69ba7-3952-47b1-bd80-f05237b2e018",
   "metadata": {},
   "source": [
    "Cell 6 — Split train/val + Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97fda4ae-e1c8-4fd1-bf0c-c82ae2a85764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 6] Normalizing using ALL training data (no train/val split)...\n",
      "[STEP 6] Normalization done ✅\n",
      "  X_train_norm: (5100, 14, 128, 1)\n",
      "  X_test_norm : (300, 14, 128, 1)\n",
      "  y_train_norm: (5100,)\n",
      "  y_test_norm : (300,)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 6: Normalize (ALL training data)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 6] Normalizing using ALL training data (no train/val split)...\")\n",
    "\n",
    "def normalize_eeg(train_data: np.ndarray, test_data: np.ndarray):\n",
    "    if train_data.ndim == 5:\n",
    "        train_flat = train_data.reshape(-1, *train_data.shape[2:])\n",
    "        test_flat  = test_data.reshape(-1, *test_data.shape[2:])\n",
    "    else:\n",
    "        train_flat, test_flat = train_data, test_data\n",
    "\n",
    "    # Compute stats on ALL training data\n",
    "    train_mean = np.mean(train_flat, axis=(0, 2, 3), keepdims=True)\n",
    "    train_std  = np.std(train_flat,  axis=(0, 2, 3), keepdims=True) + 1e-8\n",
    "\n",
    "    train_norm = (train_flat - train_mean) / train_std\n",
    "    test_norm  = (test_flat  - train_mean) / train_std\n",
    "\n",
    "    if train_data.ndim == 5:\n",
    "        train_norm = train_norm.reshape(train_data.shape)\n",
    "        test_norm  = test_norm.reshape(test_data.shape)\n",
    "\n",
    "    return (\n",
    "        train_norm.astype(np.float32),\n",
    "        test_norm.astype(np.float32),\n",
    "        train_mean,\n",
    "        train_std\n",
    "    )\n",
    "\n",
    "# Normalize ALL training data\n",
    "X_train_norm, X_test_norm, mean, std = normalize_eeg(X_train, X_test)\n",
    "\n",
    "y_train_norm = y_train.astype(np.float32)\n",
    "y_test_norm  = y_test.astype(np.float32)\n",
    "\n",
    "print(\"[STEP 6] Normalization done ✅\")\n",
    "print(\"  X_train_norm:\", X_train_norm.shape)\n",
    "print(\"  X_test_norm :\", X_test_norm.shape)\n",
    "print(\"  y_train_norm:\", y_train_norm.shape)\n",
    "print(\"  y_test_norm :\", y_test_norm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f527c-34db-4c23-bdbd-c8ce03b519f0",
   "metadata": {},
   "source": [
    "Cell 7 — EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ea4279-53c2-49c1-b834-7b1e1c20775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\DL\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Xtr: (5100, 14, 128, 1), ytr: (5100,)\n",
      "[INFO] Test / LOSO subject is NOT used here.\n",
      "[SPLIT] Train: (4080, 14, 128, 1), Val: (1020, 14, 128, 1)\n",
      "Epoch 1/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 127ms/step - accuracy: 0.5375 - loss: 0.7202 - val_accuracy: 0.4716 - val_loss: 0.6954 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.5542 - loss: 0.6832 - val_accuracy: 0.4824 - val_loss: 0.6944 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.5931 - loss: 0.6560 - val_accuracy: 0.5088 - val_loss: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.6333 - loss: 0.6247 - val_accuracy: 0.5676 - val_loss: 0.6850 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.6740 - loss: 0.5906 - val_accuracy: 0.6020 - val_loss: 0.6771 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.6934 - loss: 0.5641 - val_accuracy: 0.6578 - val_loss: 0.6611 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.7306 - loss: 0.5276 - val_accuracy: 0.6618 - val_loss: 0.6447 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7444 - loss: 0.5017 - val_accuracy: 0.7118 - val_loss: 0.6238 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.7676 - loss: 0.4730 - val_accuracy: 0.6578 - val_loss: 0.6095 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.7848 - loss: 0.4358 - val_accuracy: 0.6402 - val_loss: 0.6019 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.7877 - loss: 0.4317 - val_accuracy: 0.6618 - val_loss: 0.5784 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8017 - loss: 0.4260 - val_accuracy: 0.6775 - val_loss: 0.5557 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8123 - loss: 0.4014 - val_accuracy: 0.6980 - val_loss: 0.5372 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8203 - loss: 0.3873 - val_accuracy: 0.6853 - val_loss: 0.5321 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8319 - loss: 0.3859 - val_accuracy: 0.6971 - val_loss: 0.5179 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8353 - loss: 0.3689 - val_accuracy: 0.6716 - val_loss: 0.5444 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8382 - loss: 0.3559 - val_accuracy: 0.6912 - val_loss: 0.5180 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8419 - loss: 0.3616 - val_accuracy: 0.6863 - val_loss: 0.5161 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8502 - loss: 0.3435 - val_accuracy: 0.7324 - val_loss: 0.4591 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8517 - loss: 0.3410 - val_accuracy: 0.7304 - val_loss: 0.4712 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8569 - loss: 0.3267 - val_accuracy: 0.7647 - val_loss: 0.4450 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.8620 - loss: 0.3154 - val_accuracy: 0.7951 - val_loss: 0.3971 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.8608 - loss: 0.3179 - val_accuracy: 0.7971 - val_loss: 0.4085 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.8510 - loss: 0.3451 - val_accuracy: 0.8216 - val_loss: 0.3875 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.8588 - loss: 0.3299 - val_accuracy: 0.8059 - val_loss: 0.3973 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8632 - loss: 0.3327\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.8522 - loss: 0.3443 - val_accuracy: 0.8618 - val_loss: 0.3444 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8635 - loss: 0.3055 - val_accuracy: 0.8667 - val_loss: 0.3067 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8804 - loss: 0.2873 - val_accuracy: 0.8657 - val_loss: 0.3079 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8836 - loss: 0.2863 - val_accuracy: 0.8951 - val_loss: 0.2812 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8863 - loss: 0.2810 - val_accuracy: 0.8765 - val_loss: 0.2772 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8860 - loss: 0.2731 - val_accuracy: 0.8814 - val_loss: 0.2706 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8917 - loss: 0.2754 - val_accuracy: 0.9206 - val_loss: 0.2356 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8885 - loss: 0.2744 - val_accuracy: 0.9255 - val_loss: 0.2271 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8887 - loss: 0.2718 - val_accuracy: 0.9304 - val_loss: 0.2192 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8890 - loss: 0.2645 - val_accuracy: 0.9275 - val_loss: 0.2150 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8922 - loss: 0.2682 - val_accuracy: 0.9353 - val_loss: 0.2099 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8924 - loss: 0.2673 - val_accuracy: 0.9490 - val_loss: 0.1976 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.8882 - loss: 0.2658 - val_accuracy: 0.9471 - val_loss: 0.1953 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8867 - loss: 0.2756 \n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8875 - loss: 0.2671 - val_accuracy: 0.9382 - val_loss: 0.1878 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8895 - loss: 0.2619 - val_accuracy: 0.9480 - val_loss: 0.1804 - learning_rate: 2.5000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8966 - loss: 0.2485 - val_accuracy: 0.9500 - val_loss: 0.1740 - learning_rate: 2.5000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9025 - loss: 0.2335 - val_accuracy: 0.9500 - val_loss: 0.1689 - learning_rate: 2.5000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9034 - loss: 0.2440 - val_accuracy: 0.9520 - val_loss: 0.1691 - learning_rate: 2.5000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9049 - loss: 0.2450 - val_accuracy: 0.9520 - val_loss: 0.1692 - learning_rate: 2.5000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8980 - loss: 0.2507 - val_accuracy: 0.9549 - val_loss: 0.1641 - learning_rate: 2.5000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8996 - loss: 0.2593\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.8966 - loss: 0.2572 - val_accuracy: 0.9520 - val_loss: 0.1769 - learning_rate: 2.5000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 132ms/step - accuracy: 0.8998 - loss: 0.2464 - val_accuracy: 0.9480 - val_loss: 0.1640 - learning_rate: 1.2500e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9049 - loss: 0.2369 - val_accuracy: 0.9520 - val_loss: 0.1638 - learning_rate: 1.2500e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9076 - loss: 0.2331 - val_accuracy: 0.9510 - val_loss: 0.1655 - learning_rate: 1.2500e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9017 - loss: 0.2485 - val_accuracy: 0.9520 - val_loss: 0.1583 - learning_rate: 1.2500e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9051 - loss: 0.2437 - val_accuracy: 0.9480 - val_loss: 0.1566 - learning_rate: 1.2500e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9042 - loss: 0.2364 - val_accuracy: 0.9520 - val_loss: 0.1641 - learning_rate: 1.2500e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9020 - loss: 0.2312 - val_accuracy: 0.9529 - val_loss: 0.1603 - learning_rate: 1.2500e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.8993 - loss: 0.2397 - val_accuracy: 0.9529 - val_loss: 0.1583 - learning_rate: 1.2500e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9034 - loss: 0.2390 - val_accuracy: 0.9520 - val_loss: 0.1577 - learning_rate: 1.2500e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9020 - loss: 0.2414 - val_accuracy: 0.9549 - val_loss: 0.1565 - learning_rate: 1.2500e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9071 - loss: 0.2300 - val_accuracy: 0.9549 - val_loss: 0.1535 - learning_rate: 1.2500e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9034 - loss: 0.2270 - val_accuracy: 0.9539 - val_loss: 0.1523 - learning_rate: 1.2500e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9056 - loss: 0.2302 - val_accuracy: 0.9549 - val_loss: 0.1528 - learning_rate: 1.2500e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9071 - loss: 0.2311 - val_accuracy: 0.9529 - val_loss: 0.1483 - learning_rate: 1.2500e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9022 - loss: 0.2310 - val_accuracy: 0.9539 - val_loss: 0.1478 - learning_rate: 1.2500e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9136 - loss: 0.2278\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9105 - loss: 0.2286 - val_accuracy: 0.9559 - val_loss: 0.1549 - learning_rate: 1.2500e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9086 - loss: 0.2366 - val_accuracy: 0.9549 - val_loss: 0.1502 - learning_rate: 6.2500e-05\n",
      "Epoch 64/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9076 - loss: 0.2331 - val_accuracy: 0.9559 - val_loss: 0.1520 - learning_rate: 6.2500e-05\n",
      "Epoch 65/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9157 - loss: 0.2173 - val_accuracy: 0.9510 - val_loss: 0.1463 - learning_rate: 6.2500e-05\n",
      "Epoch 66/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9064 - loss: 0.2288 - val_accuracy: 0.9559 - val_loss: 0.1537 - learning_rate: 6.2500e-05\n",
      "Epoch 67/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9056 - loss: 0.2332 - val_accuracy: 0.9539 - val_loss: 0.1436 - learning_rate: 6.2500e-05\n",
      "Epoch 68/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9069 - loss: 0.2352 - val_accuracy: 0.9529 - val_loss: 0.1552 - learning_rate: 6.2500e-05\n",
      "Epoch 69/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9037 - loss: 0.2271\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9051 - loss: 0.2258 - val_accuracy: 0.9549 - val_loss: 0.1443 - learning_rate: 6.2500e-05\n",
      "Epoch 70/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9162 - loss: 0.2215 - val_accuracy: 0.9588 - val_loss: 0.1488 - learning_rate: 3.1250e-05\n",
      "Epoch 71/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9115 - loss: 0.2225 - val_accuracy: 0.9578 - val_loss: 0.1458 - learning_rate: 3.1250e-05\n",
      "Epoch 72/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9152 - loss: 0.2184 - val_accuracy: 0.9578 - val_loss: 0.1449 - learning_rate: 3.1250e-05\n",
      "Epoch 73/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9095 - loss: 0.2211\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9093 - loss: 0.2300 - val_accuracy: 0.9529 - val_loss: 0.1464 - learning_rate: 3.1250e-05\n",
      "Epoch 74/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9074 - loss: 0.2313 - val_accuracy: 0.9569 - val_loss: 0.1460 - learning_rate: 1.5625e-05\n",
      "Epoch 75/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9113 - loss: 0.2220 - val_accuracy: 0.9588 - val_loss: 0.1464 - learning_rate: 1.5625e-05\n",
      "Epoch 76/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9130 - loss: 0.2181 - val_accuracy: 0.9588 - val_loss: 0.1467 - learning_rate: 1.5625e-05\n",
      "Epoch 77/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9099 - loss: 0.2369\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9098 - loss: 0.2342 - val_accuracy: 0.9569 - val_loss: 0.1442 - learning_rate: 1.5625e-05\n",
      "Epoch 78/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9081 - loss: 0.2399 - val_accuracy: 0.9569 - val_loss: 0.1448 - learning_rate: 7.8125e-06\n",
      "Epoch 79/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9132 - loss: 0.2160 - val_accuracy: 0.9559 - val_loss: 0.1448 - learning_rate: 7.8125e-06\n",
      "Epoch 80/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9100 - loss: 0.2343 - val_accuracy: 0.9578 - val_loss: 0.1451 - learning_rate: 7.8125e-06\n",
      "Epoch 81/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9127 - loss: 0.2175 - val_accuracy: 0.9569 - val_loss: 0.1439 - learning_rate: 7.8125e-06\n",
      "Epoch 82/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9145 - loss: 0.2181 - val_accuracy: 0.9569 - val_loss: 0.1439 - learning_rate: 7.8125e-06\n",
      "Epoch 82: early stopping\n",
      "Restoring model weights from the end of the best epoch: 67.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] Model saved as cole-final.h5\n",
      "\n",
      "[CONFUSION MATRIX]\n",
      "[[453  27]\n",
      " [ 20 520]]\n",
      "[VAL] acc=0.9539, prec=0.9506, rec=0.9630, f1=0.9568\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 4: 80:20 Train / Validation Split (EEGNet)\n",
    "# ====================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, SeparableConv2D, BatchNormalization, Activation,\n",
    "    DepthwiseConv2D, Conv2D, AveragePooling2D,\n",
    "    Dropout, Dense, Flatten\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# --------------------------------------------------------------------\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# EEGNet model\n",
    "# --------------------------------------------------------------------\n",
    "def create_eegnet_model(input_shape, dropout_rate=0.5, num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(16, (1, 64), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = DepthwiseConv2D(\n",
    "        (input_shape[0], 1),\n",
    "        depth_multiplier=2,\n",
    "        padding='valid',\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 4))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = SeparableConv2D(16, (1, 16), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 8))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Compile model\n",
    "# --------------------------------------------------------------------\n",
    "def compile_model():\n",
    "    input_shape = (Xtr.shape[1], Xtr.shape[2], Xtr.shape[3])\n",
    "    model = create_eegnet_model(input_shape)\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-3),\n",
    "        loss=BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[BinaryAccuracy(name=\"accuracy\", threshold=0.5)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def make_callbacks():\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"loss\",\n",
    "            factor=0.5,\n",
    "            patience=4,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Use ALL normalized training data\n",
    "# --------------------------------------------------------------------\n",
    "Xtr = X_train_norm\n",
    "ytr = y_train_norm\n",
    "\n",
    "print(f\"[TRAIN] Xtr: {Xtr.shape}, ytr: {ytr.shape}\")\n",
    "print(\"[INFO] Test / LOSO subject is NOT used here.\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 80:20 Stratified Split\n",
    "# --------------------------------------------------------------------\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(sss.split(Xtr, ytr))\n",
    "\n",
    "X_train, X_val = Xtr[train_idx], Xtr[val_idx]\n",
    "y_train, y_val = ytr[train_idx], ytr[val_idx]\n",
    "\n",
    "print(f\"[SPLIT] Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Train model\n",
    "# --------------------------------------------------------------------\n",
    "model = compile_model()\n",
    "callbacks = make_callbacks()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Save final model\n",
    "# --------------------------------------------------------------------\n",
    "model.save(\"mina1-final.h5\")\n",
    "print(\"[SAVE] Model saved as cole-final.h5\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Validation Confusion Matrix\n",
    "# --------------------------------------------------------------------\n",
    "y_prob = model.predict(X_val, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "y_true = y_val.astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "eps = 1e-12\n",
    "\n",
    "acc  = (tp + tn) / max(tp + tn + fp + fn, eps)\n",
    "prec = tp / max(tp + fp, eps)\n",
    "rec  = tp / max(tp + fn, eps)\n",
    "f1   = 2 * prec * rec / max(prec + rec, eps)\n",
    "\n",
    "print(\"\\n[CONFUSION MATRIX]\")\n",
    "print(cm)\n",
    "print(f\"[VAL] acc={acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e2a13-6367-41ca-be66-a43594c2940e",
   "metadata": {},
   "source": [
    "Cell 8 — Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ac6662-33c1-4e25-b6c5-d47c860fba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Preparing TEST data...\n",
      "[TEST] Xte shape: (300, 14, 128, 1)\n",
      "[TEST] yte shape: (300,), class distribution: (array([0]), array([300]))\n",
      "[TEST] Loading final model: mina1-final.h5\n",
      "[TEST] Running inference on TEST subject...\n",
      "[TEST] Mean predicted probability: 0.2966\n",
      "\n",
      "[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[230  70]\n",
      " [  0   0]]\n",
      "Accuracy : 0.7667\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-score : 0.0000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Inference on TEST subject (adam) using FINAL EEGNet\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Prepare TEST data\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Preparing TEST data...\")\n",
    "\n",
    "Xte = X_test_norm\n",
    "yte = y_test_norm.astype(int)\n",
    "\n",
    "print(f\"[TEST] Xte shape: {Xte.shape}\")\n",
    "print(f\"[TEST] yte shape: {yte.shape}, class distribution: {np.unique(yte, return_counts=True)}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Load FINAL trained model\n",
    "# ------------------------------------------------------------------\n",
    "model_path = \"mina1-final.h5\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        \"Final model 'cole-final.h5' not found. \"\n",
    "        \"Please run Cell 4 (80:20 training) first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[TEST] Loading final model: {model_path}\")\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Run inference on TEST subject\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Running inference on TEST subject...\")\n",
    "\n",
    "y_prob = model.predict(Xte, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(f\"[TEST] Mean predicted probability: {y_prob.mean():.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Metrics on TEST subject (segment-level)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc = accuracy_score(yte, y_pred)\n",
    "prec = precision_score(yte, y_pred, zero_division=0)\n",
    "rec = recall_score(yte, y_pred, zero_division=0)\n",
    "f1 = f1_score(yte, y_pred, zero_division=0)\n",
    "\n",
    "print(\"\\n[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572dbf3e-bbb5-4865-883f-30e374700949",
   "metadata": {},
   "source": [
    "Cell 9 - Performance Matrices for each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a57240-50ea-4736-a7ac-52ef7878b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] Verifying predictions and labels are available...\n",
      "[CHECK] Found predictions for 300 test segments.\n",
      "\n",
      "[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[230  70]\n",
      " [  0   0]]\n",
      "Accuracy : 0.7667\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-score : 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Final model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================\n",
    "# Cell 6: Confusion Matrix & Metrics on TEST (FINAL EEGNet)\n",
    "# =======================================================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# ------------------------------------------------------------------\n",
    "def compute_metrics_from_cm(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    eps = 1e-12\n",
    "    acc  = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec  = tp / (tp + fn + eps)\n",
    "    f1   = 2 * prec * rec / (prec + rec + eps)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], ['0', '1'])\n",
    "    plt.yticks([0, 1], ['0', '1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(\n",
    "                j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Validate upstream artifacts\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[CHECK] Verifying predictions and labels are available...\")\n",
    "\n",
    "if 'y_pred' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Predictions 'y_pred' not found. \"\n",
    "        \"Please run Cell 5 (final model inference) first.\"\n",
    "    )\n",
    "\n",
    "if 'yte' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Test labels 'yte' not found. \"\n",
    "        \"Please run Cell 5 first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[CHECK] Found predictions for {len(y_pred)} test segments.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Confusion Matrix & Metrics (FINAL model)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc, prec, rec, f1 = compute_metrics_from_cm(cm)\n",
    "\n",
    "print(\"\\n[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(cm, title=\"Confusion Matrix - FINAL EEGNet\")\n",
    "\n",
    "print(\"[DONE] Final model evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0f120-39fd-497e-9098-05e6c82f4807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
