{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af5b471-cea9-4194-bc23-da5cdc2f9c67",
   "metadata": {},
   "source": [
    "Cell 1 — Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64aaa897-ab61-4eb4-a558-4c078acf751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Base path set to:\n",
      "C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\n",
      "[INFO] Found 18 folders:\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Selected test subject (LOSO): Mina\n",
      "[INFO] Training subjects (17):\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Sampling rate (SFREQ) = 128.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 1: Setup & discovery\n",
    "# =========================\n",
    "\n",
    "import os  # file ops\n",
    "import glob  # file matching\n",
    "import numpy as np  # arrays\n",
    "import random  # seeds\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data path\n",
    "BASE_PATH = r\"C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\"\n",
    "print(f\"[INFO] Base path set to:\\n{BASE_PATH}\")\n",
    "\n",
    "# List subject folders\n",
    "all_folders = [p for p in sorted(os.listdir(BASE_PATH)) if os.path.isdir(os.path.join(BASE_PATH, p))]\n",
    "print(f\"[INFO] Found {len(all_folders)} folders:\\n{all_folders}\")\n",
    "\n",
    "# Expected count (change if needed)\n",
    "assert len(all_folders) == 18, f\"Expected 18 folders, found {len(all_folders)}.\"\n",
    "\n",
    "# LOSO test subject\n",
    "TEST_SUBJECT = \"Mina\"\n",
    "print(f\"[INFO] Selected test subject (LOSO): {TEST_SUBJECT}\")\n",
    "\n",
    "# Train subjects = remaining\n",
    "train_subjects = [s for s in all_folders if s.lower() != TEST_SUBJECT.lower()]\n",
    "print(f\"[INFO] Training subjects ({len(train_subjects)}):\\n{train_subjects}\")\n",
    "\n",
    "# Sampling rate\n",
    "SFREQ = 128.0\n",
    "print(f\"[INFO] Sampling rate (SFREQ) = {SFREQ} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e649681-2861-4ce2-bbd1-0d9dd3359f42",
   "metadata": {},
   "source": [
    "Cell 2 — Load EEG + labels per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943c5c6e-a7aa-4618-899c-daf341c25358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 2] Loading data from folders...\n",
      "[STEP 2]       Amin | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Amin | EEG=          Amin.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Amin1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]      Amin1 | EEG=         Amin1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Cole | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Cole | EEG=          Cole.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Daniel | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]     Daniel | EEG=        Daniel.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]    Ismayil | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]    Ismayil | EEG=       Ismayil.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Jack | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Jack | EEG=          Jack.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]      James | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      James | EEG=         James.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Josh | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Josh | EEG=          Josh.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Marjan | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Marjan | EEG=        Marjan.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]        Max | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Max | EEG=           Max.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Mina | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mina | EEG=          Mina.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 1 | EEG=        Mina 1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 3 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 3 | EEG=        Mina 3.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]   Mohammad | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]   Mohammad | EEG=      Mohammad.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Mona | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mona | EEG=          Mona.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Roddy | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      Roddy | EEG=         Roddy.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]        Sam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Sam | EEG=           Sam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       adam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       adam | EEG=          adam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2] Loading complete ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 2: Load data (EMPTY LABEL => CLASS 0)\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path  # helper\n",
    "\n",
    "def load_subject(subj_dir: str, subj_name: str, expected_channels: int = 14):\n",
    "    \"\"\"\n",
    "    EEG file: {subj_name}.npy\n",
    "    Label file: {subj_name}_target.npy\n",
    "    Label rule:\n",
    "      - empty array -> class 0\n",
    "      - [0] -> class 0\n",
    "      - [1] -> class 1\n",
    "    \"\"\"\n",
    "\n",
    "    # EEG file path\n",
    "    eeg_file = os.path.join(subj_dir, f\"{subj_name}.npy\")\n",
    "    assert os.path.exists(eeg_file), f\"[ERROR] Missing EEG file: {eeg_file}\"\n",
    "\n",
    "    # Label file path\n",
    "    target_file = os.path.join(subj_dir, f\"{subj_name}_target.npy\")\n",
    "    assert os.path.exists(target_file), f\"[ERROR] Missing label file: {target_file}\"\n",
    "\n",
    "    # Load EEG\n",
    "    X_raw = np.load(eeg_file, allow_pickle=True)\n",
    "    X_raw = np.asarray(X_raw)\n",
    "\n",
    "    # Standardize EEG to (n_trials, 14, T)\n",
    "    if X_raw.ndim == 2:\n",
    "        X_trials = X_raw[None, :, :]\n",
    "    elif X_raw.ndim == 3:\n",
    "        dims = list(X_raw.shape)\n",
    "        chan_axes = [i for i, d in enumerate(dims) if d == expected_channels]\n",
    "        chan_axis = chan_axes[0] if len(chan_axes) == 1 else 1\n",
    "        time_axis = int(np.argmax(dims))\n",
    "        trial_axis = [i for i in range(3) if i not in (chan_axis, time_axis)][0]\n",
    "        X_trials = np.moveaxis(X_raw, (trial_axis, chan_axis, time_axis), (0, 1, 2))\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] Unsupported EEG shape {X_raw.shape} in {eeg_file}\")\n",
    "\n",
    "    assert X_trials.shape[1] == expected_channels, f\"[ERROR] Expected {expected_channels} channels, got {X_trials.shape[1]} in {eeg_file}\"\n",
    "\n",
    "    # Load label\n",
    "    y_raw = np.load(target_file, allow_pickle=True)\n",
    "    y_arr = np.asarray(y_raw).ravel()\n",
    "\n",
    "    # Apply your rule\n",
    "    if y_arr.size == 0:\n",
    "        label = 0\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\")\n",
    "    else:\n",
    "        label = int(y_arr[0])\n",
    "        if label not in (0, 1):\n",
    "            raise ValueError(f\"[ERROR] Invalid label {label} in {target_file} (must be 0 or 1)\")\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape={y_arr.shape}, value={y_arr} -> class={label}\")\n",
    "\n",
    "    # Repeat label for all trials\n",
    "    y_trials = np.full((X_trials.shape[0],), label, dtype=np.int32)\n",
    "\n",
    "    return X_trials.astype(np.float32), y_trials.astype(np.int32), eeg_file, target_file\n",
    "\n",
    "\n",
    "# Load all subjects\n",
    "subject_data = {}\n",
    "print(\"[STEP 2] Loading data from folders...\")\n",
    "\n",
    "for subj in all_folders:\n",
    "    subj_dir = os.path.join(BASE_PATH, subj)\n",
    "    X_trials, y_trials, eeg_file, target_file = load_subject(subj_dir, subj)\n",
    "\n",
    "    subject_data[subj] = (X_trials, y_trials)\n",
    "\n",
    "    print(f\"[STEP 2] {subj:>10} | EEG={Path(eeg_file).name:>18} | X={X_trials.shape} | y={y_trials.shape} | class={y_trials[0]}\")\n",
    "\n",
    "print(\"[STEP 2] Loading complete ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd93ca8-a94c-485c-8a61-e6e7441f6299",
   "metadata": {},
   "source": [
    "Cell 3 — Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733e592c-98e1-4cbd-afd2-6fdc8cf407f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 3] Preprocessing functions ready ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3: Preprocessing functions\n",
    "# =========================\n",
    "\n",
    "import mne  # EEG processing\n",
    "import pywt  # wavelets\n",
    "from sklearn.decomposition import FastICA  # ICA\n",
    "from typing import Optional, Tuple, Union, Sequence, Dict, List  # typing\n",
    "\n",
    "def wavelet_enhanced_ica(\n",
    "    data: np.ndarray,\n",
    "    n_components: int = 10,\n",
    "    wavelet: str = \"db4\",\n",
    "    level: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> np.ndarray:\n",
    "    n_ch, n_t = data.shape\n",
    "    n_components = min(n_components, n_ch)\n",
    "\n",
    "    coeffs = pywt.wavedec(data, wavelet=wavelet, level=level, axis=1)\n",
    "    A = coeffs[0]\n",
    "\n",
    "    ica = FastICA(n_components=n_components, random_state=random_state)\n",
    "    S = ica.fit_transform(A.T).T\n",
    "    A_denoised = ica.inverse_transform(S.T).T\n",
    "\n",
    "    coeffs[0] = A_denoised\n",
    "    cleaned = pywt.waverec(coeffs, wavelet=wavelet, axis=1)\n",
    "\n",
    "    if cleaned.shape[1] != n_t:\n",
    "        if cleaned.shape[1] > n_t:\n",
    "            cleaned = cleaned[:, :n_t]\n",
    "        else:\n",
    "            cleaned = np.pad(cleaned, ((0, 0), (0, n_t - cleaned.shape[1])), mode=\"constant\")\n",
    "    return cleaned\n",
    "\n",
    "def _names_from_index_mapping(n_channels: int, index_to_name: Optional[Dict[int, str]]) -> List[str]:\n",
    "    if index_to_name is None:\n",
    "        return [f\"EEG{i+1}\" for i in range(n_channels)]\n",
    "    keys = list(index_to_name.keys())\n",
    "    is_zero_based = (0 in keys) and (1 not in keys)\n",
    "    names = []\n",
    "    for i in range(n_channels):\n",
    "        key = i if is_zero_based else (i + 1)\n",
    "        names.append(index_to_name.get(key, f\"EEG{i+1}\"))\n",
    "    return names\n",
    "\n",
    "def preprocess_eeg(\n",
    "    eeg: np.ndarray,\n",
    "    sfreq: float,\n",
    "    *,\n",
    "    index_to_name: Optional[Dict[int, str]] = None,\n",
    "    use_standard_1010: bool = True,\n",
    "    resample_to: Optional[float] = None,\n",
    "    notch_freqs: Union[None, float, Sequence[float]] = 50.0,\n",
    "    highpass: Optional[float] = 0.05,\n",
    "    bad_point_z: float = 6.0,\n",
    "    bad_channel_z: float = 5.0,\n",
    "    interpolate_bad_channels: bool = True,\n",
    "    car: bool = True,\n",
    "    use_wica: bool = True,\n",
    "    wica_components: int = 10,\n",
    "    wica_wavelet: str = \"db4\",\n",
    "    wica_level: int = 3,\n",
    "    wica_random_state: int = 42,\n",
    "    return_raw: bool = False,\n",
    "):\n",
    "\n",
    "    eeg = np.asarray(eeg, dtype=np.float32)\n",
    "    assert eeg.ndim == 2, \"eeg must be 2D: (n_channels, n_times)\"\n",
    "\n",
    "    n_channels, _ = eeg.shape\n",
    "    ch_names = _names_from_index_mapping(n_channels, index_to_name)\n",
    "    ch_types = ['eog' if str(n).upper().startswith(\"EOG\") else 'eeg' for n in ch_names]\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    raw = mne.io.RawArray(eeg, info, verbose=False)\n",
    "\n",
    "    montage_applied = False\n",
    "    if use_standard_1010:\n",
    "        try:\n",
    "            mont = mne.channels.make_standard_montage(\"standard_1010\")\n",
    "            raw.set_montage(mont, match_case=False, on_missing=\"ignore\")\n",
    "            montage_applied = True\n",
    "        except Exception:\n",
    "            montage_applied = False\n",
    "\n",
    "    if resample_to is not None and float(resample_to) != float(sfreq):\n",
    "        raw.resample(sfreq=resample_to, npad=\"auto\")\n",
    "    sfreq_out = float(raw.info[\"sfreq\"])\n",
    "\n",
    "    if notch_freqs is not None:\n",
    "        raw.notch_filter(freqs=notch_freqs, verbose=False)\n",
    "    if highpass is not None:\n",
    "        raw.filter(l_freq=highpass, h_freq=None, verbose=False)\n",
    "\n",
    "    X = raw.get_data()\n",
    "    mu = np.mean(X, axis=1, keepdims=True)\n",
    "    sd = np.std(X, axis=1, keepdims=True) + 1e-12\n",
    "    hi = mu + bad_point_z * sd\n",
    "    lo = mu - bad_point_z * sd\n",
    "    bad_idx = (X > hi) | (X < lo)\n",
    "\n",
    "    if np.any(bad_idx):\n",
    "        X_fixed = X.copy()\n",
    "        t = np.arange(X.shape[1], dtype=float)\n",
    "        for ch in range(n_channels):\n",
    "            mask = bad_idx[ch]\n",
    "            if mask.any():\n",
    "                good = ~mask\n",
    "                if good.sum() >= 2:\n",
    "                    X_fixed[ch, mask] = np.interp(t[mask], t[good], X_fixed[ch, good])\n",
    "        raw._data = X_fixed\n",
    "\n",
    "    if interpolate_bad_channels and montage_applied:\n",
    "        X = raw.get_data(picks=\"eeg\")\n",
    "        if X.size > 0:\n",
    "            ch_std = X.std(axis=1)\n",
    "            med = np.median(ch_std)\n",
    "            mad = np.median(np.abs(ch_std - med)) + 1e-12\n",
    "            z = 0.6745 * (ch_std - med) / mad\n",
    "            eeg_names = mne.pick_info(raw.info, mne.pick_types(raw.info, eeg=True)).ch_names\n",
    "            bads = [eeg_names[i] for i in np.where(np.abs(z) > bad_channel_z)[0]]\n",
    "            raw.info[\"bads\"] = bads\n",
    "            if bads:\n",
    "                raw.interpolate_bads(reset_bads=True, verbose=False)\n",
    "\n",
    "    if car:\n",
    "        raw.set_eeg_reference(\"average\", projection=True)\n",
    "        raw.apply_proj()\n",
    "\n",
    "    if use_wica:\n",
    "        cleaned = wavelet_enhanced_ica(\n",
    "            raw.get_data(),\n",
    "            n_components=wica_components,\n",
    "            wavelet=wica_wavelet,\n",
    "            level=wica_level,\n",
    "            random_state=wica_random_state\n",
    "        )\n",
    "        raw = mne.io.RawArray(cleaned, raw.info, verbose=False)\n",
    "\n",
    "    cleaned = raw.get_data()\n",
    "    return cleaned, sfreq_out\n",
    "\n",
    "print(\"[STEP 3] Preprocessing functions ready ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a9d20-889d-45c4-bbc2-de62216908cd",
   "metadata": {},
   "source": [
    "Cell 4 — Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bfcbde0-1cc2-409c-8627-93da46606661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 4] Starting preprocessing...\n",
      "[INFO] USE_WICA = True\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 01/17: Amin | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 02/17: Amin1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 03/17: Cole | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 04/17: Daniel | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 05/17: Ismayil | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 06/17: Jack | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 07/17: James | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 08/17: Josh | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 09/17: Marjan | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 10/17: Max | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 11/17: Mina 1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 12/17: Mina 3 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 13/17: Mohammad | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 14/17: Mona | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 15/17: Roddy | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 16/17: Sam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 17/17: adam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Test subject: Mina | trials=1 | fs_out=128.0\n",
      "[STEP 4] Preprocessing done ✅\n",
      "  X_train_trials_all: (17, 14, 38400)\n",
      "  y_train_trials_all: (17,) (array([0, 1], dtype=int32), array([8, 9]))\n",
      "  X_test_trials_all : (1, 14, 38400)\n",
      "  y_test_trials_all : (1,) (array([0], dtype=int32), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 4: Apply preprocessing (LOSO)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 4] Starting preprocessing...\")\n",
    "\n",
    "USE_WICA = True\n",
    "print(f\"[INFO] USE_WICA = {USE_WICA}\")\n",
    "\n",
    "X_train_trials_all, y_train_trials_all = [], []\n",
    "X_test_trials_all, y_test_trials_all = [], []\n",
    "\n",
    "def preprocess_trials(X_trials, y_trials):\n",
    "    cleaned_list = []\n",
    "    label_list = []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        trial = X_trials[i]\n",
    "        label = int(y_trials[i])\n",
    "\n",
    "        cleaned, fs_out = preprocess_eeg(\n",
    "            eeg=trial,\n",
    "            sfreq=SFREQ,\n",
    "            use_wica=USE_WICA\n",
    "        )\n",
    "\n",
    "        cleaned_list.append(cleaned.astype(np.float32, copy=False))\n",
    "        label_list.append(label)\n",
    "\n",
    "    return cleaned_list, np.array(label_list, dtype=np.int32), fs_out\n",
    "\n",
    "# Train subjects\n",
    "for idx, subj in enumerate(train_subjects, start=1):\n",
    "    X_trials, y_trials = subject_data[subj]\n",
    "    cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "    X_train_trials_all.extend(cleaned_trials)\n",
    "    y_train_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "    print(f\"[STEP 4] Train {idx:02d}/{len(train_subjects)}: {subj} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Test subject\n",
    "X_trials, y_trials = subject_data[TEST_SUBJECT]\n",
    "cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "X_test_trials_all.extend(cleaned_trials)\n",
    "y_test_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "print(f\"[STEP 4] Test subject: {TEST_SUBJECT} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Convert to arrays\n",
    "X_train_trials_all = np.array(X_train_trials_all, dtype=np.float32)\n",
    "y_train_trials_all = np.array(y_train_trials_all, dtype=np.int32)\n",
    "\n",
    "X_test_trials_all = np.array(X_test_trials_all, dtype=np.float32)\n",
    "y_test_trials_all = np.array(y_test_trials_all, dtype=np.int32)\n",
    "\n",
    "print(\"[STEP 4] Preprocessing done ✅\")\n",
    "print(\"  X_train_trials_all:\", X_train_trials_all.shape)\n",
    "print(\"  y_train_trials_all:\", y_train_trials_all.shape, np.unique(y_train_trials_all, return_counts=True))\n",
    "print(\"  X_test_trials_all :\", X_test_trials_all.shape)\n",
    "print(\"  y_test_trials_all :\", y_test_trials_all.shape, np.unique(y_test_trials_all, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc3da1-2967-4ee3-84cd-bfcf0f98c0b1",
   "metadata": {},
   "source": [
    "Cell 5 — Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c21ad54-7001-4039-9360-29dfa2905127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 5] Starting segmentation...\n",
      "[STEP 5] Segmented 5/17 trials...\n",
      "[STEP 5] Segmented 10/17 trials...\n",
      "[STEP 5] Segmented 15/17 trials...\n",
      "[STEP 5] Segmented 17/17 trials...\n",
      "[STEP 5] Segmented 1/1 trials...\n",
      "[STEP 5] Segmentation done ✅\n",
      "  X_train: (5100, 14, 128, 1)\n",
      "  y_train: (5100,) (array([0, 1], dtype=int32), array([2400, 2700]))\n",
      "  X_test : (300, 14, 128, 1)\n",
      "  y_test : (300,) (array([0], dtype=int32), array([300]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 5: Segmentation\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 5] Starting segmentation...\")\n",
    "\n",
    "def segment_trial(trial_2d: np.ndarray, label: int, segment_len: int = 128):\n",
    "    C, T = trial_2d.shape\n",
    "    n_segs = T // segment_len\n",
    "    use_T = n_segs * segment_len\n",
    "\n",
    "    if n_segs == 0:\n",
    "        return np.empty((0, C, segment_len), dtype=np.float32), np.empty((0,), dtype=np.int32)\n",
    "\n",
    "    trial_trim = trial_2d[:, :use_T]\n",
    "    Xsegs = trial_trim.reshape(C, n_segs, segment_len).transpose(1, 0, 2)\n",
    "    ysegs = np.full((n_segs,), int(label), dtype=np.int32)\n",
    "    return Xsegs.astype(np.float32, copy=False), ysegs\n",
    "\n",
    "def segment_dataset(X_trials: np.ndarray, y_trials: np.ndarray, segment_len: int = 128):\n",
    "    X_all, y_all = [], []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        Xsegs, ysegs = segment_trial(X_trials[i], int(y_trials[i]), segment_len=segment_len)\n",
    "        X_all.append(Xsegs)\n",
    "        y_all.append(ysegs)\n",
    "\n",
    "        if (i + 1) % 5 == 0 or (i + 1) == X_trials.shape[0]:\n",
    "            print(f\"[STEP 5] Segmented {i+1}/{X_trials.shape[0]} trials...\")\n",
    "\n",
    "    X_out = np.concatenate(X_all, axis=0)\n",
    "    y_out = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    X_out = np.expand_dims(X_out, axis=-1)  # (N,14,128,1)\n",
    "    return X_out.astype(np.float32), y_out.astype(np.int32)\n",
    "\n",
    "X_train, y_train = segment_dataset(X_train_trials_all, y_train_trials_all, segment_len=128)\n",
    "X_test,  y_test  = segment_dataset(X_test_trials_all,  y_test_trials_all,  segment_len=128)\n",
    "\n",
    "print(\"[STEP 5] Segmentation done ✅\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  y_train:\", y_train.shape, np.unique(y_train, return_counts=True))\n",
    "print(\"  X_test :\", X_test.shape)\n",
    "print(\"  y_test :\", y_test.shape, np.unique(y_test, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b4ffd5-bf2f-490e-8854-0a2fc747f1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e69ba7-3952-47b1-bd80-f05237b2e018",
   "metadata": {},
   "source": [
    "Cell 6 — Split train/val + Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97fda4ae-e1c8-4fd1-bf0c-c82ae2a85764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 6] Normalizing using ALL training data (no train/val split)...\n",
      "[STEP 6] Normalization done ✅\n",
      "  X_train_norm: (5100, 14, 128, 1)\n",
      "  X_test_norm : (300, 14, 128, 1)\n",
      "  y_train_norm: (5100,)\n",
      "  y_test_norm : (300,)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 6: Normalize (ALL training data)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 6] Normalizing using ALL training data (no train/val split)...\")\n",
    "\n",
    "def normalize_eeg(train_data: np.ndarray, test_data: np.ndarray):\n",
    "    if train_data.ndim == 5:\n",
    "        train_flat = train_data.reshape(-1, *train_data.shape[2:])\n",
    "        test_flat  = test_data.reshape(-1, *test_data.shape[2:])\n",
    "    else:\n",
    "        train_flat, test_flat = train_data, test_data\n",
    "\n",
    "    # Compute stats on ALL training data\n",
    "    train_mean = np.mean(train_flat, axis=(0, 2, 3), keepdims=True)\n",
    "    train_std  = np.std(train_flat,  axis=(0, 2, 3), keepdims=True) + 1e-8\n",
    "\n",
    "    train_norm = (train_flat - train_mean) / train_std\n",
    "    test_norm  = (test_flat  - train_mean) / train_std\n",
    "\n",
    "    if train_data.ndim == 5:\n",
    "        train_norm = train_norm.reshape(train_data.shape)\n",
    "        test_norm  = test_norm.reshape(test_data.shape)\n",
    "\n",
    "    return (\n",
    "        train_norm.astype(np.float32),\n",
    "        test_norm.astype(np.float32),\n",
    "        train_mean,\n",
    "        train_std\n",
    "    )\n",
    "\n",
    "# Normalize ALL training data\n",
    "X_train_norm, X_test_norm, mean, std = normalize_eeg(X_train, X_test)\n",
    "\n",
    "y_train_norm = y_train.astype(np.float32)\n",
    "y_test_norm  = y_test.astype(np.float32)\n",
    "\n",
    "print(\"[STEP 6] Normalization done ✅\")\n",
    "print(\"  X_train_norm:\", X_train_norm.shape)\n",
    "print(\"  X_test_norm :\", X_test_norm.shape)\n",
    "print(\"  y_train_norm:\", y_train_norm.shape)\n",
    "print(\"  y_test_norm :\", y_test_norm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f527c-34db-4c23-bdbd-c8ce03b519f0",
   "metadata": {},
   "source": [
    "Cell 7 — EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ea4279-53c2-49c1-b834-7b1e1c20775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\DL\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Xtr: (5100, 14, 128, 1), ytr: (5100,)\n",
      "[INFO] Test / LOSO subject is NOT used here.\n",
      "[SPLIT] Train: (4080, 14, 128, 1), Val: (1020, 14, 128, 1)\n",
      "Epoch 1/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 126ms/step - accuracy: 0.5348 - loss: 0.7238 - val_accuracy: 0.4716 - val_loss: 0.6955 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.5392 - loss: 0.6892 - val_accuracy: 0.4755 - val_loss: 0.6949 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.5679 - loss: 0.6645 - val_accuracy: 0.4922 - val_loss: 0.6922 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.6074 - loss: 0.6499 - val_accuracy: 0.5206 - val_loss: 0.6882 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.6216 - loss: 0.6283 - val_accuracy: 0.5627 - val_loss: 0.6829 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.6404 - loss: 0.6074 - val_accuracy: 0.5892 - val_loss: 0.6733 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.6863 - loss: 0.5822 - val_accuracy: 0.6490 - val_loss: 0.6578 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.7039 - loss: 0.5606 - val_accuracy: 0.6882 - val_loss: 0.6371 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.7164 - loss: 0.5450 - val_accuracy: 0.6912 - val_loss: 0.6196 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7328 - loss: 0.5223 - val_accuracy: 0.6451 - val_loss: 0.6160 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7522 - loss: 0.4976 - val_accuracy: 0.6461 - val_loss: 0.6044 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.7831 - loss: 0.4594 - val_accuracy: 0.6373 - val_loss: 0.6010 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 125ms/step - accuracy: 0.7985 - loss: 0.4430 - val_accuracy: 0.6353 - val_loss: 0.6013 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.8140 - loss: 0.4206 - val_accuracy: 0.6363 - val_loss: 0.6189 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.8083 - loss: 0.4275 - val_accuracy: 0.6676 - val_loss: 0.5720 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.8145 - loss: 0.4079 - val_accuracy: 0.6529 - val_loss: 0.5859 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.8218 - loss: 0.3904 - val_accuracy: 0.6706 - val_loss: 0.5636 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8257 - loss: 0.3940 - val_accuracy: 0.6912 - val_loss: 0.5435 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.8493 - loss: 0.3590 - val_accuracy: 0.6824 - val_loss: 0.5358 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8483 - loss: 0.3562 - val_accuracy: 0.7059 - val_loss: 0.5095 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8659 - loss: 0.3228 - val_accuracy: 0.7539 - val_loss: 0.4268 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.8694 - loss: 0.3064 - val_accuracy: 0.7245 - val_loss: 0.4904 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.8657 - loss: 0.3209 - val_accuracy: 0.7716 - val_loss: 0.4352 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.8748 - loss: 0.3093 - val_accuracy: 0.7627 - val_loss: 0.4353 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.8873 - loss: 0.2850 - val_accuracy: 0.7931 - val_loss: 0.3768 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.8846 - loss: 0.2884 - val_accuracy: 0.8088 - val_loss: 0.3765 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.8931 - loss: 0.2690 - val_accuracy: 0.8010 - val_loss: 0.3956 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8821 - loss: 0.2951 - val_accuracy: 0.8078 - val_loss: 0.3770 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8998 - loss: 0.2606 - val_accuracy: 0.7912 - val_loss: 0.4015 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8951 - loss: 0.2537 - val_accuracy: 0.8667 - val_loss: 0.2802 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9081 - loss: 0.2347 - val_accuracy: 0.8706 - val_loss: 0.2736 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.9029 - loss: 0.2478 - val_accuracy: 0.8902 - val_loss: 0.2627 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.9000 - loss: 0.2506 - val_accuracy: 0.9422 - val_loss: 0.1632 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.9127 - loss: 0.2157 - val_accuracy: 0.9402 - val_loss: 0.1676 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9213 - loss: 0.2026 - val_accuracy: 0.9588 - val_loss: 0.1398 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9096 - loss: 0.2258 - val_accuracy: 0.9422 - val_loss: 0.1495 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9208 - loss: 0.2025 - val_accuracy: 0.9618 - val_loss: 0.1316 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9282 - loss: 0.1964 - val_accuracy: 0.9608 - val_loss: 0.1223 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9250 - loss: 0.1897 - val_accuracy: 0.9676 - val_loss: 0.1131 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9255 - loss: 0.2005 - val_accuracy: 0.9588 - val_loss: 0.1178 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9235 - loss: 0.2015 - val_accuracy: 0.9647 - val_loss: 0.1189 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9265 - loss: 0.1853 - val_accuracy: 0.9618 - val_loss: 0.1068 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9316 - loss: 0.1824 - val_accuracy: 0.9569 - val_loss: 0.1204 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9245 - loss: 0.1879 - val_accuracy: 0.9608 - val_loss: 0.1186 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9301 - loss: 0.1846 - val_accuracy: 0.9657 - val_loss: 0.1119 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9331 - loss: 0.1824 - val_accuracy: 0.9569 - val_loss: 0.1295 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9169 - loss: 0.2171\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9186 - loss: 0.2164 - val_accuracy: 0.9510 - val_loss: 0.1268 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9370 - loss: 0.1579 - val_accuracy: 0.9588 - val_loss: 0.1128 - learning_rate: 5.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9414 - loss: 0.1529 - val_accuracy: 0.9618 - val_loss: 0.1060 - learning_rate: 5.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9353 - loss: 0.1569 - val_accuracy: 0.9608 - val_loss: 0.1055 - learning_rate: 5.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9387 - loss: 0.1540 - val_accuracy: 0.9657 - val_loss: 0.1002 - learning_rate: 5.0000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9336 - loss: 0.1681 - val_accuracy: 0.9667 - val_loss: 0.0972 - learning_rate: 5.0000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9478 - loss: 0.1424 - val_accuracy: 0.9657 - val_loss: 0.0986 - learning_rate: 5.0000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9309 - loss: 0.1718 - val_accuracy: 0.9637 - val_loss: 0.0988 - learning_rate: 5.0000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9353 - loss: 0.1668 - val_accuracy: 0.9647 - val_loss: 0.0987 - learning_rate: 5.0000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9370 - loss: 0.1666 - val_accuracy: 0.9657 - val_loss: 0.0981 - learning_rate: 5.0000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9328 - loss: 0.1638\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9373 - loss: 0.1592 - val_accuracy: 0.9637 - val_loss: 0.0993 - learning_rate: 5.0000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9328 - loss: 0.1695 - val_accuracy: 0.9676 - val_loss: 0.0983 - learning_rate: 2.5000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9377 - loss: 0.1628 - val_accuracy: 0.9657 - val_loss: 0.0994 - learning_rate: 2.5000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9436 - loss: 0.1546 - val_accuracy: 0.9647 - val_loss: 0.0981 - learning_rate: 2.5000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.9445 - loss: 0.1564\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9404 - loss: 0.1585 - val_accuracy: 0.9637 - val_loss: 0.0987 - learning_rate: 2.5000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9451 - loss: 0.1420 - val_accuracy: 0.9657 - val_loss: 0.0969 - learning_rate: 1.2500e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9483 - loss: 0.1454 - val_accuracy: 0.9676 - val_loss: 0.0938 - learning_rate: 1.2500e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9439 - loss: 0.1522 - val_accuracy: 0.9686 - val_loss: 0.0928 - learning_rate: 1.2500e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 125ms/step - accuracy: 0.9414 - loss: 0.1509 - val_accuracy: 0.9676 - val_loss: 0.0939 - learning_rate: 1.2500e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9427 - loss: 0.1447\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9392 - loss: 0.1538 - val_accuracy: 0.9647 - val_loss: 0.0952 - learning_rate: 1.2500e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9451 - loss: 0.1422 - val_accuracy: 0.9637 - val_loss: 0.0958 - learning_rate: 6.2500e-05\n",
      "Epoch 68/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9466 - loss: 0.1430 - val_accuracy: 0.9647 - val_loss: 0.0953 - learning_rate: 6.2500e-05\n",
      "Epoch 69/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9441 - loss: 0.1464 - val_accuracy: 0.9647 - val_loss: 0.0949 - learning_rate: 6.2500e-05\n",
      "Epoch 70/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9418 - loss: 0.1530\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9417 - loss: 0.1500 - val_accuracy: 0.9667 - val_loss: 0.0950 - learning_rate: 6.2500e-05\n",
      "Epoch 71/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9414 - loss: 0.1467 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 3.1250e-05\n",
      "Epoch 72/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9431 - loss: 0.1426 - val_accuracy: 0.9667 - val_loss: 0.0944 - learning_rate: 3.1250e-05\n",
      "Epoch 73/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9475 - loss: 0.1412 - val_accuracy: 0.9667 - val_loss: 0.0944 - learning_rate: 3.1250e-05\n",
      "Epoch 74/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9422 - loss: 0.1504 - val_accuracy: 0.9667 - val_loss: 0.0938 - learning_rate: 3.1250e-05\n",
      "Epoch 75/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9456 - loss: 0.1359 - val_accuracy: 0.9667 - val_loss: 0.0935 - learning_rate: 3.1250e-05\n",
      "Epoch 76/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9436 - loss: 0.1393 - val_accuracy: 0.9657 - val_loss: 0.0940 - learning_rate: 3.1250e-05\n",
      "Epoch 77/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9453 - loss: 0.1508 - val_accuracy: 0.9676 - val_loss: 0.0928 - learning_rate: 3.1250e-05\n",
      "Epoch 78/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9422 - loss: 0.1560 - val_accuracy: 0.9676 - val_loss: 0.0927 - learning_rate: 3.1250e-05\n",
      "Epoch 79/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9426 - loss: 0.1501\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9407 - loss: 0.1518 - val_accuracy: 0.9676 - val_loss: 0.0930 - learning_rate: 3.1250e-05\n",
      "Epoch 80/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9434 - loss: 0.1469 - val_accuracy: 0.9676 - val_loss: 0.0930 - learning_rate: 1.5625e-05\n",
      "Epoch 81/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9478 - loss: 0.1390 - val_accuracy: 0.9667 - val_loss: 0.0924 - learning_rate: 1.5625e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9429 - loss: 0.1466 - val_accuracy: 0.9667 - val_loss: 0.0926 - learning_rate: 1.5625e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9414 - loss: 0.1541\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9446 - loss: 0.1467 - val_accuracy: 0.9667 - val_loss: 0.0925 - learning_rate: 1.5625e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9458 - loss: 0.1451 - val_accuracy: 0.9667 - val_loss: 0.0926 - learning_rate: 7.8125e-06\n",
      "Epoch 85/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9387 - loss: 0.1555 - val_accuracy: 0.9676 - val_loss: 0.0925 - learning_rate: 7.8125e-06\n",
      "Epoch 86/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9451 - loss: 0.1421 - val_accuracy: 0.9676 - val_loss: 0.0924 - learning_rate: 7.8125e-06\n",
      "Epoch 87/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9405 - loss: 0.1480\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9404 - loss: 0.1503 - val_accuracy: 0.9676 - val_loss: 0.0923 - learning_rate: 7.8125e-06\n",
      "Epoch 88/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9446 - loss: 0.1394 - val_accuracy: 0.9676 - val_loss: 0.0923 - learning_rate: 3.9063e-06\n",
      "Epoch 89/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9446 - loss: 0.1422 - val_accuracy: 0.9676 - val_loss: 0.0922 - learning_rate: 3.9063e-06\n",
      "Epoch 90/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9417 - loss: 0.1520 - val_accuracy: 0.9676 - val_loss: 0.0922 - learning_rate: 3.9063e-06\n",
      "Epoch 91/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9419 - loss: 0.1493\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9414 - loss: 0.1472 - val_accuracy: 0.9676 - val_loss: 0.0922 - learning_rate: 3.9063e-06\n",
      "Epoch 92/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9419 - loss: 0.1454 - val_accuracy: 0.9676 - val_loss: 0.0921 - learning_rate: 1.9531e-06\n",
      "Epoch 93/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9424 - loss: 0.1471 - val_accuracy: 0.9676 - val_loss: 0.0920 - learning_rate: 1.9531e-06\n",
      "Epoch 94/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9461 - loss: 0.1442 - val_accuracy: 0.9676 - val_loss: 0.0921 - learning_rate: 1.9531e-06\n",
      "Epoch 95/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9484 - loss: 0.1445\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9451 - loss: 0.1459 - val_accuracy: 0.9676 - val_loss: 0.0923 - learning_rate: 1.9531e-06\n",
      "Epoch 96/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9412 - loss: 0.1505 - val_accuracy: 0.9676 - val_loss: 0.0923 - learning_rate: 1.0000e-06\n",
      "Epoch 97/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9475 - loss: 0.1392 - val_accuracy: 0.9676 - val_loss: 0.0923 - learning_rate: 1.0000e-06\n",
      "Epoch 98/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9483 - loss: 0.1397 - val_accuracy: 0.9676 - val_loss: 0.0922 - learning_rate: 1.0000e-06\n",
      "Epoch 99/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9395 - loss: 0.1511 - val_accuracy: 0.9676 - val_loss: 0.0921 - learning_rate: 1.0000e-06\n",
      "Epoch 100/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9485 - loss: 0.1408 - val_accuracy: 0.9676 - val_loss: 0.0922 - learning_rate: 1.0000e-06\n",
      "Epoch 101/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9478 - loss: 0.1476 - val_accuracy: 0.9676 - val_loss: 0.0921 - learning_rate: 1.0000e-06\n",
      "Epoch 102/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9458 - loss: 0.1447 - val_accuracy: 0.9676 - val_loss: 0.0922 - learning_rate: 1.0000e-06\n",
      "Epoch 103/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9429 - loss: 0.1408 - val_accuracy: 0.9676 - val_loss: 0.0920 - learning_rate: 1.0000e-06\n",
      "Epoch 104/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9419 - loss: 0.1495 - val_accuracy: 0.9676 - val_loss: 0.0920 - learning_rate: 1.0000e-06\n",
      "Epoch 105/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9483 - loss: 0.1402 - val_accuracy: 0.9676 - val_loss: 0.0919 - learning_rate: 1.0000e-06\n",
      "Epoch 106/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9439 - loss: 0.1462 - val_accuracy: 0.9676 - val_loss: 0.0919 - learning_rate: 1.0000e-06\n",
      "Epoch 107/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9478 - loss: 0.1409 - val_accuracy: 0.9676 - val_loss: 0.0919 - learning_rate: 1.0000e-06\n",
      "Epoch 108/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9434 - loss: 0.1450 - val_accuracy: 0.9676 - val_loss: 0.0920 - learning_rate: 1.0000e-06\n",
      "Epoch 109/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9444 - loss: 0.1453 - val_accuracy: 0.9676 - val_loss: 0.0919 - learning_rate: 1.0000e-06\n",
      "Epoch 110/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9473 - loss: 0.1396 - val_accuracy: 0.9676 - val_loss: 0.0918 - learning_rate: 1.0000e-06\n",
      "Epoch 111/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9453 - loss: 0.1419 - val_accuracy: 0.9676 - val_loss: 0.0917 - learning_rate: 1.0000e-06\n",
      "Epoch 112/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9461 - loss: 0.1404 - val_accuracy: 0.9676 - val_loss: 0.0918 - learning_rate: 1.0000e-06\n",
      "Epoch 113/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9493 - loss: 0.1407 - val_accuracy: 0.9676 - val_loss: 0.0919 - learning_rate: 1.0000e-06\n",
      "Epoch 114/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9429 - loss: 0.1449 - val_accuracy: 0.9676 - val_loss: 0.0919 - learning_rate: 1.0000e-06\n",
      "Epoch 115/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9493 - loss: 0.1357 - val_accuracy: 0.9676 - val_loss: 0.0919 - learning_rate: 1.0000e-06\n",
      "Epoch 116/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9434 - loss: 0.1466 - val_accuracy: 0.9676 - val_loss: 0.0920 - learning_rate: 1.0000e-06\n",
      "Epoch 117/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9451 - loss: 0.1505 - val_accuracy: 0.9676 - val_loss: 0.0921 - learning_rate: 1.0000e-06\n",
      "Epoch 118/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9466 - loss: 0.1493 - val_accuracy: 0.9676 - val_loss: 0.0921 - learning_rate: 1.0000e-06\n",
      "Epoch 119/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9446 - loss: 0.1423 - val_accuracy: 0.9676 - val_loss: 0.0921 - learning_rate: 1.0000e-06\n",
      "Epoch 120/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9498 - loss: 0.1423 - val_accuracy: 0.9676 - val_loss: 0.0921 - learning_rate: 1.0000e-06\n",
      "Epoch 121/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9458 - loss: 0.1455 - val_accuracy: 0.9676 - val_loss: 0.0919 - learning_rate: 1.0000e-06\n",
      "Epoch 122/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9456 - loss: 0.1434 - val_accuracy: 0.9676 - val_loss: 0.0920 - learning_rate: 1.0000e-06\n",
      "Epoch 123/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9456 - loss: 0.1459 - val_accuracy: 0.9676 - val_loss: 0.0920 - learning_rate: 1.0000e-06\n",
      "Epoch 124/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9461 - loss: 0.1444 - val_accuracy: 0.9676 - val_loss: 0.0920 - learning_rate: 1.0000e-06\n",
      "Epoch 125/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9392 - loss: 0.1491 - val_accuracy: 0.9676 - val_loss: 0.0919 - learning_rate: 1.0000e-06\n",
      "Epoch 126/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9431 - loss: 0.1427 - val_accuracy: 0.9676 - val_loss: 0.0919 - learning_rate: 1.0000e-06\n",
      "Epoch 126: early stopping\n",
      "Restoring model weights from the end of the best epoch: 111.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] Model saved as cole-final.h5\n",
      "\n",
      "[CONFUSION MATRIX]\n",
      "[[468  12]\n",
      " [ 21 519]]\n",
      "[VAL] acc=0.9676, prec=0.9774, rec=0.9611, f1=0.9692\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 4: 80:20 Train / Validation Split (EEGNet)\n",
    "# ====================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, SeparableConv2D, BatchNormalization, Activation,\n",
    "    DepthwiseConv2D, Conv2D, AveragePooling2D,\n",
    "    Dropout, Dense, Flatten\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# --------------------------------------------------------------------\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# EEGNet model\n",
    "# --------------------------------------------------------------------\n",
    "def create_eegnet_model(input_shape, dropout_rate=0.5, num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(16, (1, 64), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = DepthwiseConv2D(\n",
    "        (input_shape[0], 1),\n",
    "        depth_multiplier=2,\n",
    "        padding='valid',\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 4))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = SeparableConv2D(16, (1, 16), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 8))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Compile model\n",
    "# --------------------------------------------------------------------\n",
    "def compile_model():\n",
    "    input_shape = (Xtr.shape[1], Xtr.shape[2], Xtr.shape[3])\n",
    "    model = create_eegnet_model(input_shape)\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-3),\n",
    "        loss=BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[BinaryAccuracy(name=\"accuracy\", threshold=0.5)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def make_callbacks():\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"loss\",\n",
    "            factor=0.5,\n",
    "            patience=4,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Use ALL normalized training data\n",
    "# --------------------------------------------------------------------\n",
    "Xtr = X_train_norm\n",
    "ytr = y_train_norm\n",
    "\n",
    "print(f\"[TRAIN] Xtr: {Xtr.shape}, ytr: {ytr.shape}\")\n",
    "print(\"[INFO] Test / LOSO subject is NOT used here.\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 80:20 Stratified Split\n",
    "# --------------------------------------------------------------------\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(sss.split(Xtr, ytr))\n",
    "\n",
    "X_train, X_val = Xtr[train_idx], Xtr[val_idx]\n",
    "y_train, y_val = ytr[train_idx], ytr[val_idx]\n",
    "\n",
    "print(f\"[SPLIT] Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Train model\n",
    "# --------------------------------------------------------------------\n",
    "model = compile_model()\n",
    "callbacks = make_callbacks()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Save final model\n",
    "# --------------------------------------------------------------------\n",
    "model.save(\"mina-final.h5\")\n",
    "print(\"[SAVE] Model saved as cole-final.h5\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Validation Confusion Matrix\n",
    "# --------------------------------------------------------------------\n",
    "y_prob = model.predict(X_val, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "y_true = y_val.astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "eps = 1e-12\n",
    "\n",
    "acc  = (tp + tn) / max(tp + tn + fp + fn, eps)\n",
    "prec = tp / max(tp + fp, eps)\n",
    "rec  = tp / max(tp + fn, eps)\n",
    "f1   = 2 * prec * rec / max(prec + rec, eps)\n",
    "\n",
    "print(\"\\n[CONFUSION MATRIX]\")\n",
    "print(cm)\n",
    "print(f\"[VAL] acc={acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e2a13-6367-41ca-be66-a43594c2940e",
   "metadata": {},
   "source": [
    "Cell 8 — Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ac6662-33c1-4e25-b6c5-d47c860fba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Preparing TEST data...\n",
      "[TEST] Xte shape: (300, 14, 128, 1)\n",
      "[TEST] yte shape: (300,), class distribution: (array([0]), array([300]))\n",
      "[TEST] Loading final model: mina-final.h5\n",
      "[TEST] Running inference on TEST subject...\n",
      "[TEST] Mean predicted probability: 0.0301\n",
      "\n",
      "[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[295   5]\n",
      " [  0   0]]\n",
      "Accuracy : 0.9833\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-score : 0.0000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Inference on TEST subject (adam) using FINAL EEGNet\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Prepare TEST data\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Preparing TEST data...\")\n",
    "\n",
    "Xte = X_test_norm\n",
    "yte = y_test_norm.astype(int)\n",
    "\n",
    "print(f\"[TEST] Xte shape: {Xte.shape}\")\n",
    "print(f\"[TEST] yte shape: {yte.shape}, class distribution: {np.unique(yte, return_counts=True)}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Load FINAL trained model\n",
    "# ------------------------------------------------------------------\n",
    "model_path = \"mina-final.h5\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        \"Final model 'cole-final.h5' not found. \"\n",
    "        \"Please run Cell 4 (80:20 training) first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[TEST] Loading final model: {model_path}\")\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Run inference on TEST subject\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Running inference on TEST subject...\")\n",
    "\n",
    "y_prob = model.predict(Xte, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(f\"[TEST] Mean predicted probability: {y_prob.mean():.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Metrics on TEST subject (segment-level)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc = accuracy_score(yte, y_pred)\n",
    "prec = precision_score(yte, y_pred, zero_division=0)\n",
    "rec = recall_score(yte, y_pred, zero_division=0)\n",
    "f1 = f1_score(yte, y_pred, zero_division=0)\n",
    "\n",
    "print(\"\\n[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572dbf3e-bbb5-4865-883f-30e374700949",
   "metadata": {},
   "source": [
    "Cell 9 - Performance Matrices for each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a57240-50ea-4736-a7ac-52ef7878b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] Verifying predictions and labels are available...\n",
      "[CHECK] Found predictions for 300 test segments.\n",
      "\n",
      "[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[295   5]\n",
      " [  0   0]]\n",
      "Accuracy : 0.9833\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-score : 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Final model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================\n",
    "# Cell 6: Confusion Matrix & Metrics on TEST (FINAL EEGNet)\n",
    "# =======================================================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# ------------------------------------------------------------------\n",
    "def compute_metrics_from_cm(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    eps = 1e-12\n",
    "    acc  = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec  = tp / (tp + fn + eps)\n",
    "    f1   = 2 * prec * rec / (prec + rec + eps)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], ['0', '1'])\n",
    "    plt.yticks([0, 1], ['0', '1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(\n",
    "                j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Validate upstream artifacts\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[CHECK] Verifying predictions and labels are available...\")\n",
    "\n",
    "if 'y_pred' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Predictions 'y_pred' not found. \"\n",
    "        \"Please run Cell 5 (final model inference) first.\"\n",
    "    )\n",
    "\n",
    "if 'yte' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Test labels 'yte' not found. \"\n",
    "        \"Please run Cell 5 first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[CHECK] Found predictions for {len(y_pred)} test segments.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Confusion Matrix & Metrics (FINAL model)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc, prec, rec, f1 = compute_metrics_from_cm(cm)\n",
    "\n",
    "print(\"\\n[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(cm, title=\"Confusion Matrix - FINAL EEGNet\")\n",
    "\n",
    "print(\"[DONE] Final model evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0f120-39fd-497e-9098-05e6c82f4807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
