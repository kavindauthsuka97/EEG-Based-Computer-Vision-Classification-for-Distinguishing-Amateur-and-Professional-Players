{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af5b471-cea9-4194-bc23-da5cdc2f9c67",
   "metadata": {},
   "source": [
    "Cell 1 — Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64aaa897-ab61-4eb4-a558-4c078acf751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Base path set to:\n",
      "C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\n",
      "[INFO] Found 18 folders:\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Selected test subject (LOSO): Cole\n",
      "[INFO] Training subjects (17):\n",
      "['Amin', 'Amin1', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Sampling rate (SFREQ) = 128.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 1: Setup & discovery\n",
    "# =========================\n",
    "\n",
    "import os  # file ops\n",
    "import glob  # file matching\n",
    "import numpy as np  # arrays\n",
    "import random  # seeds\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data path\n",
    "BASE_PATH = r\"C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\"\n",
    "print(f\"[INFO] Base path set to:\\n{BASE_PATH}\")\n",
    "\n",
    "# List subject folders\n",
    "all_folders = [p for p in sorted(os.listdir(BASE_PATH)) if os.path.isdir(os.path.join(BASE_PATH, p))]\n",
    "print(f\"[INFO] Found {len(all_folders)} folders:\\n{all_folders}\")\n",
    "\n",
    "# Expected count (change if needed)\n",
    "assert len(all_folders) == 18, f\"Expected 18 folders, found {len(all_folders)}.\"\n",
    "\n",
    "# LOSO test subject\n",
    "TEST_SUBJECT = \"Cole\"\n",
    "print(f\"[INFO] Selected test subject (LOSO): {TEST_SUBJECT}\")\n",
    "\n",
    "# Train subjects = remaining\n",
    "train_subjects = [s for s in all_folders if s.lower() != TEST_SUBJECT.lower()]\n",
    "print(f\"[INFO] Training subjects ({len(train_subjects)}):\\n{train_subjects}\")\n",
    "\n",
    "# Sampling rate\n",
    "SFREQ = 128.0\n",
    "print(f\"[INFO] Sampling rate (SFREQ) = {SFREQ} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e649681-2861-4ce2-bbd1-0d9dd3359f42",
   "metadata": {},
   "source": [
    "Cell 2 — Load EEG + labels per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943c5c6e-a7aa-4618-899c-daf341c25358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 2] Loading data from folders...\n",
      "[STEP 2]       Amin | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Amin | EEG=          Amin.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Amin1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]      Amin1 | EEG=         Amin1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Cole | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Cole | EEG=          Cole.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Daniel | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]     Daniel | EEG=        Daniel.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]    Ismayil | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]    Ismayil | EEG=       Ismayil.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Jack | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Jack | EEG=          Jack.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]      James | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      James | EEG=         James.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Josh | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Josh | EEG=          Josh.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Marjan | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Marjan | EEG=        Marjan.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]        Max | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Max | EEG=           Max.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Mina | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mina | EEG=          Mina.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 1 | EEG=        Mina 1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 3 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 3 | EEG=        Mina 3.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]   Mohammad | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]   Mohammad | EEG=      Mohammad.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Mona | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mona | EEG=          Mona.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Roddy | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      Roddy | EEG=         Roddy.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]        Sam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Sam | EEG=           Sam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       adam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       adam | EEG=          adam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2] Loading complete ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 2: Load data (EMPTY LABEL => CLASS 0)\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path  # helper\n",
    "\n",
    "def load_subject(subj_dir: str, subj_name: str, expected_channels: int = 14):\n",
    "    \"\"\"\n",
    "    EEG file: {subj_name}.npy\n",
    "    Label file: {subj_name}_target.npy\n",
    "    Label rule:\n",
    "      - empty array -> class 0\n",
    "      - [0] -> class 0\n",
    "      - [1] -> class 1\n",
    "    \"\"\"\n",
    "\n",
    "    # EEG file path\n",
    "    eeg_file = os.path.join(subj_dir, f\"{subj_name}.npy\")\n",
    "    assert os.path.exists(eeg_file), f\"[ERROR] Missing EEG file: {eeg_file}\"\n",
    "\n",
    "    # Label file path\n",
    "    target_file = os.path.join(subj_dir, f\"{subj_name}_target.npy\")\n",
    "    assert os.path.exists(target_file), f\"[ERROR] Missing label file: {target_file}\"\n",
    "\n",
    "    # Load EEG\n",
    "    X_raw = np.load(eeg_file, allow_pickle=True)\n",
    "    X_raw = np.asarray(X_raw)\n",
    "\n",
    "    # Standardize EEG to (n_trials, 14, T)\n",
    "    if X_raw.ndim == 2:\n",
    "        X_trials = X_raw[None, :, :]\n",
    "    elif X_raw.ndim == 3:\n",
    "        dims = list(X_raw.shape)\n",
    "        chan_axes = [i for i, d in enumerate(dims) if d == expected_channels]\n",
    "        chan_axis = chan_axes[0] if len(chan_axes) == 1 else 1\n",
    "        time_axis = int(np.argmax(dims))\n",
    "        trial_axis = [i for i in range(3) if i not in (chan_axis, time_axis)][0]\n",
    "        X_trials = np.moveaxis(X_raw, (trial_axis, chan_axis, time_axis), (0, 1, 2))\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] Unsupported EEG shape {X_raw.shape} in {eeg_file}\")\n",
    "\n",
    "    assert X_trials.shape[1] == expected_channels, f\"[ERROR] Expected {expected_channels} channels, got {X_trials.shape[1]} in {eeg_file}\"\n",
    "\n",
    "    # Load label\n",
    "    y_raw = np.load(target_file, allow_pickle=True)\n",
    "    y_arr = np.asarray(y_raw).ravel()\n",
    "\n",
    "    # Apply your rule\n",
    "    if y_arr.size == 0:\n",
    "        label = 0\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\")\n",
    "    else:\n",
    "        label = int(y_arr[0])\n",
    "        if label not in (0, 1):\n",
    "            raise ValueError(f\"[ERROR] Invalid label {label} in {target_file} (must be 0 or 1)\")\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape={y_arr.shape}, value={y_arr} -> class={label}\")\n",
    "\n",
    "    # Repeat label for all trials\n",
    "    y_trials = np.full((X_trials.shape[0],), label, dtype=np.int32)\n",
    "\n",
    "    return X_trials.astype(np.float32), y_trials.astype(np.int32), eeg_file, target_file\n",
    "\n",
    "\n",
    "# Load all subjects\n",
    "subject_data = {}\n",
    "print(\"[STEP 2] Loading data from folders...\")\n",
    "\n",
    "for subj in all_folders:\n",
    "    subj_dir = os.path.join(BASE_PATH, subj)\n",
    "    X_trials, y_trials, eeg_file, target_file = load_subject(subj_dir, subj)\n",
    "\n",
    "    subject_data[subj] = (X_trials, y_trials)\n",
    "\n",
    "    print(f\"[STEP 2] {subj:>10} | EEG={Path(eeg_file).name:>18} | X={X_trials.shape} | y={y_trials.shape} | class={y_trials[0]}\")\n",
    "\n",
    "print(\"[STEP 2] Loading complete ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd93ca8-a94c-485c-8a61-e6e7441f6299",
   "metadata": {},
   "source": [
    "Cell 3 — Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733e592c-98e1-4cbd-afd2-6fdc8cf407f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 3] Preprocessing functions ready ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3: Preprocessing functions\n",
    "# =========================\n",
    "\n",
    "import mne  # EEG processing\n",
    "import pywt  # wavelets\n",
    "from sklearn.decomposition import FastICA  # ICA\n",
    "from typing import Optional, Tuple, Union, Sequence, Dict, List  # typing\n",
    "\n",
    "def wavelet_enhanced_ica(\n",
    "    data: np.ndarray,\n",
    "    n_components: int = 10,\n",
    "    wavelet: str = \"db4\",\n",
    "    level: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> np.ndarray:\n",
    "    n_ch, n_t = data.shape\n",
    "    n_components = min(n_components, n_ch)\n",
    "\n",
    "    coeffs = pywt.wavedec(data, wavelet=wavelet, level=level, axis=1)\n",
    "    A = coeffs[0]\n",
    "\n",
    "    ica = FastICA(n_components=n_components, random_state=random_state)\n",
    "    S = ica.fit_transform(A.T).T\n",
    "    A_denoised = ica.inverse_transform(S.T).T\n",
    "\n",
    "    coeffs[0] = A_denoised\n",
    "    cleaned = pywt.waverec(coeffs, wavelet=wavelet, axis=1)\n",
    "\n",
    "    if cleaned.shape[1] != n_t:\n",
    "        if cleaned.shape[1] > n_t:\n",
    "            cleaned = cleaned[:, :n_t]\n",
    "        else:\n",
    "            cleaned = np.pad(cleaned, ((0, 0), (0, n_t - cleaned.shape[1])), mode=\"constant\")\n",
    "    return cleaned\n",
    "\n",
    "def _names_from_index_mapping(n_channels: int, index_to_name: Optional[Dict[int, str]]) -> List[str]:\n",
    "    if index_to_name is None:\n",
    "        return [f\"EEG{i+1}\" for i in range(n_channels)]\n",
    "    keys = list(index_to_name.keys())\n",
    "    is_zero_based = (0 in keys) and (1 not in keys)\n",
    "    names = []\n",
    "    for i in range(n_channels):\n",
    "        key = i if is_zero_based else (i + 1)\n",
    "        names.append(index_to_name.get(key, f\"EEG{i+1}\"))\n",
    "    return names\n",
    "\n",
    "def preprocess_eeg(\n",
    "    eeg: np.ndarray,\n",
    "    sfreq: float,\n",
    "    *,\n",
    "    index_to_name: Optional[Dict[int, str]] = None,\n",
    "    use_standard_1010: bool = True,\n",
    "    resample_to: Optional[float] = None,\n",
    "    notch_freqs: Union[None, float, Sequence[float]] = 50.0,\n",
    "    highpass: Optional[float] = 0.05,\n",
    "    bad_point_z: float = 6.0,\n",
    "    bad_channel_z: float = 5.0,\n",
    "    interpolate_bad_channels: bool = True,\n",
    "    car: bool = True,\n",
    "    use_wica: bool = True,\n",
    "    wica_components: int = 10,\n",
    "    wica_wavelet: str = \"db4\",\n",
    "    wica_level: int = 3,\n",
    "    wica_random_state: int = 42,\n",
    "    return_raw: bool = False,\n",
    "):\n",
    "\n",
    "    eeg = np.asarray(eeg, dtype=np.float32)\n",
    "    assert eeg.ndim == 2, \"eeg must be 2D: (n_channels, n_times)\"\n",
    "\n",
    "    n_channels, _ = eeg.shape\n",
    "    ch_names = _names_from_index_mapping(n_channels, index_to_name)\n",
    "    ch_types = ['eog' if str(n).upper().startswith(\"EOG\") else 'eeg' for n in ch_names]\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    raw = mne.io.RawArray(eeg, info, verbose=False)\n",
    "\n",
    "    montage_applied = False\n",
    "    if use_standard_1010:\n",
    "        try:\n",
    "            mont = mne.channels.make_standard_montage(\"standard_1010\")\n",
    "            raw.set_montage(mont, match_case=False, on_missing=\"ignore\")\n",
    "            montage_applied = True\n",
    "        except Exception:\n",
    "            montage_applied = False\n",
    "\n",
    "    if resample_to is not None and float(resample_to) != float(sfreq):\n",
    "        raw.resample(sfreq=resample_to, npad=\"auto\")\n",
    "    sfreq_out = float(raw.info[\"sfreq\"])\n",
    "\n",
    "    if notch_freqs is not None:\n",
    "        raw.notch_filter(freqs=notch_freqs, verbose=False)\n",
    "    if highpass is not None:\n",
    "        raw.filter(l_freq=highpass, h_freq=None, verbose=False)\n",
    "\n",
    "    X = raw.get_data()\n",
    "    mu = np.mean(X, axis=1, keepdims=True)\n",
    "    sd = np.std(X, axis=1, keepdims=True) + 1e-12\n",
    "    hi = mu + bad_point_z * sd\n",
    "    lo = mu - bad_point_z * sd\n",
    "    bad_idx = (X > hi) | (X < lo)\n",
    "\n",
    "    if np.any(bad_idx):\n",
    "        X_fixed = X.copy()\n",
    "        t = np.arange(X.shape[1], dtype=float)\n",
    "        for ch in range(n_channels):\n",
    "            mask = bad_idx[ch]\n",
    "            if mask.any():\n",
    "                good = ~mask\n",
    "                if good.sum() >= 2:\n",
    "                    X_fixed[ch, mask] = np.interp(t[mask], t[good], X_fixed[ch, good])\n",
    "        raw._data = X_fixed\n",
    "\n",
    "    if interpolate_bad_channels and montage_applied:\n",
    "        X = raw.get_data(picks=\"eeg\")\n",
    "        if X.size > 0:\n",
    "            ch_std = X.std(axis=1)\n",
    "            med = np.median(ch_std)\n",
    "            mad = np.median(np.abs(ch_std - med)) + 1e-12\n",
    "            z = 0.6745 * (ch_std - med) / mad\n",
    "            eeg_names = mne.pick_info(raw.info, mne.pick_types(raw.info, eeg=True)).ch_names\n",
    "            bads = [eeg_names[i] for i in np.where(np.abs(z) > bad_channel_z)[0]]\n",
    "            raw.info[\"bads\"] = bads\n",
    "            if bads:\n",
    "                raw.interpolate_bads(reset_bads=True, verbose=False)\n",
    "\n",
    "    if car:\n",
    "        raw.set_eeg_reference(\"average\", projection=True)\n",
    "        raw.apply_proj()\n",
    "\n",
    "    if use_wica:\n",
    "        cleaned = wavelet_enhanced_ica(\n",
    "            raw.get_data(),\n",
    "            n_components=wica_components,\n",
    "            wavelet=wica_wavelet,\n",
    "            level=wica_level,\n",
    "            random_state=wica_random_state\n",
    "        )\n",
    "        raw = mne.io.RawArray(cleaned, raw.info, verbose=False)\n",
    "\n",
    "    cleaned = raw.get_data()\n",
    "    return cleaned, sfreq_out\n",
    "\n",
    "print(\"[STEP 3] Preprocessing functions ready ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a9d20-889d-45c4-bbc2-de62216908cd",
   "metadata": {},
   "source": [
    "Cell 4 — Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bfcbde0-1cc2-409c-8627-93da46606661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 4] Starting preprocessing...\n",
      "[INFO] USE_WICA = True\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 01/17: Amin | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 02/17: Amin1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 03/17: Daniel | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 04/17: Ismayil | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 05/17: Jack | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 06/17: James | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 07/17: Josh | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 08/17: Marjan | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 09/17: Max | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 10/17: Mina | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 11/17: Mina 1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 12/17: Mina 3 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 13/17: Mohammad | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 14/17: Mona | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 15/17: Roddy | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 16/17: Sam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 17/17: adam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Test subject: Cole | trials=1 | fs_out=128.0\n",
      "[STEP 4] Preprocessing done ✅\n",
      "  X_train_trials_all: (17, 14, 38400)\n",
      "  y_train_trials_all: (17,) (array([0, 1], dtype=int32), array([9, 8]))\n",
      "  X_test_trials_all : (1, 14, 38400)\n",
      "  y_test_trials_all : (1,) (array([1], dtype=int32), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 4: Apply preprocessing (LOSO)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 4] Starting preprocessing...\")\n",
    "\n",
    "USE_WICA = True\n",
    "print(f\"[INFO] USE_WICA = {USE_WICA}\")\n",
    "\n",
    "X_train_trials_all, y_train_trials_all = [], []\n",
    "X_test_trials_all, y_test_trials_all = [], []\n",
    "\n",
    "def preprocess_trials(X_trials, y_trials):\n",
    "    cleaned_list = []\n",
    "    label_list = []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        trial = X_trials[i]\n",
    "        label = int(y_trials[i])\n",
    "\n",
    "        cleaned, fs_out = preprocess_eeg(\n",
    "            eeg=trial,\n",
    "            sfreq=SFREQ,\n",
    "            use_wica=USE_WICA\n",
    "        )\n",
    "\n",
    "        cleaned_list.append(cleaned.astype(np.float32, copy=False))\n",
    "        label_list.append(label)\n",
    "\n",
    "    return cleaned_list, np.array(label_list, dtype=np.int32), fs_out\n",
    "\n",
    "# Train subjects\n",
    "for idx, subj in enumerate(train_subjects, start=1):\n",
    "    X_trials, y_trials = subject_data[subj]\n",
    "    cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "    X_train_trials_all.extend(cleaned_trials)\n",
    "    y_train_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "    print(f\"[STEP 4] Train {idx:02d}/{len(train_subjects)}: {subj} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Test subject\n",
    "X_trials, y_trials = subject_data[TEST_SUBJECT]\n",
    "cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "X_test_trials_all.extend(cleaned_trials)\n",
    "y_test_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "print(f\"[STEP 4] Test subject: {TEST_SUBJECT} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Convert to arrays\n",
    "X_train_trials_all = np.array(X_train_trials_all, dtype=np.float32)\n",
    "y_train_trials_all = np.array(y_train_trials_all, dtype=np.int32)\n",
    "\n",
    "X_test_trials_all = np.array(X_test_trials_all, dtype=np.float32)\n",
    "y_test_trials_all = np.array(y_test_trials_all, dtype=np.int32)\n",
    "\n",
    "print(\"[STEP 4] Preprocessing done ✅\")\n",
    "print(\"  X_train_trials_all:\", X_train_trials_all.shape)\n",
    "print(\"  y_train_trials_all:\", y_train_trials_all.shape, np.unique(y_train_trials_all, return_counts=True))\n",
    "print(\"  X_test_trials_all :\", X_test_trials_all.shape)\n",
    "print(\"  y_test_trials_all :\", y_test_trials_all.shape, np.unique(y_test_trials_all, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc3da1-2967-4ee3-84cd-bfcf0f98c0b1",
   "metadata": {},
   "source": [
    "Cell 5 — Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c21ad54-7001-4039-9360-29dfa2905127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 5] Starting segmentation...\n",
      "[STEP 5] Segmented 5/17 trials...\n",
      "[STEP 5] Segmented 10/17 trials...\n",
      "[STEP 5] Segmented 15/17 trials...\n",
      "[STEP 5] Segmented 17/17 trials...\n",
      "[STEP 5] Segmented 1/1 trials...\n",
      "[STEP 5] Segmentation done ✅\n",
      "  X_train: (5100, 14, 128, 1)\n",
      "  y_train: (5100,) (array([0, 1], dtype=int32), array([2700, 2400]))\n",
      "  X_test : (300, 14, 128, 1)\n",
      "  y_test : (300,) (array([1], dtype=int32), array([300]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 5: Segmentation\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 5] Starting segmentation...\")\n",
    "\n",
    "def segment_trial(trial_2d: np.ndarray, label: int, segment_len: int = 128):\n",
    "    C, T = trial_2d.shape\n",
    "    n_segs = T // segment_len\n",
    "    use_T = n_segs * segment_len\n",
    "\n",
    "    if n_segs == 0:\n",
    "        return np.empty((0, C, segment_len), dtype=np.float32), np.empty((0,), dtype=np.int32)\n",
    "\n",
    "    trial_trim = trial_2d[:, :use_T]\n",
    "    Xsegs = trial_trim.reshape(C, n_segs, segment_len).transpose(1, 0, 2)\n",
    "    ysegs = np.full((n_segs,), int(label), dtype=np.int32)\n",
    "    return Xsegs.astype(np.float32, copy=False), ysegs\n",
    "\n",
    "def segment_dataset(X_trials: np.ndarray, y_trials: np.ndarray, segment_len: int = 128):\n",
    "    X_all, y_all = [], []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        Xsegs, ysegs = segment_trial(X_trials[i], int(y_trials[i]), segment_len=segment_len)\n",
    "        X_all.append(Xsegs)\n",
    "        y_all.append(ysegs)\n",
    "\n",
    "        if (i + 1) % 5 == 0 or (i + 1) == X_trials.shape[0]:\n",
    "            print(f\"[STEP 5] Segmented {i+1}/{X_trials.shape[0]} trials...\")\n",
    "\n",
    "    X_out = np.concatenate(X_all, axis=0)\n",
    "    y_out = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    X_out = np.expand_dims(X_out, axis=-1)  # (N,14,128,1)\n",
    "    return X_out.astype(np.float32), y_out.astype(np.int32)\n",
    "\n",
    "X_train, y_train = segment_dataset(X_train_trials_all, y_train_trials_all, segment_len=128)\n",
    "X_test,  y_test  = segment_dataset(X_test_trials_all,  y_test_trials_all,  segment_len=128)\n",
    "\n",
    "print(\"[STEP 5] Segmentation done ✅\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  y_train:\", y_train.shape, np.unique(y_train, return_counts=True))\n",
    "print(\"  X_test :\", X_test.shape)\n",
    "print(\"  y_test :\", y_test.shape, np.unique(y_test, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b4ffd5-bf2f-490e-8854-0a2fc747f1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e69ba7-3952-47b1-bd80-f05237b2e018",
   "metadata": {},
   "source": [
    "Cell 6 — Split train/val + Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97fda4ae-e1c8-4fd1-bf0c-c82ae2a85764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 6] Normalizing using ALL training data (no train/val split)...\n",
      "[STEP 6] Normalization done ✅\n",
      "  X_train_norm: (5100, 14, 128, 1)\n",
      "  X_test_norm : (300, 14, 128, 1)\n",
      "  y_train_norm: (5100,)\n",
      "  y_test_norm : (300,)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 6: Normalize (ALL training data)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 6] Normalizing using ALL training data (no train/val split)...\")\n",
    "\n",
    "def normalize_eeg(train_data: np.ndarray, test_data: np.ndarray):\n",
    "    if train_data.ndim == 5:\n",
    "        train_flat = train_data.reshape(-1, *train_data.shape[2:])\n",
    "        test_flat  = test_data.reshape(-1, *test_data.shape[2:])\n",
    "    else:\n",
    "        train_flat, test_flat = train_data, test_data\n",
    "\n",
    "    # Compute stats on ALL training data\n",
    "    train_mean = np.mean(train_flat, axis=(0, 2, 3), keepdims=True)\n",
    "    train_std  = np.std(train_flat,  axis=(0, 2, 3), keepdims=True) + 1e-8\n",
    "\n",
    "    train_norm = (train_flat - train_mean) / train_std\n",
    "    test_norm  = (test_flat  - train_mean) / train_std\n",
    "\n",
    "    if train_data.ndim == 5:\n",
    "        train_norm = train_norm.reshape(train_data.shape)\n",
    "        test_norm  = test_norm.reshape(test_data.shape)\n",
    "\n",
    "    return (\n",
    "        train_norm.astype(np.float32),\n",
    "        test_norm.astype(np.float32),\n",
    "        train_mean,\n",
    "        train_std\n",
    "    )\n",
    "\n",
    "# Normalize ALL training data\n",
    "X_train_norm, X_test_norm, mean, std = normalize_eeg(X_train, X_test)\n",
    "\n",
    "y_train_norm = y_train.astype(np.float32)\n",
    "y_test_norm  = y_test.astype(np.float32)\n",
    "\n",
    "print(\"[STEP 6] Normalization done ✅\")\n",
    "print(\"  X_train_norm:\", X_train_norm.shape)\n",
    "print(\"  X_test_norm :\", X_test_norm.shape)\n",
    "print(\"  y_train_norm:\", y_train_norm.shape)\n",
    "print(\"  y_test_norm :\", y_test_norm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f527c-34db-4c23-bdbd-c8ce03b519f0",
   "metadata": {},
   "source": [
    "Cell 7 — EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ea4279-53c2-49c1-b834-7b1e1c20775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\DL\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Xtr: (5100, 14, 128, 1), ytr: (5100,)\n",
      "[INFO] Test / LOSO subject is NOT used here.\n",
      "[SPLIT] Train: (4080, 14, 128, 1), Val: (1020, 14, 128, 1)\n",
      "Epoch 1/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 135ms/step - accuracy: 0.5002 - loss: 0.7457 - val_accuracy: 0.5294 - val_loss: 0.6912 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.5331 - loss: 0.6876 - val_accuracy: 0.5294 - val_loss: 0.6913 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.5708 - loss: 0.6701 - val_accuracy: 0.5304 - val_loss: 0.6907 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.5963 - loss: 0.6531 - val_accuracy: 0.5343 - val_loss: 0.6895 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.6164 - loss: 0.6349 - val_accuracy: 0.5451 - val_loss: 0.6874 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.6306 - loss: 0.6144 - val_accuracy: 0.5961 - val_loss: 0.6808 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.6755 - loss: 0.5847 - val_accuracy: 0.6127 - val_loss: 0.6719 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.6934 - loss: 0.5628 - val_accuracy: 0.6657 - val_loss: 0.6593 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.7167 - loss: 0.5419 - val_accuracy: 0.6814 - val_loss: 0.6426 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.7400 - loss: 0.5163 - val_accuracy: 0.7225 - val_loss: 0.6228 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.7417 - loss: 0.5004 - val_accuracy: 0.7275 - val_loss: 0.6044 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7593 - loss: 0.4854 - val_accuracy: 0.7235 - val_loss: 0.5821 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7831 - loss: 0.4578 - val_accuracy: 0.7225 - val_loss: 0.5760 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.7988 - loss: 0.4218 - val_accuracy: 0.7490 - val_loss: 0.5398 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8199 - loss: 0.3973 - val_accuracy: 0.7539 - val_loss: 0.5159 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8324 - loss: 0.3735 - val_accuracy: 0.7578 - val_loss: 0.4816 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.8463 - loss: 0.3572 - val_accuracy: 0.7353 - val_loss: 0.4852 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.8407 - loss: 0.3600 - val_accuracy: 0.7490 - val_loss: 0.4756 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.8360 - loss: 0.3633 - val_accuracy: 0.7618 - val_loss: 0.4640 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8529 - loss: 0.3455 - val_accuracy: 0.7853 - val_loss: 0.4341 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8566 - loss: 0.3335 - val_accuracy: 0.7686 - val_loss: 0.4344 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8659 - loss: 0.3272 - val_accuracy: 0.7667 - val_loss: 0.4454 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8711 - loss: 0.3021 - val_accuracy: 0.7559 - val_loss: 0.4752 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8746 - loss: 0.2945\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8725 - loss: 0.3021 - val_accuracy: 0.7833 - val_loss: 0.4538 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8831 - loss: 0.2734 - val_accuracy: 0.7706 - val_loss: 0.4077 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8853 - loss: 0.2687 - val_accuracy: 0.7804 - val_loss: 0.4018 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8939 - loss: 0.2614 - val_accuracy: 0.8167 - val_loss: 0.3611 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8951 - loss: 0.2596 - val_accuracy: 0.8363 - val_loss: 0.3365 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8973 - loss: 0.2552 - val_accuracy: 0.8667 - val_loss: 0.2858 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8985 - loss: 0.2577 - val_accuracy: 0.8941 - val_loss: 0.2708 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 160ms/step - accuracy: 0.9032 - loss: 0.2420 - val_accuracy: 0.8990 - val_loss: 0.2471 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9150 - loss: 0.2288 - val_accuracy: 0.9127 - val_loss: 0.2174 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9042 - loss: 0.2403 - val_accuracy: 0.9078 - val_loss: 0.2385 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9066 - loss: 0.2313 - val_accuracy: 0.9265 - val_loss: 0.1910 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9088 - loss: 0.2312 - val_accuracy: 0.9471 - val_loss: 0.1650 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9105 - loss: 0.2192 - val_accuracy: 0.9363 - val_loss: 0.1831 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9086 - loss: 0.2273 - val_accuracy: 0.9549 - val_loss: 0.1654 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9130 - loss: 0.2158 - val_accuracy: 0.9461 - val_loss: 0.1630 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9064 - loss: 0.2346 - val_accuracy: 0.9333 - val_loss: 0.1880 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9135 - loss: 0.2263 - val_accuracy: 0.9569 - val_loss: 0.1477 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9176 - loss: 0.2074 - val_accuracy: 0.9480 - val_loss: 0.1569 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9179 - loss: 0.2014 - val_accuracy: 0.9598 - val_loss: 0.1373 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9255 - loss: 0.1997 - val_accuracy: 0.9618 - val_loss: 0.1482 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9223 - loss: 0.2015 - val_accuracy: 0.9657 - val_loss: 0.1198 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9297 - loss: 0.1874 - val_accuracy: 0.9716 - val_loss: 0.1200 - learning_rate: 5.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9277 - loss: 0.1844 - val_accuracy: 0.9745 - val_loss: 0.1100 - learning_rate: 5.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9282 - loss: 0.1791 - val_accuracy: 0.9686 - val_loss: 0.1250 - learning_rate: 5.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9248 - loss: 0.1932 - val_accuracy: 0.9706 - val_loss: 0.1256 - learning_rate: 5.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9248 - loss: 0.1898 - val_accuracy: 0.9696 - val_loss: 0.1129 - learning_rate: 5.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.9246 - loss: 0.1933\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9282 - loss: 0.1951 - val_accuracy: 0.9696 - val_loss: 0.1209 - learning_rate: 5.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9350 - loss: 0.1757 - val_accuracy: 0.9735 - val_loss: 0.1071 - learning_rate: 2.5000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9321 - loss: 0.1808 - val_accuracy: 0.9765 - val_loss: 0.1013 - learning_rate: 2.5000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 125ms/step - accuracy: 0.9338 - loss: 0.1826 - val_accuracy: 0.9784 - val_loss: 0.0968 - learning_rate: 2.5000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9387 - loss: 0.1557 - val_accuracy: 0.9794 - val_loss: 0.0950 - learning_rate: 2.5000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9311 - loss: 0.1714 - val_accuracy: 0.9833 - val_loss: 0.0906 - learning_rate: 2.5000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9272 - loss: 0.1877 - val_accuracy: 0.9784 - val_loss: 0.0922 - learning_rate: 2.5000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9324 - loss: 0.1660 - val_accuracy: 0.9804 - val_loss: 0.0912 - learning_rate: 2.5000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9365 - loss: 0.1636 - val_accuracy: 0.9765 - val_loss: 0.0937 - learning_rate: 2.5000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9407 - loss: 0.1628 - val_accuracy: 0.9784 - val_loss: 0.0885 - learning_rate: 2.5000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9336 - loss: 0.1645 - val_accuracy: 0.9775 - val_loss: 0.0912 - learning_rate: 2.5000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9382 - loss: 0.1586 - val_accuracy: 0.9775 - val_loss: 0.0851 - learning_rate: 2.5000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9368 - loss: 0.1676 - val_accuracy: 0.9775 - val_loss: 0.0922 - learning_rate: 2.5000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9404 - loss: 0.1615 - val_accuracy: 0.9775 - val_loss: 0.0867 - learning_rate: 2.5000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9373 - loss: 0.1642 - val_accuracy: 0.9775 - val_loss: 0.0827 - learning_rate: 2.5000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9400 - loss: 0.1572 - val_accuracy: 0.9755 - val_loss: 0.0932 - learning_rate: 2.5000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9377 - loss: 0.1642 - val_accuracy: 0.9784 - val_loss: 0.0857 - learning_rate: 2.5000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9375 - loss: 0.1551 - val_accuracy: 0.9745 - val_loss: 0.0946 - learning_rate: 2.5000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9390 - loss: 0.1542\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9380 - loss: 0.1553 - val_accuracy: 0.9784 - val_loss: 0.0896 - learning_rate: 2.5000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9446 - loss: 0.1449 - val_accuracy: 0.9784 - val_loss: 0.0800 - learning_rate: 1.2500e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9397 - loss: 0.1627 - val_accuracy: 0.9755 - val_loss: 0.0840 - learning_rate: 1.2500e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9385 - loss: 0.1610 - val_accuracy: 0.9794 - val_loss: 0.0808 - learning_rate: 1.2500e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9449 - loss: 0.1465 - val_accuracy: 0.9824 - val_loss: 0.0766 - learning_rate: 1.2500e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9434 - loss: 0.1444 - val_accuracy: 0.9784 - val_loss: 0.0803 - learning_rate: 1.2500e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9424 - loss: 0.1510 - val_accuracy: 0.9804 - val_loss: 0.0833 - learning_rate: 1.2500e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9350 - loss: 0.1567 - val_accuracy: 0.9814 - val_loss: 0.0760 - learning_rate: 1.2500e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9404 - loss: 0.1541 - val_accuracy: 0.9804 - val_loss: 0.0750 - learning_rate: 1.2500e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9429 - loss: 0.1508 - val_accuracy: 0.9794 - val_loss: 0.0778 - learning_rate: 1.2500e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9409 - loss: 0.1586 - val_accuracy: 0.9804 - val_loss: 0.0761 - learning_rate: 1.2500e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9490 - loss: 0.1455 - val_accuracy: 0.9814 - val_loss: 0.0760 - learning_rate: 1.2500e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9466 - loss: 0.1428\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9434 - loss: 0.1469 - val_accuracy: 0.9784 - val_loss: 0.0784 - learning_rate: 1.2500e-04\n",
      "Epoch 81/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9387 - loss: 0.1528 - val_accuracy: 0.9794 - val_loss: 0.0797 - learning_rate: 6.2500e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9471 - loss: 0.1404 - val_accuracy: 0.9794 - val_loss: 0.0781 - learning_rate: 6.2500e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9407 - loss: 0.1569 - val_accuracy: 0.9784 - val_loss: 0.0844 - learning_rate: 6.2500e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9488 - loss: 0.1443 - val_accuracy: 0.9804 - val_loss: 0.0749 - learning_rate: 6.2500e-05\n",
      "Epoch 85/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9475 - loss: 0.1510 - val_accuracy: 0.9824 - val_loss: 0.0768 - learning_rate: 6.2500e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9434 - loss: 0.1507 - val_accuracy: 0.9804 - val_loss: 0.0775 - learning_rate: 6.2500e-05\n",
      "Epoch 87/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.9441 - loss: 0.1437 - val_accuracy: 0.9824 - val_loss: 0.0747 - learning_rate: 6.2500e-05\n",
      "Epoch 88/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9431 - loss: 0.1488 - val_accuracy: 0.9814 - val_loss: 0.0757 - learning_rate: 6.2500e-05\n",
      "Epoch 89/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9456 - loss: 0.1496 - val_accuracy: 0.9824 - val_loss: 0.0732 - learning_rate: 6.2500e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9475 - loss: 0.1341 - val_accuracy: 0.9804 - val_loss: 0.0771 - learning_rate: 6.2500e-05\n",
      "Epoch 91/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9441 - loss: 0.1484 - val_accuracy: 0.9814 - val_loss: 0.0715 - learning_rate: 6.2500e-05\n",
      "Epoch 92/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9473 - loss: 0.1366 - val_accuracy: 0.9804 - val_loss: 0.0754 - learning_rate: 6.2500e-05\n",
      "Epoch 93/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9500 - loss: 0.1355 - val_accuracy: 0.9814 - val_loss: 0.0768 - learning_rate: 6.2500e-05\n",
      "Epoch 94/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9473 - loss: 0.1458 - val_accuracy: 0.9804 - val_loss: 0.0776 - learning_rate: 6.2500e-05\n",
      "Epoch 95/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9494 - loss: 0.1350\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.9458 - loss: 0.1380 - val_accuracy: 0.9814 - val_loss: 0.0736 - learning_rate: 6.2500e-05\n",
      "Epoch 96/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9456 - loss: 0.1437 - val_accuracy: 0.9814 - val_loss: 0.0748 - learning_rate: 3.1250e-05\n",
      "Epoch 97/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9505 - loss: 0.1388 - val_accuracy: 0.9814 - val_loss: 0.0713 - learning_rate: 3.1250e-05\n",
      "Epoch 98/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9429 - loss: 0.1578 - val_accuracy: 0.9824 - val_loss: 0.0734 - learning_rate: 3.1250e-05\n",
      "Epoch 99/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9463 - loss: 0.1411 - val_accuracy: 0.9814 - val_loss: 0.0713 - learning_rate: 3.1250e-05\n",
      "Epoch 100/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9500 - loss: 0.1407 - val_accuracy: 0.9804 - val_loss: 0.0720 - learning_rate: 3.1250e-05\n",
      "Epoch 101/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.9539 - loss: 0.1245 \n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9507 - loss: 0.1356 - val_accuracy: 0.9824 - val_loss: 0.0723 - learning_rate: 3.1250e-05\n",
      "Epoch 102/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9461 - loss: 0.1399 - val_accuracy: 0.9824 - val_loss: 0.0709 - learning_rate: 1.5625e-05\n",
      "Epoch 103/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9431 - loss: 0.1423 - val_accuracy: 0.9814 - val_loss: 0.0725 - learning_rate: 1.5625e-05\n",
      "Epoch 104/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9500 - loss: 0.1358 - val_accuracy: 0.9804 - val_loss: 0.0720 - learning_rate: 1.5625e-05\n",
      "Epoch 105/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9404 - loss: 0.1465 - val_accuracy: 0.9833 - val_loss: 0.0745 - learning_rate: 1.5625e-05\n",
      "Epoch 106/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9510 - loss: 0.1300\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9485 - loss: 0.1359 - val_accuracy: 0.9814 - val_loss: 0.0711 - learning_rate: 1.5625e-05\n",
      "Epoch 107/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9495 - loss: 0.1378 - val_accuracy: 0.9814 - val_loss: 0.0729 - learning_rate: 7.8125e-06\n",
      "Epoch 108/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9441 - loss: 0.1435 - val_accuracy: 0.9824 - val_loss: 0.0725 - learning_rate: 7.8125e-06\n",
      "Epoch 109/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9478 - loss: 0.1317 - val_accuracy: 0.9833 - val_loss: 0.0720 - learning_rate: 7.8125e-06\n",
      "Epoch 110/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9541 - loss: 0.1257\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9490 - loss: 0.1355 - val_accuracy: 0.9824 - val_loss: 0.0732 - learning_rate: 7.8125e-06\n",
      "Epoch 111/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.9473 - loss: 0.1354 - val_accuracy: 0.9814 - val_loss: 0.0734 - learning_rate: 3.9063e-06\n",
      "Epoch 112/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9490 - loss: 0.1407 - val_accuracy: 0.9804 - val_loss: 0.0724 - learning_rate: 3.9063e-06\n",
      "Epoch 113/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9527 - loss: 0.1285 - val_accuracy: 0.9814 - val_loss: 0.0719 - learning_rate: 3.9063e-06\n",
      "Epoch 114/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9530 - loss: 0.1324\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9466 - loss: 0.1439 - val_accuracy: 0.9804 - val_loss: 0.0719 - learning_rate: 3.9063e-06\n",
      "Epoch 115/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9532 - loss: 0.1292 - val_accuracy: 0.9804 - val_loss: 0.0721 - learning_rate: 1.9531e-06\n",
      "Epoch 116/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9483 - loss: 0.1323 - val_accuracy: 0.9804 - val_loss: 0.0718 - learning_rate: 1.9531e-06\n",
      "Epoch 117/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9446 - loss: 0.1382 - val_accuracy: 0.9804 - val_loss: 0.0716 - learning_rate: 1.9531e-06\n",
      "Epoch 117: early stopping\n",
      "Restoring model weights from the end of the best epoch: 102.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] Model saved as cole-final.h5\n",
      "\n",
      "[CONFUSION MATRIX]\n",
      "[[532   8]\n",
      " [ 10 470]]\n",
      "[VAL] acc=0.9824, prec=0.9833, rec=0.9792, f1=0.9812\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 4: 80:20 Train / Validation Split (EEGNet)\n",
    "# ====================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, SeparableConv2D, BatchNormalization, Activation,\n",
    "    DepthwiseConv2D, Conv2D, AveragePooling2D,\n",
    "    Dropout, Dense, Flatten\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# --------------------------------------------------------------------\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# EEGNet model\n",
    "# --------------------------------------------------------------------\n",
    "def create_eegnet_model(input_shape, dropout_rate=0.5, num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(16, (1, 64), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = DepthwiseConv2D(\n",
    "        (input_shape[0], 1),\n",
    "        depth_multiplier=2,\n",
    "        padding='valid',\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 4))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = SeparableConv2D(16, (1, 16), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 8))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Compile model\n",
    "# --------------------------------------------------------------------\n",
    "def compile_model():\n",
    "    input_shape = (Xtr.shape[1], Xtr.shape[2], Xtr.shape[3])\n",
    "    model = create_eegnet_model(input_shape)\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-3),\n",
    "        loss=BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[BinaryAccuracy(name=\"accuracy\", threshold=0.5)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def make_callbacks():\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\",\n",
    "            factor=0.5,\n",
    "            patience=4,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Use ALL normalized training data\n",
    "# --------------------------------------------------------------------\n",
    "Xtr = X_train_norm\n",
    "ytr = y_train_norm\n",
    "\n",
    "print(f\"[TRAIN] Xtr: {Xtr.shape}, ytr: {ytr.shape}\")\n",
    "print(\"[INFO] Test / LOSO subject is NOT used here.\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 80:20 Stratified Split\n",
    "# --------------------------------------------------------------------\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(sss.split(Xtr, ytr))\n",
    "\n",
    "X_train, X_val = Xtr[train_idx], Xtr[val_idx]\n",
    "y_train, y_val = ytr[train_idx], ytr[val_idx]\n",
    "\n",
    "print(f\"[SPLIT] Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Train model\n",
    "# --------------------------------------------------------------------\n",
    "model = compile_model()\n",
    "callbacks = make_callbacks()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Save final model\n",
    "# --------------------------------------------------------------------\n",
    "model.save(\"cole-final.h5\")\n",
    "print(\"[SAVE] Model saved as cole-final.h5\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Validation Confusion Matrix\n",
    "# --------------------------------------------------------------------\n",
    "y_prob = model.predict(X_val, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "y_true = y_val.astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "eps = 1e-12\n",
    "\n",
    "acc  = (tp + tn) / max(tp + tn + fp + fn, eps)\n",
    "prec = tp / max(tp + fp, eps)\n",
    "rec  = tp / max(tp + fn, eps)\n",
    "f1   = 2 * prec * rec / max(prec + rec, eps)\n",
    "\n",
    "print(\"\\n[CONFUSION MATRIX]\")\n",
    "print(cm)\n",
    "print(f\"[VAL] acc={acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e2a13-6367-41ca-be66-a43594c2940e",
   "metadata": {},
   "source": [
    "Cell 8 — Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ac6662-33c1-4e25-b6c5-d47c860fba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Preparing TEST data...\n",
      "[TEST] Xte shape: (300, 14, 128, 1)\n",
      "[TEST] yte shape: (300,), class distribution: (array([1]), array([300]))\n",
      "[TEST] Loading final model: cole-final.h5\n",
      "[TEST] Running inference on TEST subject...\n",
      "[TEST] Mean predicted probability: 0.4559\n",
      "\n",
      "[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[  0   0]\n",
      " [169 131]]\n",
      "Accuracy : 0.4367\n",
      "Precision: 1.0000\n",
      "Recall   : 0.4367\n",
      "F1-score : 0.6079\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Inference on TEST subject (adam) using FINAL EEGNet\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Prepare TEST data\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Preparing TEST data...\")\n",
    "\n",
    "Xte = X_test_norm\n",
    "yte = y_test_norm.astype(int)\n",
    "\n",
    "print(f\"[TEST] Xte shape: {Xte.shape}\")\n",
    "print(f\"[TEST] yte shape: {yte.shape}, class distribution: {np.unique(yte, return_counts=True)}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Load FINAL trained model\n",
    "# ------------------------------------------------------------------\n",
    "model_path = \"cole-final.h5\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        \"Final model 'cole-final.h5' not found. \"\n",
    "        \"Please run Cell 4 (80:20 training) first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[TEST] Loading final model: {model_path}\")\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Run inference on TEST subject\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Running inference on TEST subject...\")\n",
    "\n",
    "y_prob = model.predict(Xte, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(f\"[TEST] Mean predicted probability: {y_prob.mean():.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Metrics on TEST subject (segment-level)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc = accuracy_score(yte, y_pred)\n",
    "prec = precision_score(yte, y_pred, zero_division=0)\n",
    "rec = recall_score(yte, y_pred, zero_division=0)\n",
    "f1 = f1_score(yte, y_pred, zero_division=0)\n",
    "\n",
    "print(\"\\n[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572dbf3e-bbb5-4865-883f-30e374700949",
   "metadata": {},
   "source": [
    "Cell 9 - Performance Matrices for each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a57240-50ea-4736-a7ac-52ef7878b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] Verifying predictions and labels are available...\n",
      "[CHECK] Found predictions for 300 test segments.\n",
      "\n",
      "[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[  0   0]\n",
      " [169 131]]\n",
      "Accuracy : 0.4367\n",
      "Precision: 1.0000\n",
      "Recall   : 0.4367\n",
      "F1-score : 0.6079\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg8AAAHWCAYAAADw/GrYAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQxxJREFUeJzt3XtcVVX+//H3RuQicEA0QRIvaXkpR0vNIUsxKfOWpk1ZVmimXaRSyy5TppnFZGWaqdRMaRft1ziVTdZYjqbmNzMvQxfHTIqSyQDLAMG4yNm/P4xTR0BZsuF44vV8PPZjOmuvs/ZnHw/Dh89ae2/Ltm1bAAAANRTg6wAAAIB/IXkAAABGSB4AAIARkgcAAGCE5AEAABgheQAAAEZIHgAAgBGSBwAAYITkAQAAGCF5QI3s2bNHF198sSIjI2VZllauXOno+N98840sy9LSpUsdHdefJSYmKjEx0ddhAEAlJA9+5KuvvtKNN96o0047TSEhIXK5XOrTp4/mz5+vn3/+uU6PnZycrM8++0wPP/ywXnrpJfXs2bNOj1efxo4dK8uy5HK5qvwc9+zZI8uyZFmWHn/8cePx9+3bp5kzZyo9Pd2BaOtH27ZtPed89FZcXCxJWrp0qSzL0rZt2zzvmzlzpizLUkxMjA4dOlTluEOHDq3ymHl5eQoJCZFlWdq1a1eVfcaOHavw8HDj86lITqvb/vKXv3j6JiYmVtuvU6dOlcbOzMxUSkqKzjjjDDVp0kRNmjRRly5dNGnSJH366adefWvz+RzPokWLSL5RbwJ9HQBq5u2339af/vQnBQcH67rrrtNZZ52l0tJSbdq0SdOmTdPOnTv17LPP1smxf/75Z23evFn33XefUlJS6uQYbdq00c8//6zGjRvXyfjHExgYqEOHDumtt97SFVdc4bVv2bJlCgkJ8fzSNLVv3z49+OCDatu2rbp3717j97333nsndDyndO/eXXfccUel9qCgoOO+Nzc3V4sXL67y/dVZsWKFLMtSbGysli1bptmzZxvFWxNXXXWVBg8eXKn97LPP9nrdqlUrpaamVuoXGRnp9XrVqlW68sorFRgYqDFjxqhbt24KCAjQF198oddff12LFy9WZmam2rRp4/W+E/l8jmfRokVq3ry5xo4d69iYQHVIHvxAZmamRo8erTZt2mjdunVq2bKlZ9+kSZOUkZGht99+u86Ov3//fklSVFRUnR3DsiyFhITU2fjHExwcrD59+uiVV16plDwsX75cQ4YM0WuvvVYvsRw6dEhNmjSp0S/punTqqafqmmuuOaH3du/eXY899phuueUWhYaG1ug9L7/8sgYPHqw2bdpo+fLldZI8nHPOOTU6p8jIyOP2++qrrzw/l2vXrvX6uZSkRx99VIsWLVJAQOUC74l8PsDJhGkLPzBnzhwVFhbqueeeq/R/UJLUoUMH3X777Z7Xhw8f1kMPPaT27dsrODhYbdu21Z///GeVlJR4va+iRLpp0yade+65CgkJ0WmnnaYXX3zR02fmzJmev5qmTZsmy7LUtm1bSUdKyBX//VsVpdnfWrNmjc4//3xFRUUpPDxcHTt21J///GfP/urWPKxbt04XXHCBwsLCFBUVpeHDh1cqaVccLyMjQ2PHjlVUVJQiIyM1bty4KkvD1bn66qv1r3/9S3l5eZ62rVu3as+ePbr66qsr9T9w4IDuvPNOde3aVeHh4XK5XBo0aJA++eQTT5/169erV69ekqRx48Z5yt8V55mYmKizzjpL27dvV9++fdWkSRPP53L0mofk5GSFhIRUOv+BAweqadOm2rdvX43Pta498MADysnJ0eLFi2vUf+/evfrggw80evRojR49WpmZmfrwww/rOMramTNnjoqKirRkyZIqfy4DAwN12223KT4+vtI+k8/H7XZr3rx5OvPMMxUSEqKYmBjdeOON+umnnzx92rZtq507d2rDhg2e7xjrZVCXSB78wFtvvaXTTjtN5513Xo3633DDDXrggQd0zjnn6Mknn1S/fv2Umpqq0aNHV+qbkZGhyy+/XBdddJGeeOIJNW3aVGPHjtXOnTslSSNHjtSTTz4p6UjJ96WXXtK8efOM4t+5c6eGDh2qkpISzZo1S0888YQuvfRS/d///d8x3/fvf/9bAwcOVG5urmbOnKmpU6fqww8/VJ8+ffTNN99U6n/FFVfo4MGDSk1N1RVXXKGlS5fqwQcfrHGcI0eOlGVZev311z1ty5cvV6dOnXTOOedU6v/1119r5cqVGjp0qObOnatp06bps88+U79+/Ty/yDt37qxZs2ZJkiZOnKiXXnpJL730kvr27esZ58cff9SgQYPUvXt3zZs3T/37968yvvnz5+uUU05RcnKyysvLJUnPPPOM3nvvPS1YsEBxcXE1PteaKCsr0w8//OC11TQZu+CCC3ThhRdqzpw5NVqP88orrygsLExDhw7Vueeeq/bt22vZsmW1PYVKDh06VOmcfvjhBx0+fNirX3l5eZX9ioqKPH1WrVqlDh06qHfv3sZxmHw+N954o6ZNm+ZZ3zRu3DgtW7ZMAwcOVFlZmSRp3rx5atWqlTp16uT5jt13333GcQE1ZuOklp+fb0uyhw8fXqP+6enptiT7hhtu8Gq/8847bUn2unXrPG1t2rSxJdkbN270tOXm5trBwcH2HXfc4WnLzMy0JdmPPfaY15jJycl2mzZtKsUwY8YM+7dfrSeffNKWZO/fv7/auCuOsWTJEk9b9+7d7RYtWtg//vijp+2TTz6xAwIC7Ouuu67S8a6//nqvMS+77DK7WbNm1R7zt+cRFhZm27ZtX3755faAAQNs27bt8vJyOzY21n7wwQer/AyKi4vt8vLySucRHBxsz5o1y9O2devWSudWoV+/frYkOy0trcp9/fr182p79913bUn27Nmz7a+//toODw+3R4wYcdxzNFXx3Th6mzFjhqfPkiVLbEn21q1bPW0V/xb79++3N2zYYEuy586d6zXukCFDKh2va9eu9pgxYzyv//znP9vNmze3y8rKvPr99t/KRMW/X3Xb5s2bPX0r/k2q2m688Ubbtn/9uazqs//pp5/s/fv3e7ZDhw6d8OfzwQcf2JLsZcuWeR1j9erVldrPPPPMSt8XoK5QeTjJFRQUSJIiIiJq1P+dd96RJE2dOtWrvWJh1tFrI7p06aILLrjA8/qUU05Rx44d9fXXX59wzEerWCvx5ptvyu121+g933//vdLT0zV27FhFR0d72v/whz/ooosu8pznb910001ery+44AL9+OOPns+wJq6++mqtX79e2dnZWrdunbKzs6ucspCOrJOomM8uLy/Xjz/+6JmS2bFjR42PGRwcrHHjxtWo78UXX6wbb7xRs2bN0siRIxUSEqJnnnmmxscy0bt3b61Zs8Zru+6662r8/r59+6p///7H/ev6008/1WeffaarrrrK03bVVVfphx9+0LvvvlurczjaxIkTK53TmjVr1KVLF69+bdu2rbLf5MmTJf36c1nVlR+JiYk65ZRTPNvChQurjKUmn8+KFSsUGRmpiy66yKsC0qNHD4WHh+v999+vxacBnDgWTJ7kXC6XJOngwYM16v/tt98qICBAHTp08GqPjY1VVFSUvv32W6/21q1bVxqjadOmXvOptXXllVfqb3/7m2644Qbdc889GjBggEaOHKnLL7+8ysVkFechSR07dqy0r3Pnznr33XdVVFSksLAwT/vR59K0aVNJ0k8//eT5HI9n8ODBioiI0Kuvvqr09HT16tVLHTp0qHKaxO12a/78+Vq0aJEyMzM9UwmS1KxZsxodTzqyMNFkceTjjz+uN998U+np6Vq+fLlatGhx3Pfs37/fK77w8PDjXvLYvHlzJSUl1TiuqsycOVP9+vVTWlqapkyZUmWfl19+WWFhYTrttNOUkZEhSQoJCVHbtm21bNkyDRkypFYx/Nbpp59eo3MKCws7Zr+KZL6wsLDSvmeeeUYHDx5UTk7OcRddHu/z2bNnj/Lz86v9N87NzT3m+EBdIXk4yblcLsXFxenzzz83et/RCxar06hRoyrbbds+4WP89peUJIWGhmrjxo16//339fbbb2v16tV69dVXdeGFF+q9996rNgZTtTmXCsHBwRo5cqReeOEFff3115o5c2a1fR955BFNnz5d119/vR566CFFR0crICBAkydPrnGFRZLxavv//Oc/nl8aR//FXp1evXp5JY4zZsw45rk5pW/fvkpMTNScOXMqVYakI/82r7zyioqKiir99S8d+eVYWFh4Qvd2qEuRkZFq2bJllT+XFWsgqko4j3a8z8ftdqtFixbVrv845ZRTzAIHHELy4AeGDh2qZ599Vps3b1ZCQsIx+7Zp00Zut1t79uxR586dPe05OTnKy8urdL15bTRt2tTryoQKR1c3JCkgIEADBgzQgAEDNHfuXD3yyCO677779P7771f5F15FnLt3766074svvlDz5s29qg5Ouvrqq/X8888rICCgykWmFf7xj3+of//+eu6557za8/Ly1Lx5c8/rmiZyNVFUVKRx48apS5cuOu+88zRnzhxddtllnis6qrNs2TKv0vhpp53mWEzHM3PmTCUmJlY5vbJhwwb973//06xZs7y+r9KRitHEiRO1cuXKE75ktC4NGTJEf/vb3/Txxx/r3HPPPeFxjvX5tG/fXv/+97/Vp0+f4yaZTn7PgONhzYMfuOuuuxQWFqYbbrhBOTk5lfZ/9dVXmj9/viR5boBz9BURc+fOlSRHS8Dt27dXfn6+1130vv/+e73xxhte/Q4cOFDpvRU3Szr68tEKLVu2VPfu3fXCCy94JSiff/653nvvvSpv9OOU/v3766GHHtLTTz+t2NjYavs1atSoUlVjxYoV+u6777zaKpKcqhItU3fffbf27t2rF154QXPnzlXbtm2VnJxc7edYoU+fPkpKSvJs9Zk89OvXT4mJiXr00Ucr3WirYspi2rRpuvzyy722CRMm6PTTT6+Tqy6ccNddd6lJkya6/vrrq/y5rGnF61ifzxVXXKHy8nI99NBDld53+PBhr+9UWFiYI98xoCaoPPiB9u3ba/ny5bryyivVuXNnrztMfvjhh1qxYoXnrnLdunVTcnKynn32WeXl5alfv376+OOP9cILL2jEiBHVXgZ4IkaPHq27775bl112mW677TYdOnRIixcv1hlnnOG1YHDWrFnauHGjhgwZojZt2ig3N1eLFi1Sq1atdP7551c7/mOPPaZBgwYpISFB48eP188//6wFCxYoMjKyTkvuAQEBuv/++4/bb+jQoZo1a5bGjRun8847T5999pmWLVtW6Rdz+/btFRUVpbS0NEVERCgsLEy9e/dWu3btjOJat26dFi1apBkzZnguHV2yZIkSExM1ffp0zZkzx2i8+jRjxoxK372SkhK99tpruuiii6q9Qdill16q+fPnKzc31zPvX1ZWVuUNpKKjo3XLLbccM44dO3bo5ZdfrtTevn17r6pefn5+lf0keaogp59+upYvX66rrrpKHTt29Nxh0rZtZWZmavny5QoICFCrVq2OGZNU9ecjHUksbrzxRqWmpio9PV0XX3yxGjdurD179mjFihWaP3++Lr/8cklSjx49tHjxYs2ePVsdOnRQixYtdOGFFx732MAJ8eWlHjDz5Zdf2hMmTLDbtm1rBwUF2REREXafPn3sBQsW2MXFxZ5+ZWVl9oMPPmi3a9fObty4sR0fH2/fe++9Xn1su/rL5o6+RLC6SzVt27bfe+89+6yzzrKDgoLsjh072i+//HKlSzXXrl1rDx8+3I6Li7ODgoLsuLg4+6qrrrK//PLLSsc4+nLGf//733afPn3s0NBQ2+Vy2cOGDbP/+9//evX57eVvv1VxKWFmZma1n6lt1+zyv+ou1bzjjjvsli1b2qGhoXafPn3szZs3V3mJ5Ztvvml36dLFDgwM9DrPfv362WeeeWaVx/ztOAUFBXabNm3sc845p9Lli1OmTLEDAgK8Ljesreq+G791vEs1j1ZxCWTFuK+99potyX7uueeqPcb69ettSfb8+fNt2z7yb6VqLqNs3759teMc71LN5OTkSnFWtx0tIyPDvvnmm+0OHTrYISEhdmhoqN2pUyf7pptustPT0736mnw+v/Xss8/aPXr0sENDQ+2IiAi7a9eu9l133WXv27fP0yc7O9seMmSIHRERYUvisk3UKcu2DVaTAQCABo81DwAAwAjJAwAAMELyAAAAjJA8AAAAIyQPAADACMkDAAAw4tc3iXK73dq3b58iIiK4NSsANDC2bevgwYOKi4ur9iF7daG4uFilpaWOjRcUFFTtjdJOVn6dPOzbt0/x8fG+DgMA4ENZWVk1upOnE4qLi9WuTbiyc8uP37mGYmNjlZmZ6VcJhF8nDxWPxT1fgxWoxj6OBgBQnw6rTJv0jud3QX0oLS1Vdm65vt3eVq6I2lc7Cg661abHNyotLSV5qC8VUxWBaqxAi+QBABqUX+6P7Itp6/AIS+ERtT+uW/455e7XyQMAAL5QbrtV7sDDHcptd+0H8QGutgAAAEaoPAAAYMgtW27VvvTgxBi+QPIAAIAht9xyYsLBmVHqH9MWAADACJUHAAAMldu2yu3aTzk4MYYvkDwAAGCooa95YNoCAAAYofIAAIAht2yVN+DKA8kDAACGmLYAAAAwQOUBAABDXG0BAACMuH/ZnBjHHzFtAQCAn9i4caOGDRumuLg4WZallStXVuqza9cuXXrppYqMjFRYWJh69eqlvXv3evYXFxdr0qRJatasmcLDwzVq1Cjl5OQYxUHyAACAofJfrrZwYjNRVFSkbt26aeHChVXu/+qrr3T++eerU6dOWr9+vT799FNNnz5dISEhnj5TpkzRW2+9pRUrVmjDhg3at2+fRo4caRQH0xYAABgqt+XQI7nN+g8aNEiDBg2qdv99992nwYMHa86cOZ629u3be/47Pz9fzz33nJYvX64LL7xQkrRkyRJ17txZH330kf74xz/WKA4qDwAA/A643W69/fbbOuOMMzRw4EC1aNFCvXv39pra2L59u8rKypSUlORp69Spk1q3bq3NmzfX+FgkDwAAGHI7uElSQUGB11ZSUmIcU25urgoLC/WXv/xFl1xyid577z1ddtllGjlypDZs2CBJys7OVlBQkKKiorzeGxMTo+zs7Bofi2kLAAAMuWWpXJYj40hSfHy8V/uMGTM0c+ZMs7HcR1KR4cOHa8qUKZKk7t2768MPP1RaWpr69etX63grkDwAAOBjWVlZcrlcntfBwcHGYzRv3lyBgYHq0qWLV3vnzp21adMmSVJsbKxKS0uVl5fnVX3IyclRbGxsjY/FtAUAAIbctnObJLlcLq/tRJKHoKAg9erVS7t37/Zq//LLL9WmTRtJUo8ePdS4cWOtXbvWs3/37t3au3evEhISanwsKg8AABgqd2jawnSMwsJCZWRkeF5nZmYqPT1d0dHRat26taZNm6Yrr7xSffv2Vf/+/bV69Wq99dZbWr9+vSQpMjJS48eP19SpUxUdHS2Xy6Vbb71VCQkJNb7SQiJ5AADAb2zbtk39+/f3vJ46daokKTk5WUuXLtVll12mtLQ0paam6rbbblPHjh312muv6fzzz/e858knn1RAQIBGjRqlkpISDRw4UIsWLTKKw7JtP72xto6sTo2MjFSihivQauzrcAAA9eiwXab1elP5+fle6wXqUsXvnQ93tlR4RO1n/gsPunXemd/X6zk4gcoDAACG3LYlt+3A1RYOjOELLJgEAABGqDwAAGDIVwsmTxYkDwAAGCpXgModKN6XOxCLLzBtAQAAjFB5AADAkO3QgknbTxdMkjwAAGCooa95YNoCAAAYofIAAIChcjtA5bYDCyb99DaNJA8AABhyy5LbgeK9W/6ZPTBtAQAAjFB5AADAUENfMEnyAACAIefWPDBtAQAAGgAqDwAAGDqyYNKBp2oybQEAQMPgdujZFlxtAQAAGgQqDwAAGGroCyZJHgAAMORWADeJAgAAqCkqDwAAGCq3LZU78DhtJ8bwBZIHAAAMlTt0tUU50xYAAKAhoPIAAIAhtx0gtwNXW7i52gIAgIaBaQsAAAADVB4AADDkljNXSrhrH4pPkDwAAGDIuZtE+ecEgH9GDQAAfIbKAwAAhpx7toV//g1P8gAAgCG3LLnlxJoH/7zDpH+mPAAAwGeoPAAAYIhpCwAAYMS5m0T5Z/Lgn1EDAACfIXkAAMCQ27Yc20xs3LhRw4YNU1xcnCzL0sqVK6vte9NNN8myLM2bN8+r/cCBAxozZoxcLpeioqI0fvx4FRYWGsVB8gAAgCH3L9MWtd1MbxJVVFSkbt26aeHChcfs98Ybb+ijjz5SXFxcpX1jxozRzp07tWbNGq1atUobN27UxIkTjeJgzQMAAH5i0KBBGjRo0DH7fPfdd7r11lv17rvvasiQIV77du3apdWrV2vr1q3q2bOnJGnBggUaPHiwHn/88SqTjapQeQAAwFDFI7md2CSpoKDAayspKTmxuNxuXXvttZo2bZrOPPPMSvs3b96sqKgoT+IgSUlJSQoICNCWLVtqfBySBwAADJXLcmyTpPj4eEVGRnq21NTUE4rr0UcfVWBgoG677bYq92dnZ6tFixZebYGBgYqOjlZ2dnaNj8O0BQAAPpaVlSWXy+V5HRwcbDzG9u3bNX/+fO3YsUOWVbd3rqTyAACAIaenLVwul9d2IsnDBx98oNzcXLVu3VqBgYEKDAzUt99+qzvuuENt27aVJMXGxio3N9frfYcPH9aBAwcUGxtb42NReQAAwFC55JlyqO04Trn22muVlJTk1TZw4EBde+21GjdunCQpISFBeXl52r59u3r06CFJWrdundxut3r37l3jY5E8AADgJwoLC5WRkeF5nZmZqfT0dEVHR6t169Zq1qyZV//GjRsrNjZWHTt2lCR17txZl1xyiSZMmKC0tDSVlZUpJSVFo0ePrvGVFhLJAwAAxn475VDbcUxs27ZN/fv397yeOnWqJCk5OVlLly6t0RjLli1TSkqKBgwYoICAAI0aNUpPPfWUURwkDwAAGPLVg7ESExNl23aN+3/zzTeV2qKjo7V8+XKj4x6NBZMAAMAIlQcAAAzZsuR2YMGk7cAYvkDyAACAIV9NW5ws/DNqAADgM1QeAAAwdCKP065uHH9E8gAAgKGKR2o7MY4/8s+oAQCAz1B5AADAENMWAADAiFsBcjtQvHdiDF/wz6gBAIDPUHkAAMBQuW2p3IEpByfG8AWSBwAADDX0NQ9MWwAAACNUHgAAMGQ79Ehu209vT03yAACAoXJZKnfgoVZOjOEL/pnyAAAAn6HyAACAIbftzGJHt+1AMD5A8oATlmVn6Ft9qVIVK1yR6qizFWlF+zosoF7w/W/Y3A6teXBiDF/wz6jhc9l2lr7UpzpNXXSukhShKP1HH6jULvZ1aECd4/uPhu6kSB4WLlyotm3bKiQkRL1799bHH3/s65BwHHv1pU5VO8VZbRVuudRJ56iRGmmfvvF1aECd4/sPtyzHNn/k8+Th1Vdf1dSpUzVjxgzt2LFD3bp108CBA5Wbm+vr0FANt+3WQeUpWi08bZZlKVoxytOPPowMqHt8/yH9eodJJzZ/5PPkYe7cuZowYYLGjRunLl26KC0tTU2aNNHzzz/v69BQjTKVyJatIIV4tQcpWKWibIvfN77/gI8XTJaWlmr79u269957PW0BAQFKSkrS5s2bK/UvKSlRSUmJ53VBQUG9xAkAwG+xYNKHfvjhB5WXlysmJsarPSYmRtnZ2ZX6p6amKjIy0rPFx8fXV6j4jcYKliWr0l9ZpSqp9NcY8HvD9x/SL2sebAc21jzUvXvvvVf5+fmeLSsry9chNUgBVoAiFKUD+nVdim3bOqBcRamZDyMD6h7ff8DH0xbNmzdXo0aNlJOT49Wek5Oj2NjYSv2Dg4MVHBxcX+HhGFrrDP1XW+WymypS0dqrPSrXYbVUW1+HBtQ5vv+wHbpSwvbTyoNPk4egoCD16NFDa9eu1YgRIyRJbrdba9euVUpKii9Dw3HEWvEqs0v0tf6rEhUrQpE6W+cr2KJsi98/vv9o6I/k9vkdJqdOnark5GT17NlT5557rubNm6eioiKNGzfO16HhOOKtDopXB1+HAfgE3380ZD5PHq688krt379fDzzwgLKzs9W9e3etXr260iJKAABOFg39agufJw+SlJKSwjQFAMBvNPRpC/9MeQAAgM+cFJUHAAD8iVPPpfDX+zyQPAAAYIhpCwAAAAMkDwAAGHLk1tQnUL3YuHGjhg0bpri4OFmWpZUrV3r2lZWV6e6771bXrl0VFhamuLg4XXfdddq3b5/XGAcOHNCYMWPkcrkUFRWl8ePHq7Cw0CgOkgcAAAz5KnkoKipSt27dtHDhwkr7Dh06pB07dmj69OnasWOHXn/9de3evVuXXnqpV78xY8Zo586dWrNmjVatWqWNGzdq4sSJRnGw5gEAAD8xaNAgDRo0qMp9kZGRWrNmjVfb008/rXPPPVd79+5V69attWvXLq1evVpbt25Vz549JUkLFizQ4MGD9fjjjysuLq5GcVB5AADAkK8qD6by8/NlWZaioqIkSZs3b1ZUVJQncZCkpKQkBQQEaMuWLTUel8oDAACGbDlzmaX9y/8WFBR4tTvxIMji4mLdfffduuqqq+RyuSRJ2dnZatGihVe/wMBARUdHKzs7u8ZjU3kAAMDH4uPjFRkZ6dlSU1NrNV5ZWZmuuOIK2batxYsXOxTlr6g8AABgyOn7PGRlZXmqA5JqVXWoSBy+/fZbrVu3zmvc2NhY5ebmevU/fPiwDhw4oNjY2Bofg+QBAABDTicPLpfL65f8iapIHPbs2aP3339fzZo189qfkJCgvLw8bd++XT169JAkrVu3Tm63W717967xcUgeAADwE4WFhcrIyPC8zszMVHp6uqKjo9WyZUtdfvnl2rFjh1atWqXy8nLPOobo6GgFBQWpc+fOuuSSSzRhwgSlpaWprKxMKSkpGj16dI2vtJBIHgAAMOar21Nv27ZN/fv397yeOnWqJCk5OVkzZ87UP//5T0lS9+7dvd73/vvvKzExUZK0bNkypaSkaMCAAQoICNCoUaP01FNPGcVB8gAAgCFfJQ+JiYmybbva/cfaVyE6OlrLly83Ou7RuNoCAAAYofIAAIAh27ZkO1B5cGIMXyB5AADAkFuWIzeJcmIMX2DaAgAAGKHyAACAIV8tmDxZkDwAAGCooa95YNoCAAAYofIAAIAhpi0AAIARpi0AAAAMUHkAAMCQ7dC0hb9WHkgeAAAwZEuqwWMkajSOP2LaAgAAGKHyAACAIbcsWQ349tQkDwAAGOJqCwAAAANUHgAAMOS2LVncJAoAANSUbTt0tYWfXm7BtAUAADBC5QEAAEMNfcEkyQMAAIYaevLAtAUAADBC5QEAAENcbQEAAIxwtQUAAIABKg8AABg6UnlwYsGkA8H4AMkDAACGuNoCAADAAJUHAAAM2b9sTozjj0geAAAwxLQFAACAASoPAACYauDzFiQPAACYcmjaQkxbAACAhoDKAwAAhrg9NQAAMFJxtYUTm4mNGzdq2LBhiouLk2VZWrly5VFx2XrggQfUsmVLhYaGKikpSXv27PHqc+DAAY0ZM0Yul0tRUVEaP368CgsLjeIgeQAAwE8UFRWpW7duWrhwYZX758yZo6eeekppaWnasmWLwsLCNHDgQBUXF3v6jBkzRjt37tSaNWu0atUqbdy4URMnTjSKg2kLAABM2ZYzix0Nxxg0aJAGDRpU9VC2rXnz5un+++/X8OHDJUkvvviiYmJitHLlSo0ePVq7du3S6tWrtXXrVvXs2VOStGDBAg0ePFiPP/644uLiahQHlQcAAAxVrHlwYpOkgoICr62kpMQ4pszMTGVnZyspKcnTFhkZqd69e2vz5s2SpM2bNysqKsqTOEhSUlKSAgICtGXLlhofi+QBAAAfi4+PV2RkpGdLTU01HiM7O1uSFBMT49UeExPj2Zedna0WLVp47Q8MDFR0dLSnT00wbQEAgCmHbxKVlZUll8vlaQ4ODnZg8LpD8gAAgCGnn23hcrm8kocTERsbK0nKyclRy5YtPe05OTnq3r27p09ubq7X+w4fPqwDBw543l8TTFsAAPA70K5dO8XGxmrt2rWetoKCAm3ZskUJCQmSpISEBOXl5Wn79u2ePuvWrZPb7Vbv3r1rfCwqDwAAnAgf3OCpsLBQGRkZnteZmZlKT09XdHS0WrdurcmTJ2v27Nk6/fTT1a5dO02fPl1xcXEaMWKEJKlz58665JJLNGHCBKWlpamsrEwpKSkaPXp0ja+0kEgeAAAw5qtHcm/btk39+/f3vJ46daokKTk5WUuXLtVdd92loqIiTZw4UXl5eTr//PO1evVqhYSEeN6zbNkypaSkaMCAAQoICNCoUaP01FNPGcVh2ba/3hzzSDkmMjJSiRquQKuxr8MBANSjw3aZ1utN5efn13q9QE1V/N6Jf2aGAkJDjv+G43D/XKysGx+s13NwApUHAABMNfBHcrNgEgAAGKHyAACAMeuXzYlx/A/JAwAAppi2AAAAqDkqDwAAmGrglQeSBwAATPnokdwnC6YtAACAESoPAAAYsu0jmxPj+COSBwAATDXwNQ9MWwAAACNUHgAAMNXAF0ySPAAAYMiyj2xOjOOPmLYAAABGqDwAAGCKBZPmPvjgA11zzTVKSEjQd999J0l66aWXtGnTJkeDAwDgpFSx5sGJzQ8ZJw+vvfaaBg4cqNDQUP3nP/9RSUmJJCk/P1+PPPKI4wECAICTi3HyMHv2bKWlpemvf/2rGjdu7Gnv06ePduzY4WhwAACclGwHNz9kvOZh9+7d6tu3b6X2yMhI5eXlORETAAAnN9Y8mImNjVVGRkal9k2bNum0005zJCgAAHDyMk4eJkyYoNtvv11btmyRZVnat2+fli1bpjvvvFM333xzXcQIAMDJhWkLM/fcc4/cbrcGDBigQ4cOqW/fvgoODtadd96pW2+9tS5iBADg5MIdJs1YlqX77rtP06ZNU0ZGhgoLC9WlSxeFh4fXRXwAAOAkc8I3iQoKClKXLl2cjAUAAL/Q0G9PbZw89O/fX5ZVfZll3bp1tQoIAICTXgO/2sI4eejevbvX67KyMqWnp+vzzz9XcnKyU3EBAICTlHHy8OSTT1bZPnPmTBUWFtY6IAAAcHJz7Kma11xzjZ5//nmnhgMA4KRl6dd1D7XafH0iJ8ixp2pu3rxZISEhTg1n5I0vP5MrgqeLo+G5bV8vX4cA+ExpobS+n6+jaJiMk4eRI0d6vbZtW99//722bdum6dOnOxYYAAAnLe7zYCYyMtLrdUBAgDp27KhZs2bp4osvdiwwAABOWlxtUXPl5eUaN26cunbtqqZNm9ZVTAAA4CRmtFCgUaNGuvjii3l6JgCgYWvgz7YwXmV41lln6euvv66LWAAA8AuOXGnh0F0qfcE4eZg9e7buvPNOrVq1St9//70KCgq8NgAA4Lzy8nJNnz5d7dq1U2hoqNq3b6+HHnpItv1rBmLbth544AG1bNlSoaGhSkpK0p49exyPpcbJw6xZs1RUVKTBgwfrk08+0aWXXqpWrVqpadOmatq0qaKiolgHAQBoGHwwbfHoo49q8eLFevrpp7Vr1y49+uijmjNnjhYsWODpM2fOHD311FNKS0vTli1bFBYWpoEDB6q4uLjWp/xbNV4w+eCDD+qmm27S+++/72gAAAD4HR9cbfHhhx9q+PDhGjJkiCSpbdu2euWVV/Txxx8fGcq2NW/ePN1///0aPny4JOnFF19UTEyMVq5cqdGjRzsQ8BE1Th4qyiL9+nFHDgAA6tt5552nZ599Vl9++aXOOOMMffLJJ9q0aZPmzp0rScrMzFR2draSkpI874mMjFTv3r21efNm3yQPko75NE0AABoKpx/JffSaweDgYAUHB3u13XPPPSooKFCnTp3UqFEjlZeX6+GHH9aYMWMkSdnZ2ZKkmJgYr/fFxMR49jnFKHk444wzjptAHDhwoFYBAQBw0nP4DpPx8fFezTNmzNDMmTO92v7+979r2bJlWr58uc4880ylp6dr8uTJiouLq/enWhslDw8++GClO0wCAIDaycrKksvl8rw+uuogSdOmTdM999zjmX7o2rWrvv32W6Wmpio5OVmxsbGSpJycHLVs2dLzvpycHHXv3t3ReI2Sh9GjR6tFixaOBgAAgN9xeMGky+XySh6qcujQIQUEeF8k2ahRI7ndbklSu3btFBsbq7Vr13qShYKCAm3ZskU333yzA8H+qsbJA+sdAAA4wuk1DzUxbNgwPfzww2rdurXOPPNM/ec//9HcuXN1/fXXHxnLsjR58mTNnj1bp59+utq1a6fp06crLi5OI0aMqH2wv2F8tQUAAKh/CxYs0PTp03XLLbcoNzdXcXFxuvHGG/XAAw94+tx1110qKirSxIkTlZeXp/PPP1+rV69WSEiIo7HUOHmoKIsAANDg+eA+DxEREZo3b57mzZtXbR/LsjRr1izNmjWr9rEdg/EjuQEAaPCcei6Fnxb1jZ9tAQAAGjYqDwAAmPLBtMXJhOQBAABTDTx5YNoCAAAYofIAAIAhX9zn4WRC5QEAABgheQAAAEaYtgAAwFQDXzBJ8gAAgCHWPAAAABig8gAAwInw06qBE0geAAAw1cDXPDBtAQAAjFB5AADAUENfMEnyAACAKaYtAAAAao7KAwAAhpi2AAAAZpi2AAAAqDkqDwAAmGrglQeSBwAADDX0NQ9MWwAAACNUHgAAMMW0BQAAMNLAkwemLQAAgBEqDwAAGGroCyZJHgAAMMW0BQAAQM1ReQAAwBDTFgAAwAzTFgAAADVH5QEAAFMNvPJA8gAAgCHrl82JcfwR0xYAAMAIyQMAAKZsBzcD3333na655ho1a9ZMoaGh6tq1q7Zt2/ZrWLatBx54QC1btlRoaKiSkpK0Z8+eWp1qVUgeAAAwVHGpphNbTf3000/q06ePGjdurH/961/673//qyeeeEJNmzb19JkzZ46eeuoppaWlacuWLQoLC9PAgQNVXFzs6Pmz5gEAAD/w6KOPKj4+XkuWLPG0tWvXzvPftm1r3rx5uv/++zV8+HBJ0osvvqiYmBitXLlSo0ePdiwWKg8AAJhyeNqioKDAayspKal0yH/+85/q2bOn/vSnP6lFixY6++yz9de//tWzPzMzU9nZ2UpKSvK0RUZGqnfv3tq8ebOjp0/yAADAiXBwvUN8fLwiIyM9W2pqaqXDff3111q8eLFOP/10vfvuu7r55pt122236YUXXpAkZWdnS5JiYmK83hcTE+PZ5xSmLQAA8LGsrCy5XC7P6+Dg4Ep93G63evbsqUceeUSSdPbZZ+vzzz9XWlqakpOT6y1WicoDAADGnF4w6XK5vLaqkoeWLVuqS5cuXm2dO3fW3r17JUmxsbGSpJycHK8+OTk5nn1OIXkAAMCUDy7V7NOnj3bv3u3V9uWXX6pNmzaSjiyejI2N1dq1az37CwoKtGXLFiUkJJzASVaPaQsAAPzAlClTdN555+mRRx7RFVdcoY8//ljPPvusnn32WUmSZVmaPHmyZs+erdNPP13t2rXT9OnTFRcXpxEjRjgaC8kDAACGfPFI7l69eumNN97Qvffeq1mzZqldu3aaN2+exowZ4+lz1113qaioSBMnTlReXp7OP/98rV69WiEhIbUP9jdIHgAAMOWjB2MNHTpUQ4cOrXa/ZVmaNWuWZs2aVcvAjo01DwAAwAiVBwAADPli2uJkQvIAAIApH01bnCyYtgAAAEaoPAAAYKqBVx5IHgAAMNTQ1zwwbQEAAIxQeQAAwBTTFgAAwIRl27Ls2v/md2IMX2DaAgAAGKHyAACAKaYtAACACa62AAAAMEDlAQAAU0xbAAAAE0xbAAAAGKDyAACAKaYtAACACaYtAAAADFB5AADAFNMWAADAlL9OOTiBaQsAAGCEygMAAKZs+8jmxDh+iOQBAABDXG0BAABggMoDAACmuNoCAACYsNxHNifG8UdMWwAAACNUHuCtcS9ZYTdIjc+U1ShG7p9ulkr+7d2nUXtZEdOkoHMlNZLKM2T/lCK5v/9lf2tZEXdLQT0lBUklG2UfnCW5f6zvswGMtQ/vpKSYoWod2k6RQU317Fdz9Wn+Ns/+wS1H6ZymCWraOFrldrn2HsrUW/te1beHvvL0GRg7XGe6zlarJm102H1Yd306wRengrrUwKctfFp52Lhxo4YNG6a4uDhZlqWVK1f6MhxIkhUqHf5CdsGDVe9v1FpWs1ekw1/LPnCN7B+HyS5cKKnE836r6RJJkn3gWtkHrpSsxrKinpFk1cspALURHBCs7w59q1ezllS5P7f4e63IWqpHdt2juV/O1IHS/Uo5/V6FB0Z4+jSyAvWfvC36YP+/qxwD/q/iagsnNn/k08pDUVGRunXrpuuvv14jR470ZSioULpRdunGandb4VOkkg2yC+f82li+99f/btxDanSq7B+HS3ahJMnOv0tWi+1SUIJU+mFdRQ444r8Fn+i/BZ9Uu3/bT97f4df/97LOa95fcaGt9eXBnZKkd75/TZLUO7pv3QUK+JBPk4dBgwZp0KBBvgwBRiwpOFF20d9kNX1eCuwilf9PdlHar1MbVpAkW7JLf32bXSrJLSuoh2ySB/yONLIaqU/zC3XocJG+O7T3+G/A7wc3iQJqKKCZrIBwKWyi7MInpYOPScEXyIpaKPvAtVLZx1JpumT/LCtimuyDT0iWJSv8TllWoOyAFr4+A8ARZ7nO1rh2t6pxQJAKyvL0dEaqisoP+jos1CNuEuVHSkpKVFBQ4LWhPv3ydSlZKx1aKh3eJRU9K5W8L6vJVUf22Qdk590mBV8oK+YTWS12SAEu2WWfS/LTa5KAo3xZ+F+lfnGv5u6eqf8WfKLr292m8ECXr8MC6o1fJQ+pqamKjIz0bPHx8b4OqWFx/yTbLpN9OMO7/fBXUqOWv74u3ST7hwGyc/8oO/dc2fnTpIAY2Yez6jdeoI6Uukv0Q0mOvjmUoeV7/yq37dZ5zRJ9HRbqk+3gdoL+8pe/yLIsTZ482dNWXFysSZMmqVmzZgoPD9eoUaOUk5Nz4gephl8lD/fee6/y8/M9W1YWv4zqV5lU9pmswHbezYFtpfJ9lbvbP0n2QSnoj1JAsyMVC+B3yLIsBQY09nUYqEe+vtpi69ateuaZZ/SHP/zBq33KlCl66623tGLFCm3YsEH79u2rkwsS/GrNQ3BwsIKDg30dxu+b1URq1ObX141aSYGdJXee5P7+yGLJqHlS6Vap9CMpuK8UfKHsA9f8+p7QUUeqEe4DUuPuslz3S4eWSOWZ9X02gLGggGCdEhzred0s+BSdGtpGhw4Xqqi8UANjR+izvO3KP5yn8EYR6nvKRYpq3FQ7fvrI856mjZupSWC4mgY1U4AVoFNDj/xM7S/JVqm7pN7PCb8vhYWFGjNmjP76179q9uzZnvb8/Hw999xzWr58uS688EJJ0pIlS9S5c2d99NFH+uMf/+hYDD5NHgoLC5WR8WsJPDMzU+np6YqOjlbr1q19GFkD1vgsBUQv87wMcN0nSbJ/fl12/t1SyRrZBTNkhd0ouaZLhzNl56VIZds977EatZPC75ACIqXy72QXLj6SPAB+oE2T03T7GdM9r0e1ulaS9NGPG/T/9j6vmJCW6n3aZIUFRujQ4UJ9e+grPfnlLGUXf+d5z5C4y/XHZv08r+/tnCpJmv/lQ9pTuKuezgR1yodXW0yaNElDhgxRUlKSV/Kwfft2lZWVKSkpydPWqVMntW7dWps3b/79JA/btm1T//79Pa+nTp0qSUpOTtbSpUt9FFUDV/qx3NmnH7vPz/+Q/fM/qt1tFz4uFT7ucGBA/dhTuEspO66udv/fvp533DFe/vYZvfztMw5GhZON01dbHH0BQHWV9v/3//6fduzYoa1bt1bal52draCgIEVFRXm1x8TEKDs7u/bB/oZPk4fExETZfnqNKwAATjn6AoAZM2Zo5syZXm1ZWVm6/fbbtWbNGoWEhNRjdJX51ZoHAABOCg4/2yIrK0su16+X+1ZVddi+fbtyc3N1zjnneNrKy8u1ceNGPf3003r33XdVWlqqvLw8r+pDTk6OYmNjK41XGyQPAAAYcnrawuVyeSUPVRkwYIA+++wzr7Zx48apU6dOuvvuuxUfH6/GjRtr7dq1GjVqlCRp9+7d2rt3rxISEmof7G+QPAAA4AciIiJ01llnebWFhYWpWbNmnvbx48dr6tSpio6Olsvl0q233qqEhARHF0tKJA8AAJhz20c2J8Zx0JNPPqmAgACNGjVKJSUlGjhwoBYtWuToMSSSBwAAzDm85uFErV+/3ut1SEiIFi5cqIULF9Zu4OPwqztMAgAA36PyAACAIUsOLZis/RA+QfIAAIApH95h8mTAtAUAADBC5QEAAENO3+fB35A8AABg6iS52sJXmLYAAABGqDwAAGDIsm1ZDix2dGIMXyB5AADAlPuXzYlx/BDTFgAAwAiVBwAADDFtAQAAzHC1BQAAQM1ReQAAwFQDvz01yQMAAIYa+h0mmbYAAABGqDwAAGCKaQsAAGDCch/ZnBjHHzFtAQAAjFB5AADAFNMWAADACDeJAgAAqDkqDwAAGOLZFgAAwEwDX/PAtAUAADBC5QEAAFO2JCfu0eCfhQeSBwAATDX0NQ9MWwAAACNUHgAAMGXLoQWTtR/CF0geAAAwxdUWAAAANUflAQAAU25JlkPj+CGSBwAADHG1BQAAgAEqDwAAmGLBJAAAMFKRPDix1VBqaqp69eqliIgItWjRQiNGjNDu3bu9+hQXF2vSpElq1qyZwsPDNWrUKOXk5Dh99iQPAAD4gw0bNmjSpEn66KOPtGbNGpWVleniiy9WUVGRp8+UKVP01ltvacWKFdqwYYP27dunkSNHOh4L0xYAAJjywbTF6tWrvV4vXbpULVq00Pbt29W3b1/l5+frueee0/Lly3XhhRdKkpYsWaLOnTvro48+0h//+Mfax/sLKg8AAJhyO7hJKigo8NpKSkqOG0J+fr4kKTo6WpK0fft2lZWVKSkpydOnU6dOat26tTZv3lzbM/ZC8gAAgI/Fx8crMjLSs6Wmph6zv9vt1uTJk9WnTx+dddZZkqTs7GwFBQUpKirKq29MTIyys7MdjZdpCwAADDl9n4esrCy5XC5Pe3Bw8DHfN2nSJH3++efatGlTrWM4ESQPAACYcnjNg8vl8koejiUlJUWrVq3Sxo0b1apVK097bGysSktLlZeX51V9yMnJUWxsbO1j/Q2mLQAA8AO2bSslJUVvvPGG1q1bp3bt2nnt79Gjhxo3bqy1a9d62nbv3q29e/cqISHB0VioPAAAYMptS5YDlQd3zceYNGmSli9frjfffFMRERGedQyRkZEKDQ1VZGSkxo8fr6lTpyo6Oloul0u33nqrEhISHL3SQiJ5AADAnA8u1Vy8eLEkKTEx0at9yZIlGjt2rCTpySefVEBAgEaNGqWSkhINHDhQixYtqn2cRyF5AADAD9g1SDRCQkK0cOFCLVy4sE5jIXkAAMCYQ5UH+eezLUgeAAAwxYOxAAAAao7KAwAApty2HJlyMLja4mRC8gAAgCnbfWRzYhw/xLQFAAAwQuUBAABTDXzBJMkDAACmGviaB6YtAACAESoPAACYYtoCAAAYseVQ8lD7IXyBaQsAAGCEygMAAKaYtgAAAEbcbkkO3ODJzU2iAABAA0DlAQAAU0xbAAAAIw08eWDaAgAAGKHyAACAqQZ+e2qSBwAADNm2W7YDj9N2YgxfYNoCAAAYofIAAIAp23ZmysFPF0ySPAAAYMp2aM2DnyYPTFsAAAAjVB4AADDldkuWA4sd/XTBJMkDAACmmLYAAACoOSoPAAAYst1u2Q5MW/jrfR5IHgAAMMW0BQAAQM1ReQAAwJTblqyGW3kgeQAAwJRtS3LiUk3/TB6YtgAAAEaoPAAAYMh227IdmLawqTwAANBA2G7nNkMLFy5U27ZtFRISot69e+vjjz+ugxM8NpIHAAD8xKuvvqqpU6dqxowZ2rFjh7p166aBAwcqNze3XuMgeQAAwJDtth3bTMydO1cTJkzQuHHj1KVLF6WlpalJkyZ6/vnn6+hMq0byAACAKR9MW5SWlmr79u1KSkrytAUEBCgpKUmbN2+ui7Osll8vmKxYaFJQ6J+39wRqq7SwzNchAD5TWnTk+++LRYeHVebIDSYP68g5FBQUeLUHBwcrODjYq+2HH35QeXm5YmJivNpjYmL0xRdf1D4YA36dPBw8eFCS1Oacb3wbCOAzX/s6AMDnDh48qMjIyHo5VlBQkGJjY7Up+x3HxgwPD1d8fLxX24wZMzRz5kzHjuE0v04e4uLilJWVpYiICFmW5etwGpyCggLFx8crKytLLpfL1+EA9Yrvv+/Ztq2DBw8qLi6u3o4ZEhKizMxMlZaWOjambduVfocdXXWQpObNm6tRo0bKycnxas/JyVFsbKxj8dSEXycPAQEBatWqla/DaPBcLhf/54kGi++/b9VXxeG3QkJCFBISUu/HDQoKUo8ePbR27VqNGDFCkuR2u7V27VqlpKTUayx+nTwAANCQTJ06VcnJyerZs6fOPfdczZs3T0VFRRo3bly9xkHyAACAn7jyyiu1f/9+PfDAA8rOzlb37t21evXqSoso6xrJA05YcHCwZsyYUeXcHPB7x/cfvpKSklLv0xRHs2x/vbE2AADwCW4SBQAAjJA8AAAAIyQPAADACMkDTtjJ8FhYwBc2btyoYcOGKS4uTpZlaeXKlb4OCahXJA84ISfLY2EBXygqKlK3bt20cOFCX4cC+ARXW+CE9O7dW7169dLTTz8t6chdzuLj43Xrrbfqnnvu8XF0QP2xLEtvvPGG545/QENA5QHGTqbHwgIA6h/JA4wd67Gw2dnZPooKAFBfSB4AAIARkgcYO5keCwsAqH8kDzD228fCVqh4LGxCQoIPIwMA1AcejIUTcrI8FhbwhcLCQmVkZHheZ2ZmKj09XdHR0WrdurUPIwPqB5dq4oQ9/fTTeuyxxzyPhX3qqafUu3dvX4cF1Ln169erf//+ldqTk5O1dOnS+g8IqGckDwAAwAhrHgAAgBGSBwAAYITkAQAAGCF5AAAARkgeAACAEZIHAABghOQBAAAYIXkAAABGSB4APzF27FiNGDHC8zoxMVGTJ0+u9zjWr18vy7KUl5dX78cGcHIgeQBqaezYsbIsS5ZlKSgoSB06dNCsWbN0+PDhOj3u66+/roceeqhGffmFD8BJPBgLcMAll1yiJUuWqKSkRO+8844mTZqkxo0b69577/XqV1paqqCgIEeOGR0d7cg4AGCKygPggODgYMXGxqpNmza6+eablZSUpH/+85+eqYaHH35YcXFx6tixoyQpKytLV1xxhaKiohQdHa3hw4frm2++8YxXXl6uqVOnKioqSs2aNdNdd92lox9Dc/S0RUlJie6++27Fx8crODhYHTp00HPPPadvvvnG8xCnpk2byrIsjR07VtKRR6mnpqaqXbt2Cg0NVbdu3fSPf/zD6zjvvPOOzjjjDIWGhqp///5ecQJomEgegDoQGhqq0tJSSdLatWu1e/durVmzRqtWrVJZWZkGDhyoiIgIffDBB/q///s/hYeH65JLLvG854knntDSpUv1/PPPa9OmTTpw4IDeeOONYx7zuuuu0yuvvKKnnnpKu3bt0jPPPKPw8HDFx8frtddekyTt3r1b33//vebPny9JSk1N1Ysvvqi0tDTt3LlTU6ZM0TXXXKMNGzZIOpLkjBw5UsOGDVN6erpuuOEG3XPPPXX1sQHwFzaAWklOTraHDx9u27Ztu91ue82aNXZwcLB955132snJyXZMTIxdUlLi6f/SSy/ZHTt2tN1ut6etpKTEDg0Ntd99913btm27ZcuW9pw5czz7y8rK7FatWnmOY9u23a9fP/v222+3bdu2d+/ebUuy16xZU2WM77//vi3J/umnnzxtxcXFdpMmTewPP/zQq+/48ePtq666yrZt27733nvtLl26eO2/++67K40FoGFhzQPggFWrVik8PFxlZWVyu926+uqrNXPmTE2aNEldu3b1WufwySefKCMjQxEREV5jFBcX66uvvlJ+fr6+//579e7d27MvMDBQPXv2rDR1USE9PV2NGjVSv379ahxzRkaGDh06pIsuusirvbS0VGeffbYkadeuXV5xSFJCQkKNjwHg94nkAXBA//79tXjxYgUFBSkuLk6Bgb/+aIWFhXn1LSwsVI8ePbRs2bJK45xyyikndPzQ0FDj9xQWFkqS3n77bZ166qle+4KDg08oDgANA8kD4ICwsDB16NChRn3POeccvfrqq2rRooVcLleVfVq2bKktW7aob9++kqTDhw9r+/btOuecc6rs37VrV7ndbm3YsEFJSUmV9ldUPsrLyz1tXbp0UXBwsPbu3VttxaJz58765z//6dX20UcfHf8kAfyusWASqGdjxoxR8+bNNXz4cH3wwQfKzMzU+vXrddttt+l///ufJOn222/XX/7yF61cuVJffPGFbrnllmPeo6Ft27ZKTk7W9ddfr5UrV3rG/Pvf/y5JatOmjSzL0qpVq7R//34VFhYqIiJCd955p6ZMmaIXXnhBX331lXbs2KEFCxbohRdekCTddNNN2rNnj6ZNm6bdu3dr+fLlWrp0aV1/RABOciQPQD1r0qSJNm7cqNatW2vkyJHq3Lmzxo8fr+LiYk8l4o477tC1116r5ORkJSQkKCIiQpdddtkxx128eLEuv/xy3XLLLerUqZMmTJigoqIiSdKpp56qBx98UPfcc49iYmKUkpIiSXrooYc0ffp0paamqnPnzrrkkkv09ttvq127dpKk1q1b67XXXtPKlSvVrVs3paWl6ZFHHqnDTweAP7Ds6lZgAQAAVIHKAwAAMELyAAAAjJA8AAAAIyQPAADACMkDAAAwQvIAAACMkDwAAAAjJA8AAMAIyQMAADBC8gAAAIyQPAAAACMkDwAAwMj/B5+CT766fJgFAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Final model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================\n",
    "# Cell 6: Confusion Matrix & Metrics on TEST (FINAL EEGNet)\n",
    "# =======================================================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# ------------------------------------------------------------------\n",
    "def compute_metrics_from_cm(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    eps = 1e-12\n",
    "    acc  = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec  = tp / (tp + fn + eps)\n",
    "    f1   = 2 * prec * rec / (prec + rec + eps)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], ['0', '1'])\n",
    "    plt.yticks([0, 1], ['0', '1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(\n",
    "                j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Validate upstream artifacts\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[CHECK] Verifying predictions and labels are available...\")\n",
    "\n",
    "if 'y_pred' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Predictions 'y_pred' not found. \"\n",
    "        \"Please run Cell 5 (final model inference) first.\"\n",
    "    )\n",
    "\n",
    "if 'yte' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Test labels 'yte' not found. \"\n",
    "        \"Please run Cell 5 first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[CHECK] Found predictions for {len(y_pred)} test segments.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Confusion Matrix & Metrics (FINAL model)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc, prec, rec, f1 = compute_metrics_from_cm(cm)\n",
    "\n",
    "print(\"\\n[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(cm, title=\"Confusion Matrix - FINAL EEGNet\")\n",
    "\n",
    "print(\"[DONE] Final model evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0f120-39fd-497e-9098-05e6c82f4807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
