{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af5b471-cea9-4194-bc23-da5cdc2f9c67",
   "metadata": {},
   "source": [
    "Cell 1 — Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64aaa897-ab61-4eb4-a558-4c078acf751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Base path set to:\n",
      "C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\n",
      "[INFO] Found 18 folders:\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Selected test subject (LOSO): Mona\n",
      "[INFO] Training subjects (17):\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Sampling rate (SFREQ) = 128.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 1: Setup & discovery\n",
    "# =========================\n",
    "\n",
    "import os  # file ops\n",
    "import glob  # file matching\n",
    "import numpy as np  # arrays\n",
    "import random  # seeds\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data path\n",
    "BASE_PATH = r\"C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\"\n",
    "print(f\"[INFO] Base path set to:\\n{BASE_PATH}\")\n",
    "\n",
    "# List subject folders\n",
    "all_folders = [p for p in sorted(os.listdir(BASE_PATH)) if os.path.isdir(os.path.join(BASE_PATH, p))]\n",
    "print(f\"[INFO] Found {len(all_folders)} folders:\\n{all_folders}\")\n",
    "\n",
    "# Expected count (change if needed)\n",
    "assert len(all_folders) == 18, f\"Expected 18 folders, found {len(all_folders)}.\"\n",
    "\n",
    "# LOSO test subject\n",
    "TEST_SUBJECT = \"Mona\"\n",
    "print(f\"[INFO] Selected test subject (LOSO): {TEST_SUBJECT}\")\n",
    "\n",
    "# Train subjects = remaining\n",
    "train_subjects = [s for s in all_folders if s.lower() != TEST_SUBJECT.lower()]\n",
    "print(f\"[INFO] Training subjects ({len(train_subjects)}):\\n{train_subjects}\")\n",
    "\n",
    "# Sampling rate\n",
    "SFREQ = 128.0\n",
    "print(f\"[INFO] Sampling rate (SFREQ) = {SFREQ} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e649681-2861-4ce2-bbd1-0d9dd3359f42",
   "metadata": {},
   "source": [
    "Cell 2 — Load EEG + labels per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943c5c6e-a7aa-4618-899c-daf341c25358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 2] Loading data from folders...\n",
      "[STEP 2]       Amin | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Amin | EEG=          Amin.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Amin1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]      Amin1 | EEG=         Amin1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Cole | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Cole | EEG=          Cole.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Daniel | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]     Daniel | EEG=        Daniel.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]    Ismayil | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]    Ismayil | EEG=       Ismayil.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Jack | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Jack | EEG=          Jack.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]      James | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      James | EEG=         James.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Josh | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Josh | EEG=          Josh.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Marjan | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Marjan | EEG=        Marjan.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]        Max | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Max | EEG=           Max.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Mina | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mina | EEG=          Mina.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 1 | EEG=        Mina 1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 3 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 3 | EEG=        Mina 3.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]   Mohammad | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]   Mohammad | EEG=      Mohammad.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Mona | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mona | EEG=          Mona.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Roddy | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      Roddy | EEG=         Roddy.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]        Sam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Sam | EEG=           Sam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       adam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       adam | EEG=          adam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2] Loading complete ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 2: Load data (EMPTY LABEL => CLASS 0)\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path  # helper\n",
    "\n",
    "def load_subject(subj_dir: str, subj_name: str, expected_channels: int = 14):\n",
    "    \"\"\"\n",
    "    EEG file: {subj_name}.npy\n",
    "    Label file: {subj_name}_target.npy\n",
    "    Label rule:\n",
    "      - empty array -> class 0\n",
    "      - [0] -> class 0\n",
    "      - [1] -> class 1\n",
    "    \"\"\"\n",
    "\n",
    "    # EEG file path\n",
    "    eeg_file = os.path.join(subj_dir, f\"{subj_name}.npy\")\n",
    "    assert os.path.exists(eeg_file), f\"[ERROR] Missing EEG file: {eeg_file}\"\n",
    "\n",
    "    # Label file path\n",
    "    target_file = os.path.join(subj_dir, f\"{subj_name}_target.npy\")\n",
    "    assert os.path.exists(target_file), f\"[ERROR] Missing label file: {target_file}\"\n",
    "\n",
    "    # Load EEG\n",
    "    X_raw = np.load(eeg_file, allow_pickle=True)\n",
    "    X_raw = np.asarray(X_raw)\n",
    "\n",
    "    # Standardize EEG to (n_trials, 14, T)\n",
    "    if X_raw.ndim == 2:\n",
    "        X_trials = X_raw[None, :, :]\n",
    "    elif X_raw.ndim == 3:\n",
    "        dims = list(X_raw.shape)\n",
    "        chan_axes = [i for i, d in enumerate(dims) if d == expected_channels]\n",
    "        chan_axis = chan_axes[0] if len(chan_axes) == 1 else 1\n",
    "        time_axis = int(np.argmax(dims))\n",
    "        trial_axis = [i for i in range(3) if i not in (chan_axis, time_axis)][0]\n",
    "        X_trials = np.moveaxis(X_raw, (trial_axis, chan_axis, time_axis), (0, 1, 2))\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] Unsupported EEG shape {X_raw.shape} in {eeg_file}\")\n",
    "\n",
    "    assert X_trials.shape[1] == expected_channels, f\"[ERROR] Expected {expected_channels} channels, got {X_trials.shape[1]} in {eeg_file}\"\n",
    "\n",
    "    # Load label\n",
    "    y_raw = np.load(target_file, allow_pickle=True)\n",
    "    y_arr = np.asarray(y_raw).ravel()\n",
    "\n",
    "    # Apply your rule\n",
    "    if y_arr.size == 0:\n",
    "        label = 0\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\")\n",
    "    else:\n",
    "        label = int(y_arr[0])\n",
    "        if label not in (0, 1):\n",
    "            raise ValueError(f\"[ERROR] Invalid label {label} in {target_file} (must be 0 or 1)\")\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape={y_arr.shape}, value={y_arr} -> class={label}\")\n",
    "\n",
    "    # Repeat label for all trials\n",
    "    y_trials = np.full((X_trials.shape[0],), label, dtype=np.int32)\n",
    "\n",
    "    return X_trials.astype(np.float32), y_trials.astype(np.int32), eeg_file, target_file\n",
    "\n",
    "\n",
    "# Load all subjects\n",
    "subject_data = {}\n",
    "print(\"[STEP 2] Loading data from folders...\")\n",
    "\n",
    "for subj in all_folders:\n",
    "    subj_dir = os.path.join(BASE_PATH, subj)\n",
    "    X_trials, y_trials, eeg_file, target_file = load_subject(subj_dir, subj)\n",
    "\n",
    "    subject_data[subj] = (X_trials, y_trials)\n",
    "\n",
    "    print(f\"[STEP 2] {subj:>10} | EEG={Path(eeg_file).name:>18} | X={X_trials.shape} | y={y_trials.shape} | class={y_trials[0]}\")\n",
    "\n",
    "print(\"[STEP 2] Loading complete ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd93ca8-a94c-485c-8a61-e6e7441f6299",
   "metadata": {},
   "source": [
    "Cell 3 — Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733e592c-98e1-4cbd-afd2-6fdc8cf407f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 3] Preprocessing functions ready ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3: Preprocessing functions\n",
    "# =========================\n",
    "\n",
    "import mne  # EEG processing\n",
    "import pywt  # wavelets\n",
    "from sklearn.decomposition import FastICA  # ICA\n",
    "from typing import Optional, Tuple, Union, Sequence, Dict, List  # typing\n",
    "\n",
    "def wavelet_enhanced_ica(\n",
    "    data: np.ndarray,\n",
    "    n_components: int = 10,\n",
    "    wavelet: str = \"db4\",\n",
    "    level: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> np.ndarray:\n",
    "    n_ch, n_t = data.shape\n",
    "    n_components = min(n_components, n_ch)\n",
    "\n",
    "    coeffs = pywt.wavedec(data, wavelet=wavelet, level=level, axis=1)\n",
    "    A = coeffs[0]\n",
    "\n",
    "    ica = FastICA(n_components=n_components, random_state=random_state)\n",
    "    S = ica.fit_transform(A.T).T\n",
    "    A_denoised = ica.inverse_transform(S.T).T\n",
    "\n",
    "    coeffs[0] = A_denoised\n",
    "    cleaned = pywt.waverec(coeffs, wavelet=wavelet, axis=1)\n",
    "\n",
    "    if cleaned.shape[1] != n_t:\n",
    "        if cleaned.shape[1] > n_t:\n",
    "            cleaned = cleaned[:, :n_t]\n",
    "        else:\n",
    "            cleaned = np.pad(cleaned, ((0, 0), (0, n_t - cleaned.shape[1])), mode=\"constant\")\n",
    "    return cleaned\n",
    "\n",
    "def _names_from_index_mapping(n_channels: int, index_to_name: Optional[Dict[int, str]]) -> List[str]:\n",
    "    if index_to_name is None:\n",
    "        return [f\"EEG{i+1}\" for i in range(n_channels)]\n",
    "    keys = list(index_to_name.keys())\n",
    "    is_zero_based = (0 in keys) and (1 not in keys)\n",
    "    names = []\n",
    "    for i in range(n_channels):\n",
    "        key = i if is_zero_based else (i + 1)\n",
    "        names.append(index_to_name.get(key, f\"EEG{i+1}\"))\n",
    "    return names\n",
    "\n",
    "def preprocess_eeg(\n",
    "    eeg: np.ndarray,\n",
    "    sfreq: float,\n",
    "    *,\n",
    "    index_to_name: Optional[Dict[int, str]] = None,\n",
    "    use_standard_1010: bool = True,\n",
    "    resample_to: Optional[float] = None,\n",
    "    notch_freqs: Union[None, float, Sequence[float]] = 50.0,\n",
    "    highpass: Optional[float] = 0.05,\n",
    "    bad_point_z: float = 6.0,\n",
    "    bad_channel_z: float = 5.0,\n",
    "    interpolate_bad_channels: bool = True,\n",
    "    car: bool = True,\n",
    "    use_wica: bool = True,\n",
    "    wica_components: int = 10,\n",
    "    wica_wavelet: str = \"db4\",\n",
    "    wica_level: int = 3,\n",
    "    wica_random_state: int = 42,\n",
    "    return_raw: bool = False,\n",
    "):\n",
    "\n",
    "    eeg = np.asarray(eeg, dtype=np.float32)\n",
    "    assert eeg.ndim == 2, \"eeg must be 2D: (n_channels, n_times)\"\n",
    "\n",
    "    n_channels, _ = eeg.shape\n",
    "    ch_names = _names_from_index_mapping(n_channels, index_to_name)\n",
    "    ch_types = ['eog' if str(n).upper().startswith(\"EOG\") else 'eeg' for n in ch_names]\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    raw = mne.io.RawArray(eeg, info, verbose=False)\n",
    "\n",
    "    montage_applied = False\n",
    "    if use_standard_1010:\n",
    "        try:\n",
    "            mont = mne.channels.make_standard_montage(\"standard_1010\")\n",
    "            raw.set_montage(mont, match_case=False, on_missing=\"ignore\")\n",
    "            montage_applied = True\n",
    "        except Exception:\n",
    "            montage_applied = False\n",
    "\n",
    "    if resample_to is not None and float(resample_to) != float(sfreq):\n",
    "        raw.resample(sfreq=resample_to, npad=\"auto\")\n",
    "    sfreq_out = float(raw.info[\"sfreq\"])\n",
    "\n",
    "    if notch_freqs is not None:\n",
    "        raw.notch_filter(freqs=notch_freqs, verbose=False)\n",
    "    if highpass is not None:\n",
    "        raw.filter(l_freq=highpass, h_freq=None, verbose=False)\n",
    "\n",
    "    X = raw.get_data()\n",
    "    mu = np.mean(X, axis=1, keepdims=True)\n",
    "    sd = np.std(X, axis=1, keepdims=True) + 1e-12\n",
    "    hi = mu + bad_point_z * sd\n",
    "    lo = mu - bad_point_z * sd\n",
    "    bad_idx = (X > hi) | (X < lo)\n",
    "\n",
    "    if np.any(bad_idx):\n",
    "        X_fixed = X.copy()\n",
    "        t = np.arange(X.shape[1], dtype=float)\n",
    "        for ch in range(n_channels):\n",
    "            mask = bad_idx[ch]\n",
    "            if mask.any():\n",
    "                good = ~mask\n",
    "                if good.sum() >= 2:\n",
    "                    X_fixed[ch, mask] = np.interp(t[mask], t[good], X_fixed[ch, good])\n",
    "        raw._data = X_fixed\n",
    "\n",
    "    if interpolate_bad_channels and montage_applied:\n",
    "        X = raw.get_data(picks=\"eeg\")\n",
    "        if X.size > 0:\n",
    "            ch_std = X.std(axis=1)\n",
    "            med = np.median(ch_std)\n",
    "            mad = np.median(np.abs(ch_std - med)) + 1e-12\n",
    "            z = 0.6745 * (ch_std - med) / mad\n",
    "            eeg_names = mne.pick_info(raw.info, mne.pick_types(raw.info, eeg=True)).ch_names\n",
    "            bads = [eeg_names[i] for i in np.where(np.abs(z) > bad_channel_z)[0]]\n",
    "            raw.info[\"bads\"] = bads\n",
    "            if bads:\n",
    "                raw.interpolate_bads(reset_bads=True, verbose=False)\n",
    "\n",
    "    if car:\n",
    "        raw.set_eeg_reference(\"average\", projection=True)\n",
    "        raw.apply_proj()\n",
    "\n",
    "    if use_wica:\n",
    "        cleaned = wavelet_enhanced_ica(\n",
    "            raw.get_data(),\n",
    "            n_components=wica_components,\n",
    "            wavelet=wica_wavelet,\n",
    "            level=wica_level,\n",
    "            random_state=wica_random_state\n",
    "        )\n",
    "        raw = mne.io.RawArray(cleaned, raw.info, verbose=False)\n",
    "\n",
    "    cleaned = raw.get_data()\n",
    "    return cleaned, sfreq_out\n",
    "\n",
    "print(\"[STEP 3] Preprocessing functions ready ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a9d20-889d-45c4-bbc2-de62216908cd",
   "metadata": {},
   "source": [
    "Cell 4 — Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bfcbde0-1cc2-409c-8627-93da46606661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 4] Starting preprocessing...\n",
      "[INFO] USE_WICA = True\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 01/17: Amin | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 02/17: Amin1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 03/17: Cole | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 04/17: Daniel | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 05/17: Ismayil | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 06/17: Jack | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 07/17: James | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 08/17: Josh | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 09/17: Marjan | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 10/17: Max | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 11/17: Mina | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 12/17: Mina 1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 13/17: Mina 3 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 14/17: Mohammad | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 15/17: Roddy | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 16/17: Sam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 17/17: adam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Test subject: Mona | trials=1 | fs_out=128.0\n",
      "[STEP 4] Preprocessing done ✅\n",
      "  X_train_trials_all: (17, 14, 38400)\n",
      "  y_train_trials_all: (17,) (array([0, 1], dtype=int32), array([8, 9]))\n",
      "  X_test_trials_all : (1, 14, 38400)\n",
      "  y_test_trials_all : (1,) (array([0], dtype=int32), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 4: Apply preprocessing (LOSO)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 4] Starting preprocessing...\")\n",
    "\n",
    "USE_WICA = True\n",
    "print(f\"[INFO] USE_WICA = {USE_WICA}\")\n",
    "\n",
    "X_train_trials_all, y_train_trials_all = [], []\n",
    "X_test_trials_all, y_test_trials_all = [], []\n",
    "\n",
    "def preprocess_trials(X_trials, y_trials):\n",
    "    cleaned_list = []\n",
    "    label_list = []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        trial = X_trials[i]\n",
    "        label = int(y_trials[i])\n",
    "\n",
    "        cleaned, fs_out = preprocess_eeg(\n",
    "            eeg=trial,\n",
    "            sfreq=SFREQ,\n",
    "            use_wica=USE_WICA\n",
    "        )\n",
    "\n",
    "        cleaned_list.append(cleaned.astype(np.float32, copy=False))\n",
    "        label_list.append(label)\n",
    "\n",
    "    return cleaned_list, np.array(label_list, dtype=np.int32), fs_out\n",
    "\n",
    "# Train subjects\n",
    "for idx, subj in enumerate(train_subjects, start=1):\n",
    "    X_trials, y_trials = subject_data[subj]\n",
    "    cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "    X_train_trials_all.extend(cleaned_trials)\n",
    "    y_train_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "    print(f\"[STEP 4] Train {idx:02d}/{len(train_subjects)}: {subj} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Test subject\n",
    "X_trials, y_trials = subject_data[TEST_SUBJECT]\n",
    "cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "X_test_trials_all.extend(cleaned_trials)\n",
    "y_test_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "print(f\"[STEP 4] Test subject: {TEST_SUBJECT} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Convert to arrays\n",
    "X_train_trials_all = np.array(X_train_trials_all, dtype=np.float32)\n",
    "y_train_trials_all = np.array(y_train_trials_all, dtype=np.int32)\n",
    "\n",
    "X_test_trials_all = np.array(X_test_trials_all, dtype=np.float32)\n",
    "y_test_trials_all = np.array(y_test_trials_all, dtype=np.int32)\n",
    "\n",
    "print(\"[STEP 4] Preprocessing done ✅\")\n",
    "print(\"  X_train_trials_all:\", X_train_trials_all.shape)\n",
    "print(\"  y_train_trials_all:\", y_train_trials_all.shape, np.unique(y_train_trials_all, return_counts=True))\n",
    "print(\"  X_test_trials_all :\", X_test_trials_all.shape)\n",
    "print(\"  y_test_trials_all :\", y_test_trials_all.shape, np.unique(y_test_trials_all, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc3da1-2967-4ee3-84cd-bfcf0f98c0b1",
   "metadata": {},
   "source": [
    "Cell 5 — Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c21ad54-7001-4039-9360-29dfa2905127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 5] Starting segmentation...\n",
      "[STEP 5] Segmented 5/17 trials...\n",
      "[STEP 5] Segmented 10/17 trials...\n",
      "[STEP 5] Segmented 15/17 trials...\n",
      "[STEP 5] Segmented 17/17 trials...\n",
      "[STEP 5] Segmented 1/1 trials...\n",
      "[STEP 5] Segmentation done ✅\n",
      "  X_train: (5100, 14, 128, 1)\n",
      "  y_train: (5100,) (array([0, 1], dtype=int32), array([2400, 2700]))\n",
      "  X_test : (300, 14, 128, 1)\n",
      "  y_test : (300,) (array([0], dtype=int32), array([300]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 5: Segmentation\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 5] Starting segmentation...\")\n",
    "\n",
    "def segment_trial(trial_2d: np.ndarray, label: int, segment_len: int = 128):\n",
    "    C, T = trial_2d.shape\n",
    "    n_segs = T // segment_len\n",
    "    use_T = n_segs * segment_len\n",
    "\n",
    "    if n_segs == 0:\n",
    "        return np.empty((0, C, segment_len), dtype=np.float32), np.empty((0,), dtype=np.int32)\n",
    "\n",
    "    trial_trim = trial_2d[:, :use_T]\n",
    "    Xsegs = trial_trim.reshape(C, n_segs, segment_len).transpose(1, 0, 2)\n",
    "    ysegs = np.full((n_segs,), int(label), dtype=np.int32)\n",
    "    return Xsegs.astype(np.float32, copy=False), ysegs\n",
    "\n",
    "def segment_dataset(X_trials: np.ndarray, y_trials: np.ndarray, segment_len: int = 128):\n",
    "    X_all, y_all = [], []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        Xsegs, ysegs = segment_trial(X_trials[i], int(y_trials[i]), segment_len=segment_len)\n",
    "        X_all.append(Xsegs)\n",
    "        y_all.append(ysegs)\n",
    "\n",
    "        if (i + 1) % 5 == 0 or (i + 1) == X_trials.shape[0]:\n",
    "            print(f\"[STEP 5] Segmented {i+1}/{X_trials.shape[0]} trials...\")\n",
    "\n",
    "    X_out = np.concatenate(X_all, axis=0)\n",
    "    y_out = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    X_out = np.expand_dims(X_out, axis=-1)  # (N,14,128,1)\n",
    "    return X_out.astype(np.float32), y_out.astype(np.int32)\n",
    "\n",
    "X_train, y_train = segment_dataset(X_train_trials_all, y_train_trials_all, segment_len=128)\n",
    "X_test,  y_test  = segment_dataset(X_test_trials_all,  y_test_trials_all,  segment_len=128)\n",
    "\n",
    "print(\"[STEP 5] Segmentation done ✅\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  y_train:\", y_train.shape, np.unique(y_train, return_counts=True))\n",
    "print(\"  X_test :\", X_test.shape)\n",
    "print(\"  y_test :\", y_test.shape, np.unique(y_test, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b4ffd5-bf2f-490e-8854-0a2fc747f1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e69ba7-3952-47b1-bd80-f05237b2e018",
   "metadata": {},
   "source": [
    "Cell 6 — Split train/val + Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97fda4ae-e1c8-4fd1-bf0c-c82ae2a85764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 6] Normalizing using ALL training data (no train/val split)...\n",
      "[STEP 6] Normalization done ✅\n",
      "  X_train_norm: (5100, 14, 128, 1)\n",
      "  X_test_norm : (300, 14, 128, 1)\n",
      "  y_train_norm: (5100,)\n",
      "  y_test_norm : (300,)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 6: Normalize (ALL training data)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 6] Normalizing using ALL training data (no train/val split)...\")\n",
    "\n",
    "def normalize_eeg(train_data: np.ndarray, test_data: np.ndarray):\n",
    "    if train_data.ndim == 5:\n",
    "        train_flat = train_data.reshape(-1, *train_data.shape[2:])\n",
    "        test_flat  = test_data.reshape(-1, *test_data.shape[2:])\n",
    "    else:\n",
    "        train_flat, test_flat = train_data, test_data\n",
    "\n",
    "    # Compute stats on ALL training data\n",
    "    train_mean = np.mean(train_flat, axis=(0, 2, 3), keepdims=True)\n",
    "    train_std  = np.std(train_flat,  axis=(0, 2, 3), keepdims=True) + 1e-8\n",
    "\n",
    "    train_norm = (train_flat - train_mean) / train_std\n",
    "    test_norm  = (test_flat  - train_mean) / train_std\n",
    "\n",
    "    if train_data.ndim == 5:\n",
    "        train_norm = train_norm.reshape(train_data.shape)\n",
    "        test_norm  = test_norm.reshape(test_data.shape)\n",
    "\n",
    "    return (\n",
    "        train_norm.astype(np.float32),\n",
    "        test_norm.astype(np.float32),\n",
    "        train_mean,\n",
    "        train_std\n",
    "    )\n",
    "\n",
    "# Normalize ALL training data\n",
    "X_train_norm, X_test_norm, mean, std = normalize_eeg(X_train, X_test)\n",
    "\n",
    "y_train_norm = y_train.astype(np.float32)\n",
    "y_test_norm  = y_test.astype(np.float32)\n",
    "\n",
    "print(\"[STEP 6] Normalization done ✅\")\n",
    "print(\"  X_train_norm:\", X_train_norm.shape)\n",
    "print(\"  X_test_norm :\", X_test_norm.shape)\n",
    "print(\"  y_train_norm:\", y_train_norm.shape)\n",
    "print(\"  y_test_norm :\", y_test_norm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f527c-34db-4c23-bdbd-c8ce03b519f0",
   "metadata": {},
   "source": [
    "Cell 7 — EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ea4279-53c2-49c1-b834-7b1e1c20775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\DL\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Xtr: (5100, 14, 128, 1), ytr: (5100,)\n",
      "[INFO] Test / LOSO subject is NOT used here.\n",
      "[SPLIT] Train: (4080, 14, 128, 1), Val: (1020, 14, 128, 1)\n",
      "Epoch 1/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 123ms/step - accuracy: 0.5380 - loss: 0.7238 - val_accuracy: 0.4706 - val_loss: 0.6954 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.5586 - loss: 0.6834 - val_accuracy: 0.4765 - val_loss: 0.6942 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.5885 - loss: 0.6617 - val_accuracy: 0.5078 - val_loss: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.6218 - loss: 0.6339 - val_accuracy: 0.6265 - val_loss: 0.6853 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.6517 - loss: 0.6099 - val_accuracy: 0.6343 - val_loss: 0.6789 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.6711 - loss: 0.5906 - val_accuracy: 0.6412 - val_loss: 0.6687 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.7159 - loss: 0.5558 - val_accuracy: 0.6569 - val_loss: 0.6537 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.7377 - loss: 0.5240 - val_accuracy: 0.6402 - val_loss: 0.6437 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.7650 - loss: 0.4872 - val_accuracy: 0.6078 - val_loss: 0.6375 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.7963 - loss: 0.4515 - val_accuracy: 0.5990 - val_loss: 0.6380 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.8267 - loss: 0.4086 - val_accuracy: 0.5794 - val_loss: 0.6792 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8346 - loss: 0.3809 - val_accuracy: 0.5951 - val_loss: 0.7038 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8203 - loss: 0.4092 - val_accuracy: 0.6118 - val_loss: 0.6844 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8184 - loss: 0.4274 - val_accuracy: 0.6304 - val_loss: 0.6482 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8299 - loss: 0.3937 - val_accuracy: 0.6431 - val_loss: 0.7025 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8566 - loss: 0.3414 - val_accuracy: 0.6275 - val_loss: 0.7989 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8561 - loss: 0.3389 - val_accuracy: 0.6353 - val_loss: 0.7968 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.8672 - loss: 0.3155 - val_accuracy: 0.6304 - val_loss: 0.8448 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8650 - loss: 0.3306 - val_accuracy: 0.6853 - val_loss: 0.6846 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8561 - loss: 0.3468 - val_accuracy: 0.7216 - val_loss: 0.5793 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8527 - loss: 0.3483 - val_accuracy: 0.7275 - val_loss: 0.5660 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8654 - loss: 0.3347\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8706 - loss: 0.3278 - val_accuracy: 0.7265 - val_loss: 0.5563 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8789 - loss: 0.3006 - val_accuracy: 0.7294 - val_loss: 0.5657 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8877 - loss: 0.2798 - val_accuracy: 0.7412 - val_loss: 0.5573 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8926 - loss: 0.2600 - val_accuracy: 0.7520 - val_loss: 0.5059 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8934 - loss: 0.2705 - val_accuracy: 0.7725 - val_loss: 0.4328 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8995 - loss: 0.2572 - val_accuracy: 0.7843 - val_loss: 0.4095 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9002 - loss: 0.2508 - val_accuracy: 0.8265 - val_loss: 0.3280 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8993 - loss: 0.2586 - val_accuracy: 0.8657 - val_loss: 0.2751 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9012 - loss: 0.2436 - val_accuracy: 0.8735 - val_loss: 0.2661 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9049 - loss: 0.2443 - val_accuracy: 0.8814 - val_loss: 0.2471 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8980 - loss: 0.2512 - val_accuracy: 0.9284 - val_loss: 0.1857 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.8993 - loss: 0.2447 - val_accuracy: 0.9275 - val_loss: 0.1797 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9064 - loss: 0.2354 - val_accuracy: 0.9363 - val_loss: 0.1725 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9110 - loss: 0.2244 - val_accuracy: 0.9529 - val_loss: 0.1504 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9159 - loss: 0.2126 - val_accuracy: 0.9627 - val_loss: 0.1342 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9157 - loss: 0.2161 - val_accuracy: 0.9549 - val_loss: 0.1285 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9174 - loss: 0.2230 - val_accuracy: 0.9588 - val_loss: 0.1308 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9167 - loss: 0.2136 - val_accuracy: 0.9549 - val_loss: 0.1397 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9196 - loss: 0.2042 - val_accuracy: 0.9529 - val_loss: 0.1469 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9049 - loss: 0.2498 - val_accuracy: 0.9676 - val_loss: 0.1129 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9123 - loss: 0.2265 - val_accuracy: 0.9686 - val_loss: 0.1079 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9184 - loss: 0.2019 - val_accuracy: 0.9696 - val_loss: 0.1069 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9262 - loss: 0.1929 - val_accuracy: 0.9686 - val_loss: 0.1130 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9201 - loss: 0.2017 - val_accuracy: 0.9637 - val_loss: 0.1058 - learning_rate: 5.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9169 - loss: 0.1996 - val_accuracy: 0.9696 - val_loss: 0.1016 - learning_rate: 5.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9265 - loss: 0.1921 - val_accuracy: 0.9755 - val_loss: 0.0945 - learning_rate: 5.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9292 - loss: 0.1810 - val_accuracy: 0.9755 - val_loss: 0.0949 - learning_rate: 5.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9324 - loss: 0.1866 - val_accuracy: 0.9706 - val_loss: 0.0915 - learning_rate: 5.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9306 - loss: 0.1769 - val_accuracy: 0.9696 - val_loss: 0.1000 - learning_rate: 5.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9270 - loss: 0.1857 - val_accuracy: 0.9706 - val_loss: 0.0899 - learning_rate: 5.0000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9284 - loss: 0.1882 - val_accuracy: 0.9676 - val_loss: 0.1056 - learning_rate: 5.0000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9297 - loss: 0.1774 - val_accuracy: 0.9716 - val_loss: 0.0956 - learning_rate: 5.0000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9251 - loss: 0.1863\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9309 - loss: 0.1807 - val_accuracy: 0.9745 - val_loss: 0.0905 - learning_rate: 5.0000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9346 - loss: 0.1711 - val_accuracy: 0.9725 - val_loss: 0.0847 - learning_rate: 2.5000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9358 - loss: 0.1706 - val_accuracy: 0.9735 - val_loss: 0.0824 - learning_rate: 2.5000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9353 - loss: 0.1641 - val_accuracy: 0.9716 - val_loss: 0.0991 - learning_rate: 2.5000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9402 - loss: 0.1610 - val_accuracy: 0.9735 - val_loss: 0.0824 - learning_rate: 2.5000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9328 - loss: 0.1684 - val_accuracy: 0.9745 - val_loss: 0.0813 - learning_rate: 2.5000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9336 - loss: 0.1647 - val_accuracy: 0.9735 - val_loss: 0.0886 - learning_rate: 2.5000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9355 - loss: 0.1706 - val_accuracy: 0.9676 - val_loss: 0.0976 - learning_rate: 2.5000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9321 - loss: 0.1733\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9355 - loss: 0.1660 - val_accuracy: 0.9725 - val_loss: 0.0854 - learning_rate: 2.5000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9404 - loss: 0.1582 - val_accuracy: 0.9765 - val_loss: 0.0815 - learning_rate: 1.2500e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9414 - loss: 0.1625 - val_accuracy: 0.9755 - val_loss: 0.0803 - learning_rate: 1.2500e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9360 - loss: 0.1600 - val_accuracy: 0.9765 - val_loss: 0.0794 - learning_rate: 1.2500e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9400 - loss: 0.1615 - val_accuracy: 0.9755 - val_loss: 0.0800 - learning_rate: 1.2500e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9417 - loss: 0.1525 - val_accuracy: 0.9755 - val_loss: 0.0776 - learning_rate: 1.2500e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9390 - loss: 0.1496 - val_accuracy: 0.9735 - val_loss: 0.0791 - learning_rate: 1.2500e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9370 - loss: 0.1559 - val_accuracy: 0.9745 - val_loss: 0.0827 - learning_rate: 1.2500e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9377 - loss: 0.1595 - val_accuracy: 0.9755 - val_loss: 0.0809 - learning_rate: 1.2500e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9431 - loss: 0.1588 - val_accuracy: 0.9745 - val_loss: 0.0805 - learning_rate: 1.2500e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9373 - loss: 0.1611\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9404 - loss: 0.1527 - val_accuracy: 0.9745 - val_loss: 0.0797 - learning_rate: 1.2500e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9424 - loss: 0.1478 - val_accuracy: 0.9755 - val_loss: 0.0786 - learning_rate: 6.2500e-05\n",
      "Epoch 74/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9343 - loss: 0.1570 - val_accuracy: 0.9745 - val_loss: 0.0802 - learning_rate: 6.2500e-05\n",
      "Epoch 75/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9422 - loss: 0.1461 - val_accuracy: 0.9745 - val_loss: 0.0789 - learning_rate: 6.2500e-05\n",
      "Epoch 76/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.9350 - loss: 0.1538 - val_accuracy: 0.9765 - val_loss: 0.0774 - learning_rate: 6.2500e-05\n",
      "Epoch 77/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.9434 - loss: 0.1515 - val_accuracy: 0.9765 - val_loss: 0.0769 - learning_rate: 6.2500e-05\n",
      "Epoch 78/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9382 - loss: 0.1509 - val_accuracy: 0.9755 - val_loss: 0.0793 - learning_rate: 6.2500e-05\n",
      "Epoch 79/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9340 - loss: 0.1627\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9348 - loss: 0.1598 - val_accuracy: 0.9765 - val_loss: 0.0802 - learning_rate: 6.2500e-05\n",
      "Epoch 80/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9424 - loss: 0.1488 - val_accuracy: 0.9745 - val_loss: 0.0791 - learning_rate: 3.1250e-05\n",
      "Epoch 81/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9449 - loss: 0.1485 - val_accuracy: 0.9765 - val_loss: 0.0784 - learning_rate: 3.1250e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9377 - loss: 0.1516 - val_accuracy: 0.9775 - val_loss: 0.0775 - learning_rate: 3.1250e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9345 - loss: 0.1616\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9414 - loss: 0.1500 - val_accuracy: 0.9765 - val_loss: 0.0759 - learning_rate: 3.1250e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9424 - loss: 0.1483 - val_accuracy: 0.9765 - val_loss: 0.0759 - learning_rate: 1.5625e-05\n",
      "Epoch 85/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9417 - loss: 0.1460 - val_accuracy: 0.9775 - val_loss: 0.0760 - learning_rate: 1.5625e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9426 - loss: 0.1442 - val_accuracy: 0.9765 - val_loss: 0.0758 - learning_rate: 1.5625e-05\n",
      "Epoch 87/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9449 - loss: 0.1442 - val_accuracy: 0.9745 - val_loss: 0.0761 - learning_rate: 1.5625e-05\n",
      "Epoch 88/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9331 - loss: 0.1680 - val_accuracy: 0.9765 - val_loss: 0.0759 - learning_rate: 1.5625e-05\n",
      "Epoch 89/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9453 - loss: 0.1474 - val_accuracy: 0.9765 - val_loss: 0.0764 - learning_rate: 1.5625e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9448 - loss: 0.1459\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9456 - loss: 0.1443 - val_accuracy: 0.9775 - val_loss: 0.0761 - learning_rate: 1.5625e-05\n",
      "Epoch 91/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9419 - loss: 0.1410 - val_accuracy: 0.9775 - val_loss: 0.0760 - learning_rate: 7.8125e-06\n",
      "Epoch 92/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9436 - loss: 0.1465 - val_accuracy: 0.9755 - val_loss: 0.0759 - learning_rate: 7.8125e-06\n",
      "Epoch 93/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9426 - loss: 0.1467 - val_accuracy: 0.9765 - val_loss: 0.0761 - learning_rate: 7.8125e-06\n",
      "Epoch 94/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9426 - loss: 0.1448 - val_accuracy: 0.9775 - val_loss: 0.0760 - learning_rate: 7.8125e-06\n",
      "Epoch 95/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9333 - loss: 0.1633\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9355 - loss: 0.1552 - val_accuracy: 0.9765 - val_loss: 0.0760 - learning_rate: 7.8125e-06\n",
      "Epoch 96/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9434 - loss: 0.1415 - val_accuracy: 0.9765 - val_loss: 0.0760 - learning_rate: 3.9063e-06\n",
      "Epoch 97/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9419 - loss: 0.1470 - val_accuracy: 0.9765 - val_loss: 0.0759 - learning_rate: 3.9063e-06\n",
      "Epoch 98/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9471 - loss: 0.1451 - val_accuracy: 0.9765 - val_loss: 0.0760 - learning_rate: 3.9063e-06\n",
      "Epoch 99/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9401 - loss: 0.1499\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9404 - loss: 0.1514 - val_accuracy: 0.9765 - val_loss: 0.0759 - learning_rate: 3.9063e-06\n",
      "Epoch 100/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9400 - loss: 0.1587 - val_accuracy: 0.9765 - val_loss: 0.0759 - learning_rate: 1.9531e-06\n",
      "Epoch 101/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9409 - loss: 0.1528 - val_accuracy: 0.9765 - val_loss: 0.0759 - learning_rate: 1.9531e-06\n",
      "Epoch 101: early stopping\n",
      "Restoring model weights from the end of the best epoch: 86.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] Model saved as cole-final.h5\n",
      "\n",
      "[CONFUSION MATRIX]\n",
      "[[472   8]\n",
      " [ 16 524]]\n",
      "[VAL] acc=0.9765, prec=0.9850, rec=0.9704, f1=0.9776\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 4: 80:20 Train / Validation Split (EEGNet)\n",
    "# ====================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, SeparableConv2D, BatchNormalization, Activation,\n",
    "    DepthwiseConv2D, Conv2D, AveragePooling2D,\n",
    "    Dropout, Dense, Flatten\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# --------------------------------------------------------------------\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# EEGNet model\n",
    "# --------------------------------------------------------------------\n",
    "def create_eegnet_model(input_shape, dropout_rate=0.5, num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(16, (1, 64), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = DepthwiseConv2D(\n",
    "        (input_shape[0], 1),\n",
    "        depth_multiplier=2,\n",
    "        padding='valid',\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 4))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = SeparableConv2D(16, (1, 16), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 8))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Compile model\n",
    "# --------------------------------------------------------------------\n",
    "def compile_model():\n",
    "    input_shape = (Xtr.shape[1], Xtr.shape[2], Xtr.shape[3])\n",
    "    model = create_eegnet_model(input_shape)\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-3),\n",
    "        loss=BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[BinaryAccuracy(name=\"accuracy\", threshold=0.5)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def make_callbacks():\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"loss\",\n",
    "            factor=0.5,\n",
    "            patience=4,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Use ALL normalized training data\n",
    "# --------------------------------------------------------------------\n",
    "Xtr = X_train_norm\n",
    "ytr = y_train_norm\n",
    "\n",
    "print(f\"[TRAIN] Xtr: {Xtr.shape}, ytr: {ytr.shape}\")\n",
    "print(\"[INFO] Test / LOSO subject is NOT used here.\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 80:20 Stratified Split\n",
    "# --------------------------------------------------------------------\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(sss.split(Xtr, ytr))\n",
    "\n",
    "X_train, X_val = Xtr[train_idx], Xtr[val_idx]\n",
    "y_train, y_val = ytr[train_idx], ytr[val_idx]\n",
    "\n",
    "print(f\"[SPLIT] Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Train model\n",
    "# --------------------------------------------------------------------\n",
    "model = compile_model()\n",
    "callbacks = make_callbacks()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Save final model\n",
    "# --------------------------------------------------------------------\n",
    "model.save(\"mona-final.h5\")\n",
    "print(\"[SAVE] Model saved as cole-final.h5\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Validation Confusion Matrix\n",
    "# --------------------------------------------------------------------\n",
    "y_prob = model.predict(X_val, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "y_true = y_val.astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "eps = 1e-12\n",
    "\n",
    "acc  = (tp + tn) / max(tp + tn + fp + fn, eps)\n",
    "prec = tp / max(tp + fp, eps)\n",
    "rec  = tp / max(tp + fn, eps)\n",
    "f1   = 2 * prec * rec / max(prec + rec, eps)\n",
    "\n",
    "print(\"\\n[CONFUSION MATRIX]\")\n",
    "print(cm)\n",
    "print(f\"[VAL] acc={acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e2a13-6367-41ca-be66-a43594c2940e",
   "metadata": {},
   "source": [
    "Cell 8 — Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ac6662-33c1-4e25-b6c5-d47c860fba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Preparing TEST data...\n",
      "[TEST] Xte shape: (300, 14, 128, 1)\n",
      "[TEST] yte shape: (300,), class distribution: (array([0]), array([300]))\n",
      "[TEST] Loading final model: mona-final.h5\n",
      "[TEST] Running inference on TEST subject...\n",
      "[TEST] Mean predicted probability: 0.8835\n",
      "\n",
      "[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[ 23 277]\n",
      " [  0   0]]\n",
      "Accuracy : 0.0767\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-score : 0.0000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Inference on TEST subject (adam) using FINAL EEGNet\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Prepare TEST data\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Preparing TEST data...\")\n",
    "\n",
    "Xte = X_test_norm\n",
    "yte = y_test_norm.astype(int)\n",
    "\n",
    "print(f\"[TEST] Xte shape: {Xte.shape}\")\n",
    "print(f\"[TEST] yte shape: {yte.shape}, class distribution: {np.unique(yte, return_counts=True)}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Load FINAL trained model\n",
    "# ------------------------------------------------------------------\n",
    "model_path = \"mona-final.h5\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        \"Final model 'cole-final.h5' not found. \"\n",
    "        \"Please run Cell 4 (80:20 training) first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[TEST] Loading final model: {model_path}\")\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Run inference on TEST subject\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Running inference on TEST subject...\")\n",
    "\n",
    "y_prob = model.predict(Xte, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(f\"[TEST] Mean predicted probability: {y_prob.mean():.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Metrics on TEST subject (segment-level)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc = accuracy_score(yte, y_pred)\n",
    "prec = precision_score(yte, y_pred, zero_division=0)\n",
    "rec = recall_score(yte, y_pred, zero_division=0)\n",
    "f1 = f1_score(yte, y_pred, zero_division=0)\n",
    "\n",
    "print(\"\\n[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572dbf3e-bbb5-4865-883f-30e374700949",
   "metadata": {},
   "source": [
    "Cell 9 - Performance Matrices for each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a57240-50ea-4736-a7ac-52ef7878b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] Verifying predictions and labels are available...\n",
      "[CHECK] Found predictions for 300 test segments.\n",
      "\n",
      "[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[ 23 277]\n",
      " [  0   0]]\n",
      "Accuracy : 0.0767\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-score : 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Final model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================\n",
    "# Cell 6: Confusion Matrix & Metrics on TEST (FINAL EEGNet)\n",
    "# =======================================================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# ------------------------------------------------------------------\n",
    "def compute_metrics_from_cm(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    eps = 1e-12\n",
    "    acc  = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec  = tp / (tp + fn + eps)\n",
    "    f1   = 2 * prec * rec / (prec + rec + eps)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], ['0', '1'])\n",
    "    plt.yticks([0, 1], ['0', '1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(\n",
    "                j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Validate upstream artifacts\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[CHECK] Verifying predictions and labels are available...\")\n",
    "\n",
    "if 'y_pred' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Predictions 'y_pred' not found. \"\n",
    "        \"Please run Cell 5 (final model inference) first.\"\n",
    "    )\n",
    "\n",
    "if 'yte' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Test labels 'yte' not found. \"\n",
    "        \"Please run Cell 5 first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[CHECK] Found predictions for {len(y_pred)} test segments.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Confusion Matrix & Metrics (FINAL model)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc, prec, rec, f1 = compute_metrics_from_cm(cm)\n",
    "\n",
    "print(\"\\n[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(cm, title=\"Confusion Matrix - FINAL EEGNet\")\n",
    "\n",
    "print(\"[DONE] Final model evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0f120-39fd-497e-9098-05e6c82f4807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
