{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af5b471-cea9-4194-bc23-da5cdc2f9c67",
   "metadata": {},
   "source": [
    "Cell 1 — Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64aaa897-ab61-4eb4-a558-4c078acf751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Base path set to:\n",
      "C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\n",
      "[INFO] Found 18 folders:\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Selected test subject (LOSO): Amin\n",
      "[INFO] Training subjects (17):\n",
      "['Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Sampling rate (SFREQ) = 128.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 1: Setup & discovery\n",
    "# =========================\n",
    "\n",
    "import os  # file ops\n",
    "import glob  # file matching\n",
    "import numpy as np  # arrays\n",
    "import random  # seeds\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data path\n",
    "BASE_PATH = r\"C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\"\n",
    "print(f\"[INFO] Base path set to:\\n{BASE_PATH}\")\n",
    "\n",
    "# List subject folders\n",
    "all_folders = [p for p in sorted(os.listdir(BASE_PATH)) if os.path.isdir(os.path.join(BASE_PATH, p))]\n",
    "print(f\"[INFO] Found {len(all_folders)} folders:\\n{all_folders}\")\n",
    "\n",
    "# Expected count (change if needed)\n",
    "assert len(all_folders) == 18, f\"Expected 18 folders, found {len(all_folders)}.\"\n",
    "\n",
    "# LOSO test subject\n",
    "TEST_SUBJECT = \"Amin\"\n",
    "print(f\"[INFO] Selected test subject (LOSO): {TEST_SUBJECT}\")\n",
    "\n",
    "# Train subjects = remaining\n",
    "train_subjects = [s for s in all_folders if s.lower() != TEST_SUBJECT.lower()]\n",
    "print(f\"[INFO] Training subjects ({len(train_subjects)}):\\n{train_subjects}\")\n",
    "\n",
    "# Sampling rate\n",
    "SFREQ = 128.0\n",
    "print(f\"[INFO] Sampling rate (SFREQ) = {SFREQ} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e649681-2861-4ce2-bbd1-0d9dd3359f42",
   "metadata": {},
   "source": [
    "Cell 2 — Load EEG + labels per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "943c5c6e-a7aa-4618-899c-daf341c25358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 2] Loading data from folders...\n",
      "[STEP 2]       Amin | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Amin | EEG=          Amin.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Amin1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]      Amin1 | EEG=         Amin1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Cole | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Cole | EEG=          Cole.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Daniel | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]     Daniel | EEG=        Daniel.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]    Ismayil | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]    Ismayil | EEG=       Ismayil.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Jack | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Jack | EEG=          Jack.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]      James | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      James | EEG=         James.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Josh | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Josh | EEG=          Josh.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Marjan | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Marjan | EEG=        Marjan.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]        Max | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Max | EEG=           Max.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Mina | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mina | EEG=          Mina.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 1 | EEG=        Mina 1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 3 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 3 | EEG=        Mina 3.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]   Mohammad | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]   Mohammad | EEG=      Mohammad.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Mona | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mona | EEG=          Mona.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Roddy | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      Roddy | EEG=         Roddy.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]        Sam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Sam | EEG=           Sam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       adam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       adam | EEG=          adam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2] Loading complete ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 2: Load data (EMPTY LABEL => CLASS 0)\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path  # helper\n",
    "\n",
    "def load_subject(subj_dir: str, subj_name: str, expected_channels: int = 14):\n",
    "    \"\"\"\n",
    "    EEG file: {subj_name}.npy\n",
    "    Label file: {subj_name}_target.npy\n",
    "    Label rule:\n",
    "      - empty array -> class 0\n",
    "      - [0] -> class 0\n",
    "      - [1] -> class 1\n",
    "    \"\"\"\n",
    "\n",
    "    # EEG file path\n",
    "    eeg_file = os.path.join(subj_dir, f\"{subj_name}.npy\")\n",
    "    assert os.path.exists(eeg_file), f\"[ERROR] Missing EEG file: {eeg_file}\"\n",
    "\n",
    "    # Label file path\n",
    "    target_file = os.path.join(subj_dir, f\"{subj_name}_target.npy\")\n",
    "    assert os.path.exists(target_file), f\"[ERROR] Missing label file: {target_file}\"\n",
    "\n",
    "    # Load EEG\n",
    "    X_raw = np.load(eeg_file, allow_pickle=True)\n",
    "    X_raw = np.asarray(X_raw)\n",
    "\n",
    "    # Standardize EEG to (n_trials, 14, T)\n",
    "    if X_raw.ndim == 2:\n",
    "        X_trials = X_raw[None, :, :]\n",
    "    elif X_raw.ndim == 3:\n",
    "        dims = list(X_raw.shape)\n",
    "        chan_axes = [i for i, d in enumerate(dims) if d == expected_channels]\n",
    "        chan_axis = chan_axes[0] if len(chan_axes) == 1 else 1\n",
    "        time_axis = int(np.argmax(dims))\n",
    "        trial_axis = [i for i in range(3) if i not in (chan_axis, time_axis)][0]\n",
    "        X_trials = np.moveaxis(X_raw, (trial_axis, chan_axis, time_axis), (0, 1, 2))\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] Unsupported EEG shape {X_raw.shape} in {eeg_file}\")\n",
    "\n",
    "    assert X_trials.shape[1] == expected_channels, f\"[ERROR] Expected {expected_channels} channels, got {X_trials.shape[1]} in {eeg_file}\"\n",
    "\n",
    "    # Load label\n",
    "    y_raw = np.load(target_file, allow_pickle=True)\n",
    "    y_arr = np.asarray(y_raw).ravel()\n",
    "\n",
    "    # Apply your rule\n",
    "    if y_arr.size == 0:\n",
    "        label = 0\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\")\n",
    "    else:\n",
    "        label = int(y_arr[0])\n",
    "        if label not in (0, 1):\n",
    "            raise ValueError(f\"[ERROR] Invalid label {label} in {target_file} (must be 0 or 1)\")\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape={y_arr.shape}, value={y_arr} -> class={label}\")\n",
    "\n",
    "    # Repeat label for all trials\n",
    "    y_trials = np.full((X_trials.shape[0],), label, dtype=np.int32)\n",
    "\n",
    "    return X_trials.astype(np.float32), y_trials.astype(np.int32), eeg_file, target_file\n",
    "\n",
    "\n",
    "# Load all subjects\n",
    "subject_data = {}\n",
    "print(\"[STEP 2] Loading data from folders...\")\n",
    "\n",
    "for subj in all_folders:\n",
    "    subj_dir = os.path.join(BASE_PATH, subj)\n",
    "    X_trials, y_trials, eeg_file, target_file = load_subject(subj_dir, subj)\n",
    "\n",
    "    subject_data[subj] = (X_trials, y_trials)\n",
    "\n",
    "    print(f\"[STEP 2] {subj:>10} | EEG={Path(eeg_file).name:>18} | X={X_trials.shape} | y={y_trials.shape} | class={y_trials[0]}\")\n",
    "\n",
    "print(\"[STEP 2] Loading complete ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd93ca8-a94c-485c-8a61-e6e7441f6299",
   "metadata": {},
   "source": [
    "Cell 3 — Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "733e592c-98e1-4cbd-afd2-6fdc8cf407f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 3] Preprocessing functions ready ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3: Preprocessing functions\n",
    "# =========================\n",
    "\n",
    "import mne  # EEG processing\n",
    "import pywt  # wavelets\n",
    "from sklearn.decomposition import FastICA  # ICA\n",
    "from typing import Optional, Tuple, Union, Sequence, Dict, List  # typing\n",
    "\n",
    "def wavelet_enhanced_ica(\n",
    "    data: np.ndarray,\n",
    "    n_components: int = 10,\n",
    "    wavelet: str = \"db4\",\n",
    "    level: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> np.ndarray:\n",
    "    n_ch, n_t = data.shape\n",
    "    n_components = min(n_components, n_ch)\n",
    "\n",
    "    coeffs = pywt.wavedec(data, wavelet=wavelet, level=level, axis=1)\n",
    "    A = coeffs[0]\n",
    "\n",
    "    ica = FastICA(n_components=n_components, random_state=random_state)\n",
    "    S = ica.fit_transform(A.T).T\n",
    "    A_denoised = ica.inverse_transform(S.T).T\n",
    "\n",
    "    coeffs[0] = A_denoised\n",
    "    cleaned = pywt.waverec(coeffs, wavelet=wavelet, axis=1)\n",
    "\n",
    "    if cleaned.shape[1] != n_t:\n",
    "        if cleaned.shape[1] > n_t:\n",
    "            cleaned = cleaned[:, :n_t]\n",
    "        else:\n",
    "            cleaned = np.pad(cleaned, ((0, 0), (0, n_t - cleaned.shape[1])), mode=\"constant\")\n",
    "    return cleaned\n",
    "\n",
    "def _names_from_index_mapping(n_channels: int, index_to_name: Optional[Dict[int, str]]) -> List[str]:\n",
    "    if index_to_name is None:\n",
    "        return [f\"EEG{i+1}\" for i in range(n_channels)]\n",
    "    keys = list(index_to_name.keys())\n",
    "    is_zero_based = (0 in keys) and (1 not in keys)\n",
    "    names = []\n",
    "    for i in range(n_channels):\n",
    "        key = i if is_zero_based else (i + 1)\n",
    "        names.append(index_to_name.get(key, f\"EEG{i+1}\"))\n",
    "    return names\n",
    "\n",
    "def preprocess_eeg(\n",
    "    eeg: np.ndarray,\n",
    "    sfreq: float,\n",
    "    *,\n",
    "    index_to_name: Optional[Dict[int, str]] = None,\n",
    "    use_standard_1010: bool = True,\n",
    "    resample_to: Optional[float] = None,\n",
    "    notch_freqs: Union[None, float, Sequence[float]] = 50.0,\n",
    "    highpass: Optional[float] = 0.05,\n",
    "    bad_point_z: float = 6.0,\n",
    "    bad_channel_z: float = 5.0,\n",
    "    interpolate_bad_channels: bool = True,\n",
    "    car: bool = True,\n",
    "    use_wica: bool = True,\n",
    "    wica_components: int = 10,\n",
    "    wica_wavelet: str = \"db4\",\n",
    "    wica_level: int = 3,\n",
    "    wica_random_state: int = 42,\n",
    "    return_raw: bool = False,\n",
    "):\n",
    "\n",
    "    eeg = np.asarray(eeg, dtype=np.float32)\n",
    "    assert eeg.ndim == 2, \"eeg must be 2D: (n_channels, n_times)\"\n",
    "\n",
    "    n_channels, _ = eeg.shape\n",
    "    ch_names = _names_from_index_mapping(n_channels, index_to_name)\n",
    "    ch_types = ['eog' if str(n).upper().startswith(\"EOG\") else 'eeg' for n in ch_names]\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    raw = mne.io.RawArray(eeg, info, verbose=False)\n",
    "\n",
    "    montage_applied = False\n",
    "    if use_standard_1010:\n",
    "        try:\n",
    "            mont = mne.channels.make_standard_montage(\"standard_1010\")\n",
    "            raw.set_montage(mont, match_case=False, on_missing=\"ignore\")\n",
    "            montage_applied = True\n",
    "        except Exception:\n",
    "            montage_applied = False\n",
    "\n",
    "    if resample_to is not None and float(resample_to) != float(sfreq):\n",
    "        raw.resample(sfreq=resample_to, npad=\"auto\")\n",
    "    sfreq_out = float(raw.info[\"sfreq\"])\n",
    "\n",
    "    if notch_freqs is not None:\n",
    "        raw.notch_filter(freqs=notch_freqs, verbose=False)\n",
    "    if highpass is not None:\n",
    "        raw.filter(l_freq=highpass, h_freq=None, verbose=False)\n",
    "\n",
    "    X = raw.get_data()\n",
    "    mu = np.mean(X, axis=1, keepdims=True)\n",
    "    sd = np.std(X, axis=1, keepdims=True) + 1e-12\n",
    "    hi = mu + bad_point_z * sd\n",
    "    lo = mu - bad_point_z * sd\n",
    "    bad_idx = (X > hi) | (X < lo)\n",
    "\n",
    "    if np.any(bad_idx):\n",
    "        X_fixed = X.copy()\n",
    "        t = np.arange(X.shape[1], dtype=float)\n",
    "        for ch in range(n_channels):\n",
    "            mask = bad_idx[ch]\n",
    "            if mask.any():\n",
    "                good = ~mask\n",
    "                if good.sum() >= 2:\n",
    "                    X_fixed[ch, mask] = np.interp(t[mask], t[good], X_fixed[ch, good])\n",
    "        raw._data = X_fixed\n",
    "\n",
    "    if interpolate_bad_channels and montage_applied:\n",
    "        X = raw.get_data(picks=\"eeg\")\n",
    "        if X.size > 0:\n",
    "            ch_std = X.std(axis=1)\n",
    "            med = np.median(ch_std)\n",
    "            mad = np.median(np.abs(ch_std - med)) + 1e-12\n",
    "            z = 0.6745 * (ch_std - med) / mad\n",
    "            eeg_names = mne.pick_info(raw.info, mne.pick_types(raw.info, eeg=True)).ch_names\n",
    "            bads = [eeg_names[i] for i in np.where(np.abs(z) > bad_channel_z)[0]]\n",
    "            raw.info[\"bads\"] = bads\n",
    "            if bads:\n",
    "                raw.interpolate_bads(reset_bads=True, verbose=False)\n",
    "\n",
    "    if car:\n",
    "        raw.set_eeg_reference(\"average\", projection=True)\n",
    "        raw.apply_proj()\n",
    "\n",
    "    if use_wica:\n",
    "        cleaned = wavelet_enhanced_ica(\n",
    "            raw.get_data(),\n",
    "            n_components=wica_components,\n",
    "            wavelet=wica_wavelet,\n",
    "            level=wica_level,\n",
    "            random_state=wica_random_state\n",
    "        )\n",
    "        raw = mne.io.RawArray(cleaned, raw.info, verbose=False)\n",
    "\n",
    "    cleaned = raw.get_data()\n",
    "    return cleaned, sfreq_out\n",
    "\n",
    "print(\"[STEP 3] Preprocessing functions ready ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a9d20-889d-45c4-bbc2-de62216908cd",
   "metadata": {},
   "source": [
    "Cell 4 — Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bfcbde0-1cc2-409c-8627-93da46606661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 4] Starting preprocessing...\n",
      "[INFO] USE_WICA = True\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 01/17: Amin1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 02/17: Cole | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 03/17: Daniel | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 04/17: Ismayil | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 05/17: Jack | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 06/17: James | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 07/17: Josh | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 08/17: Marjan | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 09/17: Max | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 10/17: Mina | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 11/17: Mina 1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 12/17: Mina 3 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 13/17: Mohammad | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 14/17: Mona | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 15/17: Roddy | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 16/17: Sam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 17/17: adam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Test subject: Amin | trials=1 | fs_out=128.0\n",
      "[STEP 4] Preprocessing done ✅\n",
      "  X_train_trials_all: (17, 14, 38400)\n",
      "  y_train_trials_all: (17,) (array([0, 1], dtype=int32), array([8, 9]))\n",
      "  X_test_trials_all : (1, 14, 38400)\n",
      "  y_test_trials_all : (1,) (array([0], dtype=int32), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 4: Apply preprocessing (LOSO)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 4] Starting preprocessing...\")\n",
    "\n",
    "USE_WICA = True\n",
    "print(f\"[INFO] USE_WICA = {USE_WICA}\")\n",
    "\n",
    "X_train_trials_all, y_train_trials_all = [], []\n",
    "X_test_trials_all, y_test_trials_all = [], []\n",
    "\n",
    "def preprocess_trials(X_trials, y_trials):\n",
    "    cleaned_list = []\n",
    "    label_list = []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        trial = X_trials[i]\n",
    "        label = int(y_trials[i])\n",
    "\n",
    "        cleaned, fs_out = preprocess_eeg(\n",
    "            eeg=trial,\n",
    "            sfreq=SFREQ,\n",
    "            use_wica=USE_WICA\n",
    "        )\n",
    "\n",
    "        cleaned_list.append(cleaned.astype(np.float32, copy=False))\n",
    "        label_list.append(label)\n",
    "\n",
    "    return cleaned_list, np.array(label_list, dtype=np.int32), fs_out\n",
    "\n",
    "# Train subjects\n",
    "for idx, subj in enumerate(train_subjects, start=1):\n",
    "    X_trials, y_trials = subject_data[subj]\n",
    "    cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "    X_train_trials_all.extend(cleaned_trials)\n",
    "    y_train_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "    print(f\"[STEP 4] Train {idx:02d}/{len(train_subjects)}: {subj} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Test subject\n",
    "X_trials, y_trials = subject_data[TEST_SUBJECT]\n",
    "cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "X_test_trials_all.extend(cleaned_trials)\n",
    "y_test_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "print(f\"[STEP 4] Test subject: {TEST_SUBJECT} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Convert to arrays\n",
    "X_train_trials_all = np.array(X_train_trials_all, dtype=np.float32)\n",
    "y_train_trials_all = np.array(y_train_trials_all, dtype=np.int32)\n",
    "\n",
    "X_test_trials_all = np.array(X_test_trials_all, dtype=np.float32)\n",
    "y_test_trials_all = np.array(y_test_trials_all, dtype=np.int32)\n",
    "\n",
    "print(\"[STEP 4] Preprocessing done ✅\")\n",
    "print(\"  X_train_trials_all:\", X_train_trials_all.shape)\n",
    "print(\"  y_train_trials_all:\", y_train_trials_all.shape, np.unique(y_train_trials_all, return_counts=True))\n",
    "print(\"  X_test_trials_all :\", X_test_trials_all.shape)\n",
    "print(\"  y_test_trials_all :\", y_test_trials_all.shape, np.unique(y_test_trials_all, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc3da1-2967-4ee3-84cd-bfcf0f98c0b1",
   "metadata": {},
   "source": [
    "Cell 5 — Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c21ad54-7001-4039-9360-29dfa2905127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 5] Starting segmentation...\n",
      "[STEP 5] Segmented 5/17 trials...\n",
      "[STEP 5] Segmented 10/17 trials...\n",
      "[STEP 5] Segmented 15/17 trials...\n",
      "[STEP 5] Segmented 17/17 trials...\n",
      "[STEP 5] Segmented 1/1 trials...\n",
      "[STEP 5] Segmentation done ✅\n",
      "  X_train: (5100, 14, 128, 1)\n",
      "  y_train: (5100,) (array([0, 1], dtype=int32), array([2400, 2700]))\n",
      "  X_test : (300, 14, 128, 1)\n",
      "  y_test : (300,) (array([0], dtype=int32), array([300]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 5: Segmentation\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 5] Starting segmentation...\")\n",
    "\n",
    "def segment_trial(trial_2d: np.ndarray, label: int, segment_len: int = 128):\n",
    "    C, T = trial_2d.shape\n",
    "    n_segs = T // segment_len\n",
    "    use_T = n_segs * segment_len\n",
    "\n",
    "    if n_segs == 0:\n",
    "        return np.empty((0, C, segment_len), dtype=np.float32), np.empty((0,), dtype=np.int32)\n",
    "\n",
    "    trial_trim = trial_2d[:, :use_T]\n",
    "    Xsegs = trial_trim.reshape(C, n_segs, segment_len).transpose(1, 0, 2)\n",
    "    ysegs = np.full((n_segs,), int(label), dtype=np.int32)\n",
    "    return Xsegs.astype(np.float32, copy=False), ysegs\n",
    "\n",
    "def segment_dataset(X_trials: np.ndarray, y_trials: np.ndarray, segment_len: int = 128):\n",
    "    X_all, y_all = [], []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        Xsegs, ysegs = segment_trial(X_trials[i], int(y_trials[i]), segment_len=segment_len)\n",
    "        X_all.append(Xsegs)\n",
    "        y_all.append(ysegs)\n",
    "\n",
    "        if (i + 1) % 5 == 0 or (i + 1) == X_trials.shape[0]:\n",
    "            print(f\"[STEP 5] Segmented {i+1}/{X_trials.shape[0]} trials...\")\n",
    "\n",
    "    X_out = np.concatenate(X_all, axis=0)\n",
    "    y_out = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    X_out = np.expand_dims(X_out, axis=-1)  # (N,14,128,1)\n",
    "    return X_out.astype(np.float32), y_out.astype(np.int32)\n",
    "\n",
    "X_train, y_train = segment_dataset(X_train_trials_all, y_train_trials_all, segment_len=128)\n",
    "X_test,  y_test  = segment_dataset(X_test_trials_all,  y_test_trials_all,  segment_len=128)\n",
    "\n",
    "print(\"[STEP 5] Segmentation done ✅\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  y_train:\", y_train.shape, np.unique(y_train, return_counts=True))\n",
    "print(\"  X_test :\", X_test.shape)\n",
    "print(\"  y_test :\", y_test.shape, np.unique(y_test, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61b4ffd5-bf2f-490e-8854-0a2fc747f1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e69ba7-3952-47b1-bd80-f05237b2e018",
   "metadata": {},
   "source": [
    "Cell 6 — Split train/val + Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97fda4ae-e1c8-4fd1-bf0c-c82ae2a85764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 6] Normalizing using ALL training data (no train/val split)...\n",
      "[STEP 6] Normalization done ✅\n",
      "  X_train_norm: (5100, 14, 128, 1)\n",
      "  X_test_norm : (300, 14, 128, 1)\n",
      "  y_train_norm: (5100,)\n",
      "  y_test_norm : (300,)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 6: Normalize (ALL training data)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 6] Normalizing using ALL training data (no train/val split)...\")\n",
    "\n",
    "def normalize_eeg(train_data: np.ndarray, test_data: np.ndarray):\n",
    "    if train_data.ndim == 5:\n",
    "        train_flat = train_data.reshape(-1, *train_data.shape[2:])\n",
    "        test_flat  = test_data.reshape(-1, *test_data.shape[2:])\n",
    "    else:\n",
    "        train_flat, test_flat = train_data, test_data\n",
    "\n",
    "    # Compute stats on ALL training data\n",
    "    train_mean = np.mean(train_flat, axis=(0, 2, 3), keepdims=True)\n",
    "    train_std  = np.std(train_flat,  axis=(0, 2, 3), keepdims=True) + 1e-8\n",
    "\n",
    "    train_norm = (train_flat - train_mean) / train_std\n",
    "    test_norm  = (test_flat  - train_mean) / train_std\n",
    "\n",
    "    if train_data.ndim == 5:\n",
    "        train_norm = train_norm.reshape(train_data.shape)\n",
    "        test_norm  = test_norm.reshape(test_data.shape)\n",
    "\n",
    "    return (\n",
    "        train_norm.astype(np.float32),\n",
    "        test_norm.astype(np.float32),\n",
    "        train_mean,\n",
    "        train_std\n",
    "    )\n",
    "\n",
    "# Normalize ALL training data\n",
    "X_train_norm, X_test_norm, mean, std = normalize_eeg(X_train, X_test)\n",
    "\n",
    "y_train_norm = y_train.astype(np.float32)\n",
    "y_test_norm  = y_test.astype(np.float32)\n",
    "\n",
    "print(\"[STEP 6] Normalization done ✅\")\n",
    "print(\"  X_train_norm:\", X_train_norm.shape)\n",
    "print(\"  X_test_norm :\", X_test_norm.shape)\n",
    "print(\"  y_train_norm:\", y_train_norm.shape)\n",
    "print(\"  y_test_norm :\", y_test_norm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f527c-34db-4c23-bdbd-c8ce03b519f0",
   "metadata": {},
   "source": [
    "Cell 7 — EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9ea4279-53c2-49c1-b834-7b1e1c20775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Xtr: (5100, 14, 128, 1), ytr: (5100,)\n",
      "[INFO] Test / LOSO subject is NOT used here.\n",
      "[SPLIT] Train: (4080, 14, 128, 1), Val: (1020, 14, 128, 1)\n",
      "Epoch 1/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 138ms/step - accuracy: 0.5245 - loss: 0.7321 - val_accuracy: 0.4725 - val_loss: 0.6957 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.5600 - loss: 0.6819 - val_accuracy: 0.4794 - val_loss: 0.6946 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.5868 - loss: 0.6649 - val_accuracy: 0.4980 - val_loss: 0.6924 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.6152 - loss: 0.6463 - val_accuracy: 0.5304 - val_loss: 0.6893 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.6539 - loss: 0.6105 - val_accuracy: 0.6010 - val_loss: 0.6842 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.6730 - loss: 0.5883 - val_accuracy: 0.6059 - val_loss: 0.6749 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.7022 - loss: 0.5595 - val_accuracy: 0.6275 - val_loss: 0.6666 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.7100 - loss: 0.5480 - val_accuracy: 0.6294 - val_loss: 0.6512 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.7267 - loss: 0.5178 - val_accuracy: 0.6059 - val_loss: 0.6428 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.7596 - loss: 0.4985 - val_accuracy: 0.6137 - val_loss: 0.6254 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.7598 - loss: 0.4831 - val_accuracy: 0.6284 - val_loss: 0.6095 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.7605 - loss: 0.4779 - val_accuracy: 0.6451 - val_loss: 0.5934 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.7809 - loss: 0.4574 - val_accuracy: 0.6520 - val_loss: 0.5792 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8012 - loss: 0.4270 - val_accuracy: 0.6382 - val_loss: 0.6019 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8147 - loss: 0.4155 - val_accuracy: 0.6471 - val_loss: 0.5874 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8216 - loss: 0.3865 - val_accuracy: 0.6324 - val_loss: 0.5960 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8306 - loss: 0.3705 - val_accuracy: 0.6461 - val_loss: 0.5814 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8453 - loss: 0.3612 - val_accuracy: 0.6971 - val_loss: 0.5181 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 159ms/step - accuracy: 0.8196 - loss: 0.4039 - val_accuracy: 0.7235 - val_loss: 0.5449 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8223 - loss: 0.4024 - val_accuracy: 0.7500 - val_loss: 0.4908 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8414 - loss: 0.3655 - val_accuracy: 0.7647 - val_loss: 0.4499 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8434 - loss: 0.3589 - val_accuracy: 0.7588 - val_loss: 0.4214 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8654 - loss: 0.3340 - val_accuracy: 0.7765 - val_loss: 0.4363 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8564 - loss: 0.3328 - val_accuracy: 0.7588 - val_loss: 0.4853 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8711 - loss: 0.3185 - val_accuracy: 0.7686 - val_loss: 0.4224 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8625 - loss: 0.3218 - val_accuracy: 0.8108 - val_loss: 0.3682 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8608 - loss: 0.3240 - val_accuracy: 0.8784 - val_loss: 0.3329 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8561 - loss: 0.3373 - val_accuracy: 0.8461 - val_loss: 0.3479 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8725 - loss: 0.3057\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.8694 - loss: 0.3189 - val_accuracy: 0.8627 - val_loss: 0.2949 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.8779 - loss: 0.2951 - val_accuracy: 0.8539 - val_loss: 0.3030 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8855 - loss: 0.2726 - val_accuracy: 0.8794 - val_loss: 0.2726 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9020 - loss: 0.2576 - val_accuracy: 0.9314 - val_loss: 0.2314 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9012 - loss: 0.2576 - val_accuracy: 0.9265 - val_loss: 0.2239 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9022 - loss: 0.2465 - val_accuracy: 0.9422 - val_loss: 0.2040 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9032 - loss: 0.2400 - val_accuracy: 0.9559 - val_loss: 0.1888 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9039 - loss: 0.2419 - val_accuracy: 0.9520 - val_loss: 0.1869 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8995 - loss: 0.2560 - val_accuracy: 0.9422 - val_loss: 0.2203 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9042 - loss: 0.2455 - val_accuracy: 0.9529 - val_loss: 0.1835 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9076 - loss: 0.2273 - val_accuracy: 0.9559 - val_loss: 0.1965 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9108 - loss: 0.2219 - val_accuracy: 0.9559 - val_loss: 0.1756 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9061 - loss: 0.2413 - val_accuracy: 0.9500 - val_loss: 0.1705 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9064 - loss: 0.2243 - val_accuracy: 0.9480 - val_loss: 0.1932 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9103 - loss: 0.2329 - val_accuracy: 0.9529 - val_loss: 0.1632 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9223 - loss: 0.2022\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9105 - loss: 0.2238 - val_accuracy: 0.9578 - val_loss: 0.1757 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9164 - loss: 0.2055 - val_accuracy: 0.9618 - val_loss: 0.1538 - learning_rate: 2.5000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9277 - loss: 0.1971 - val_accuracy: 0.9578 - val_loss: 0.1487 - learning_rate: 2.5000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9189 - loss: 0.2005 - val_accuracy: 0.9598 - val_loss: 0.1476 - learning_rate: 2.5000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9245 - loss: 0.1941 - val_accuracy: 0.9588 - val_loss: 0.1421 - learning_rate: 2.5000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9289 - loss: 0.1947 - val_accuracy: 0.9588 - val_loss: 0.1431 - learning_rate: 2.5000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9245 - loss: 0.2043 - val_accuracy: 0.9588 - val_loss: 0.1572 - learning_rate: 2.5000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9186 - loss: 0.2012 - val_accuracy: 0.9618 - val_loss: 0.1344 - learning_rate: 2.5000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9247 - loss: 0.1944\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9167 - loss: 0.2058 - val_accuracy: 0.9618 - val_loss: 0.1367 - learning_rate: 2.5000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9252 - loss: 0.1934 - val_accuracy: 0.9637 - val_loss: 0.1278 - learning_rate: 1.2500e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9284 - loss: 0.1912 - val_accuracy: 0.9627 - val_loss: 0.1251 - learning_rate: 1.2500e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9243 - loss: 0.1968 - val_accuracy: 0.9647 - val_loss: 0.1282 - learning_rate: 1.2500e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9250 - loss: 0.1992 - val_accuracy: 0.9618 - val_loss: 0.1227 - learning_rate: 1.2500e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9299 - loss: 0.1931 - val_accuracy: 0.9637 - val_loss: 0.1299 - learning_rate: 1.2500e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9282 - loss: 0.1878 - val_accuracy: 0.9598 - val_loss: 0.1260 - learning_rate: 1.2500e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9228 - loss: 0.1974 - val_accuracy: 0.9647 - val_loss: 0.1221 - learning_rate: 1.2500e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9301 - loss: 0.1832 - val_accuracy: 0.9657 - val_loss: 0.1220 - learning_rate: 1.2500e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9238 - loss: 0.1940 - val_accuracy: 0.9657 - val_loss: 0.1206 - learning_rate: 1.2500e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9277 - loss: 0.1891 - val_accuracy: 0.9647 - val_loss: 0.1224 - learning_rate: 1.2500e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9260 - loss: 0.1887 - val_accuracy: 0.9608 - val_loss: 0.1202 - learning_rate: 1.2500e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9268 - loss: 0.1933\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9235 - loss: 0.1973 - val_accuracy: 0.9657 - val_loss: 0.1213 - learning_rate: 1.2500e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9287 - loss: 0.1900 - val_accuracy: 0.9676 - val_loss: 0.1209 - learning_rate: 6.2500e-05\n",
      "Epoch 66/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9279 - loss: 0.1840 - val_accuracy: 0.9676 - val_loss: 0.1173 - learning_rate: 6.2500e-05\n",
      "Epoch 67/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9353 - loss: 0.1779 - val_accuracy: 0.9657 - val_loss: 0.1164 - learning_rate: 6.2500e-05\n",
      "Epoch 68/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9338 - loss: 0.1794 - val_accuracy: 0.9657 - val_loss: 0.1151 - learning_rate: 6.2500e-05\n",
      "Epoch 69/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9299 - loss: 0.1768 - val_accuracy: 0.9637 - val_loss: 0.1151 - learning_rate: 6.2500e-05\n",
      "Epoch 70/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9221 - loss: 0.1916 - val_accuracy: 0.9647 - val_loss: 0.1145 - learning_rate: 6.2500e-05\n",
      "Epoch 71/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9260 - loss: 0.1890 - val_accuracy: 0.9647 - val_loss: 0.1157 - learning_rate: 6.2500e-05\n",
      "Epoch 72/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9319 - loss: 0.1859 - val_accuracy: 0.9647 - val_loss: 0.1169 - learning_rate: 6.2500e-05\n",
      "Epoch 73/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9282 - loss: 0.1756 - val_accuracy: 0.9637 - val_loss: 0.1136 - learning_rate: 6.2500e-05\n",
      "Epoch 74/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9297 - loss: 0.1836 - val_accuracy: 0.9647 - val_loss: 0.1152 - learning_rate: 6.2500e-05\n",
      "Epoch 75/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9292 - loss: 0.1792 - val_accuracy: 0.9676 - val_loss: 0.1190 - learning_rate: 6.2500e-05\n",
      "Epoch 76/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9304 - loss: 0.1838 - val_accuracy: 0.9637 - val_loss: 0.1166 - learning_rate: 6.2500e-05\n",
      "Epoch 77/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9400 - loss: 0.1669 - val_accuracy: 0.9667 - val_loss: 0.1145 - learning_rate: 6.2500e-05\n",
      "Epoch 78/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9250 - loss: 0.1936 - val_accuracy: 0.9667 - val_loss: 0.1152 - learning_rate: 6.2500e-05\n",
      "Epoch 79/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9309 - loss: 0.1809 - val_accuracy: 0.9676 - val_loss: 0.1143 - learning_rate: 6.2500e-05\n",
      "Epoch 80/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9299 - loss: 0.1828 - val_accuracy: 0.9657 - val_loss: 0.1129 - learning_rate: 6.2500e-05\n",
      "Epoch 81/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9433 - loss: 0.1594\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9370 - loss: 0.1802 - val_accuracy: 0.9647 - val_loss: 0.1127 - learning_rate: 6.2500e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9294 - loss: 0.1787 - val_accuracy: 0.9667 - val_loss: 0.1129 - learning_rate: 3.1250e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9380 - loss: 0.1762 - val_accuracy: 0.9667 - val_loss: 0.1124 - learning_rate: 3.1250e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9314 - loss: 0.1799 - val_accuracy: 0.9676 - val_loss: 0.1133 - learning_rate: 3.1250e-05\n",
      "Epoch 85/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.9385 - loss: 0.1674\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9353 - loss: 0.1793 - val_accuracy: 0.9657 - val_loss: 0.1136 - learning_rate: 3.1250e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9336 - loss: 0.1724 - val_accuracy: 0.9657 - val_loss: 0.1128 - learning_rate: 1.5625e-05\n",
      "Epoch 87/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9328 - loss: 0.1772 - val_accuracy: 0.9667 - val_loss: 0.1121 - learning_rate: 1.5625e-05\n",
      "Epoch 88/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9260 - loss: 0.1824 - val_accuracy: 0.9647 - val_loss: 0.1122 - learning_rate: 1.5625e-05\n",
      "Epoch 89/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.9396 - loss: 0.1600\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9338 - loss: 0.1750 - val_accuracy: 0.9667 - val_loss: 0.1115 - learning_rate: 1.5625e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9338 - loss: 0.1727 - val_accuracy: 0.9676 - val_loss: 0.1119 - learning_rate: 7.8125e-06\n",
      "Epoch 91/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9355 - loss: 0.1720 - val_accuracy: 0.9676 - val_loss: 0.1115 - learning_rate: 7.8125e-06\n",
      "Epoch 92/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9311 - loss: 0.1794 - val_accuracy: 0.9667 - val_loss: 0.1119 - learning_rate: 7.8125e-06\n",
      "Epoch 93/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9413 - loss: 0.1680\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9314 - loss: 0.1853 - val_accuracy: 0.9676 - val_loss: 0.1127 - learning_rate: 7.8125e-06\n",
      "Epoch 94/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9284 - loss: 0.1812 - val_accuracy: 0.9676 - val_loss: 0.1129 - learning_rate: 3.9063e-06\n",
      "Epoch 95/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9311 - loss: 0.1818 - val_accuracy: 0.9676 - val_loss: 0.1128 - learning_rate: 3.9063e-06\n",
      "Epoch 96/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9341 - loss: 0.1745 - val_accuracy: 0.9676 - val_loss: 0.1127 - learning_rate: 3.9063e-06\n",
      "Epoch 97/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.9400 - loss: 0.1572\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9365 - loss: 0.1685 - val_accuracy: 0.9676 - val_loss: 0.1127 - learning_rate: 3.9063e-06\n",
      "Epoch 98/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9353 - loss: 0.1741 - val_accuracy: 0.9676 - val_loss: 0.1127 - learning_rate: 1.9531e-06\n",
      "Epoch 99/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9272 - loss: 0.1833 - val_accuracy: 0.9676 - val_loss: 0.1123 - learning_rate: 1.9531e-06\n",
      "Epoch 100/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9292 - loss: 0.1783 - val_accuracy: 0.9676 - val_loss: 0.1123 - learning_rate: 1.9531e-06\n",
      "Epoch 101/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9327 - loss: 0.1689\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9275 - loss: 0.1852 - val_accuracy: 0.9676 - val_loss: 0.1123 - learning_rate: 1.9531e-06\n",
      "Epoch 102/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9346 - loss: 0.1661 - val_accuracy: 0.9676 - val_loss: 0.1122 - learning_rate: 1.0000e-06\n",
      "Epoch 103/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9355 - loss: 0.1678 - val_accuracy: 0.9676 - val_loss: 0.1121 - learning_rate: 1.0000e-06\n",
      "Epoch 104/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9331 - loss: 0.1798 - val_accuracy: 0.9676 - val_loss: 0.1120 - learning_rate: 1.0000e-06\n",
      "Epoch 105/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9279 - loss: 0.1849 - val_accuracy: 0.9676 - val_loss: 0.1119 - learning_rate: 1.0000e-06\n",
      "Epoch 106/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9331 - loss: 0.1700 - val_accuracy: 0.9676 - val_loss: 0.1119 - learning_rate: 1.0000e-06\n",
      "Epoch 106: early stopping\n",
      "Restoring model weights from the end of the best epoch: 91.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] Model saved as cole-final.h5\n",
      "\n",
      "[CONFUSION MATRIX]\n",
      "[[462  18]\n",
      " [ 15 525]]\n",
      "[VAL] acc=0.9676, prec=0.9669, rec=0.9722, f1=0.9695\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 4: 80:20 Train / Validation Split (EEGNet)\n",
    "# ====================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, SeparableConv2D, BatchNormalization, Activation,\n",
    "    DepthwiseConv2D, Conv2D, AveragePooling2D,\n",
    "    Dropout, Dense, Flatten\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# --------------------------------------------------------------------\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# EEGNet model\n",
    "# --------------------------------------------------------------------\n",
    "def create_eegnet_model(input_shape, dropout_rate=0.5, num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(16, (1, 64), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = DepthwiseConv2D(\n",
    "        (input_shape[0], 1),\n",
    "        depth_multiplier=2,\n",
    "        padding='valid',\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 4))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = SeparableConv2D(16, (1, 16), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 8))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Compile model\n",
    "# --------------------------------------------------------------------\n",
    "def compile_model():\n",
    "    input_shape = (Xtr.shape[1], Xtr.shape[2], Xtr.shape[3])\n",
    "    model = create_eegnet_model(input_shape)\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-3),\n",
    "        loss=BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[BinaryAccuracy(name=\"accuracy\", threshold=0.5)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def make_callbacks():\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"loss\",\n",
    "            factor=0.5,\n",
    "            patience=4,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Use ALL normalized training data\n",
    "# --------------------------------------------------------------------\n",
    "Xtr = X_train_norm\n",
    "ytr = y_train_norm\n",
    "\n",
    "print(f\"[TRAIN] Xtr: {Xtr.shape}, ytr: {ytr.shape}\")\n",
    "print(\"[INFO] Test / LOSO subject is NOT used here.\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 80:20 Stratified Split\n",
    "# --------------------------------------------------------------------\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(sss.split(Xtr, ytr))\n",
    "\n",
    "X_train, X_val = Xtr[train_idx], Xtr[val_idx]\n",
    "y_train, y_val = ytr[train_idx], ytr[val_idx]\n",
    "\n",
    "print(f\"[SPLIT] Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Train model\n",
    "# --------------------------------------------------------------------\n",
    "model = compile_model()\n",
    "callbacks = make_callbacks()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Save final model\n",
    "# --------------------------------------------------------------------\n",
    "model.save(\"amin1-final.h5\")\n",
    "print(\"[SAVE] Model saved as cole-final.h5\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Validation Confusion Matrix\n",
    "# --------------------------------------------------------------------\n",
    "y_prob = model.predict(X_val, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "y_true = y_val.astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "eps = 1e-12\n",
    "\n",
    "acc  = (tp + tn) / max(tp + tn + fp + fn, eps)\n",
    "prec = tp / max(tp + fp, eps)\n",
    "rec  = tp / max(tp + fn, eps)\n",
    "f1   = 2 * prec * rec / max(prec + rec, eps)\n",
    "\n",
    "print(\"\\n[CONFUSION MATRIX]\")\n",
    "print(cm)\n",
    "print(f\"[VAL] acc={acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e2a13-6367-41ca-be66-a43594c2940e",
   "metadata": {},
   "source": [
    "Cell 8 — Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2ac6662-33c1-4e25-b6c5-d47c860fba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Preparing TEST data...\n",
      "[TEST] Xte shape: (300, 14, 128, 1)\n",
      "[TEST] yte shape: (300,), class distribution: (array([0]), array([300]))\n",
      "[TEST] Loading final model: amin1-final.h5\n",
      "[TEST] Running inference on TEST subject...\n",
      "[TEST] Mean predicted probability: 0.5965\n",
      "\n",
      "[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[105 195]\n",
      " [  0   0]]\n",
      "Accuracy : 0.3500\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-score : 0.0000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Inference on TEST subject (adam) using FINAL EEGNet\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Prepare TEST data\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Preparing TEST data...\")\n",
    "\n",
    "Xte = X_test_norm\n",
    "yte = y_test_norm.astype(int)\n",
    "\n",
    "print(f\"[TEST] Xte shape: {Xte.shape}\")\n",
    "print(f\"[TEST] yte shape: {yte.shape}, class distribution: {np.unique(yte, return_counts=True)}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Load FINAL trained model\n",
    "# ------------------------------------------------------------------\n",
    "model_path = \"amin1-final.h5\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        \"Final model 'cole-final.h5' not found. \"\n",
    "        \"Please run Cell 4 (80:20 training) first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[TEST] Loading final model: {model_path}\")\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Run inference on TEST subject\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Running inference on TEST subject...\")\n",
    "\n",
    "y_prob = model.predict(Xte, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(f\"[TEST] Mean predicted probability: {y_prob.mean():.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Metrics on TEST subject (segment-level)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc = accuracy_score(yte, y_pred)\n",
    "prec = precision_score(yte, y_pred, zero_division=0)\n",
    "rec = recall_score(yte, y_pred, zero_division=0)\n",
    "f1 = f1_score(yte, y_pred, zero_division=0)\n",
    "\n",
    "print(\"\\n[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572dbf3e-bbb5-4865-883f-30e374700949",
   "metadata": {},
   "source": [
    "Cell 9 - Performance Matrices for each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21a57240-50ea-4736-a7ac-52ef7878b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] Verifying predictions and labels are available...\n",
      "[CHECK] Found predictions for 300 test segments.\n",
      "\n",
      "[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[105 195]\n",
      " [  0   0]]\n",
      "Accuracy : 0.3500\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-score : 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Final model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================\n",
    "# Cell 6: Confusion Matrix & Metrics on TEST (FINAL EEGNet)\n",
    "# =======================================================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# ------------------------------------------------------------------\n",
    "def compute_metrics_from_cm(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    eps = 1e-12\n",
    "    acc  = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec  = tp / (tp + fn + eps)\n",
    "    f1   = 2 * prec * rec / (prec + rec + eps)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], ['0', '1'])\n",
    "    plt.yticks([0, 1], ['0', '1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(\n",
    "                j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Validate upstream artifacts\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[CHECK] Verifying predictions and labels are available...\")\n",
    "\n",
    "if 'y_pred' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Predictions 'y_pred' not found. \"\n",
    "        \"Please run Cell 5 (final model inference) first.\"\n",
    "    )\n",
    "\n",
    "if 'yte' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Test labels 'yte' not found. \"\n",
    "        \"Please run Cell 5 first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[CHECK] Found predictions for {len(y_pred)} test segments.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Confusion Matrix & Metrics (FINAL model)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc, prec, rec, f1 = compute_metrics_from_cm(cm)\n",
    "\n",
    "print(\"\\n[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(cm, title=\"Confusion Matrix - FINAL EEGNet\")\n",
    "\n",
    "print(\"[DONE] Final model evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0f120-39fd-497e-9098-05e6c82f4807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
