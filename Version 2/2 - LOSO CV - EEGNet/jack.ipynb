{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af5b471-cea9-4194-bc23-da5cdc2f9c67",
   "metadata": {},
   "source": [
    "Cell 1 — Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64aaa897-ab61-4eb4-a558-4c078acf751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Base path set to:\n",
      "C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\n",
      "[INFO] Found 18 folders:\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Selected test subject (LOSO): Jack\n",
      "[INFO] Training subjects (17):\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Sampling rate (SFREQ) = 128.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 1: Setup & discovery\n",
    "# =========================\n",
    "\n",
    "import os  # file ops\n",
    "import glob  # file matching\n",
    "import numpy as np  # arrays\n",
    "import random  # seeds\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data path\n",
    "BASE_PATH = r\"C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\"\n",
    "print(f\"[INFO] Base path set to:\\n{BASE_PATH}\")\n",
    "\n",
    "# List subject folders\n",
    "all_folders = [p for p in sorted(os.listdir(BASE_PATH)) if os.path.isdir(os.path.join(BASE_PATH, p))]\n",
    "print(f\"[INFO] Found {len(all_folders)} folders:\\n{all_folders}\")\n",
    "\n",
    "# Expected count (change if needed)\n",
    "assert len(all_folders) == 18, f\"Expected 18 folders, found {len(all_folders)}.\"\n",
    "\n",
    "# LOSO test subject\n",
    "TEST_SUBJECT = \"Jack\"\n",
    "print(f\"[INFO] Selected test subject (LOSO): {TEST_SUBJECT}\")\n",
    "\n",
    "# Train subjects = remaining\n",
    "train_subjects = [s for s in all_folders if s.lower() != TEST_SUBJECT.lower()]\n",
    "print(f\"[INFO] Training subjects ({len(train_subjects)}):\\n{train_subjects}\")\n",
    "\n",
    "# Sampling rate\n",
    "SFREQ = 128.0\n",
    "print(f\"[INFO] Sampling rate (SFREQ) = {SFREQ} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e649681-2861-4ce2-bbd1-0d9dd3359f42",
   "metadata": {},
   "source": [
    "Cell 2 — Load EEG + labels per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943c5c6e-a7aa-4618-899c-daf341c25358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 2] Loading data from folders...\n",
      "[STEP 2]       Amin | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Amin | EEG=          Amin.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Amin1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]      Amin1 | EEG=         Amin1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Cole | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Cole | EEG=          Cole.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Daniel | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]     Daniel | EEG=        Daniel.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]    Ismayil | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]    Ismayil | EEG=       Ismayil.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Jack | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Jack | EEG=          Jack.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]      James | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      James | EEG=         James.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Josh | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Josh | EEG=          Josh.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Marjan | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Marjan | EEG=        Marjan.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]        Max | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Max | EEG=           Max.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Mina | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mina | EEG=          Mina.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 1 | EEG=        Mina 1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 3 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 3 | EEG=        Mina 3.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]   Mohammad | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]   Mohammad | EEG=      Mohammad.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Mona | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mona | EEG=          Mona.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Roddy | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      Roddy | EEG=         Roddy.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]        Sam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Sam | EEG=           Sam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       adam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       adam | EEG=          adam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2] Loading complete ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 2: Load data (EMPTY LABEL => CLASS 0)\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path  # helper\n",
    "\n",
    "def load_subject(subj_dir: str, subj_name: str, expected_channels: int = 14):\n",
    "    \"\"\"\n",
    "    EEG file: {subj_name}.npy\n",
    "    Label file: {subj_name}_target.npy\n",
    "    Label rule:\n",
    "      - empty array -> class 0\n",
    "      - [0] -> class 0\n",
    "      - [1] -> class 1\n",
    "    \"\"\"\n",
    "\n",
    "    # EEG file path\n",
    "    eeg_file = os.path.join(subj_dir, f\"{subj_name}.npy\")\n",
    "    assert os.path.exists(eeg_file), f\"[ERROR] Missing EEG file: {eeg_file}\"\n",
    "\n",
    "    # Label file path\n",
    "    target_file = os.path.join(subj_dir, f\"{subj_name}_target.npy\")\n",
    "    assert os.path.exists(target_file), f\"[ERROR] Missing label file: {target_file}\"\n",
    "\n",
    "    # Load EEG\n",
    "    X_raw = np.load(eeg_file, allow_pickle=True)\n",
    "    X_raw = np.asarray(X_raw)\n",
    "\n",
    "    # Standardize EEG to (n_trials, 14, T)\n",
    "    if X_raw.ndim == 2:\n",
    "        X_trials = X_raw[None, :, :]\n",
    "    elif X_raw.ndim == 3:\n",
    "        dims = list(X_raw.shape)\n",
    "        chan_axes = [i for i, d in enumerate(dims) if d == expected_channels]\n",
    "        chan_axis = chan_axes[0] if len(chan_axes) == 1 else 1\n",
    "        time_axis = int(np.argmax(dims))\n",
    "        trial_axis = [i for i in range(3) if i not in (chan_axis, time_axis)][0]\n",
    "        X_trials = np.moveaxis(X_raw, (trial_axis, chan_axis, time_axis), (0, 1, 2))\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] Unsupported EEG shape {X_raw.shape} in {eeg_file}\")\n",
    "\n",
    "    assert X_trials.shape[1] == expected_channels, f\"[ERROR] Expected {expected_channels} channels, got {X_trials.shape[1]} in {eeg_file}\"\n",
    "\n",
    "    # Load label\n",
    "    y_raw = np.load(target_file, allow_pickle=True)\n",
    "    y_arr = np.asarray(y_raw).ravel()\n",
    "\n",
    "    # Apply your rule\n",
    "    if y_arr.size == 0:\n",
    "        label = 0\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\")\n",
    "    else:\n",
    "        label = int(y_arr[0])\n",
    "        if label not in (0, 1):\n",
    "            raise ValueError(f\"[ERROR] Invalid label {label} in {target_file} (must be 0 or 1)\")\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape={y_arr.shape}, value={y_arr} -> class={label}\")\n",
    "\n",
    "    # Repeat label for all trials\n",
    "    y_trials = np.full((X_trials.shape[0],), label, dtype=np.int32)\n",
    "\n",
    "    return X_trials.astype(np.float32), y_trials.astype(np.int32), eeg_file, target_file\n",
    "\n",
    "\n",
    "# Load all subjects\n",
    "subject_data = {}\n",
    "print(\"[STEP 2] Loading data from folders...\")\n",
    "\n",
    "for subj in all_folders:\n",
    "    subj_dir = os.path.join(BASE_PATH, subj)\n",
    "    X_trials, y_trials, eeg_file, target_file = load_subject(subj_dir, subj)\n",
    "\n",
    "    subject_data[subj] = (X_trials, y_trials)\n",
    "\n",
    "    print(f\"[STEP 2] {subj:>10} | EEG={Path(eeg_file).name:>18} | X={X_trials.shape} | y={y_trials.shape} | class={y_trials[0]}\")\n",
    "\n",
    "print(\"[STEP 2] Loading complete ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd93ca8-a94c-485c-8a61-e6e7441f6299",
   "metadata": {},
   "source": [
    "Cell 3 — Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733e592c-98e1-4cbd-afd2-6fdc8cf407f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 3] Preprocessing functions ready ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3: Preprocessing functions\n",
    "# =========================\n",
    "\n",
    "import mne  # EEG processing\n",
    "import pywt  # wavelets\n",
    "from sklearn.decomposition import FastICA  # ICA\n",
    "from typing import Optional, Tuple, Union, Sequence, Dict, List  # typing\n",
    "\n",
    "def wavelet_enhanced_ica(\n",
    "    data: np.ndarray,\n",
    "    n_components: int = 10,\n",
    "    wavelet: str = \"db4\",\n",
    "    level: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> np.ndarray:\n",
    "    n_ch, n_t = data.shape\n",
    "    n_components = min(n_components, n_ch)\n",
    "\n",
    "    coeffs = pywt.wavedec(data, wavelet=wavelet, level=level, axis=1)\n",
    "    A = coeffs[0]\n",
    "\n",
    "    ica = FastICA(n_components=n_components, random_state=random_state)\n",
    "    S = ica.fit_transform(A.T).T\n",
    "    A_denoised = ica.inverse_transform(S.T).T\n",
    "\n",
    "    coeffs[0] = A_denoised\n",
    "    cleaned = pywt.waverec(coeffs, wavelet=wavelet, axis=1)\n",
    "\n",
    "    if cleaned.shape[1] != n_t:\n",
    "        if cleaned.shape[1] > n_t:\n",
    "            cleaned = cleaned[:, :n_t]\n",
    "        else:\n",
    "            cleaned = np.pad(cleaned, ((0, 0), (0, n_t - cleaned.shape[1])), mode=\"constant\")\n",
    "    return cleaned\n",
    "\n",
    "def _names_from_index_mapping(n_channels: int, index_to_name: Optional[Dict[int, str]]) -> List[str]:\n",
    "    if index_to_name is None:\n",
    "        return [f\"EEG{i+1}\" for i in range(n_channels)]\n",
    "    keys = list(index_to_name.keys())\n",
    "    is_zero_based = (0 in keys) and (1 not in keys)\n",
    "    names = []\n",
    "    for i in range(n_channels):\n",
    "        key = i if is_zero_based else (i + 1)\n",
    "        names.append(index_to_name.get(key, f\"EEG{i+1}\"))\n",
    "    return names\n",
    "\n",
    "def preprocess_eeg(\n",
    "    eeg: np.ndarray,\n",
    "    sfreq: float,\n",
    "    *,\n",
    "    index_to_name: Optional[Dict[int, str]] = None,\n",
    "    use_standard_1010: bool = True,\n",
    "    resample_to: Optional[float] = None,\n",
    "    notch_freqs: Union[None, float, Sequence[float]] = 50.0,\n",
    "    highpass: Optional[float] = 0.05,\n",
    "    bad_point_z: float = 6.0,\n",
    "    bad_channel_z: float = 5.0,\n",
    "    interpolate_bad_channels: bool = True,\n",
    "    car: bool = True,\n",
    "    use_wica: bool = True,\n",
    "    wica_components: int = 10,\n",
    "    wica_wavelet: str = \"db4\",\n",
    "    wica_level: int = 3,\n",
    "    wica_random_state: int = 42,\n",
    "    return_raw: bool = False,\n",
    "):\n",
    "\n",
    "    eeg = np.asarray(eeg, dtype=np.float32)\n",
    "    assert eeg.ndim == 2, \"eeg must be 2D: (n_channels, n_times)\"\n",
    "\n",
    "    n_channels, _ = eeg.shape\n",
    "    ch_names = _names_from_index_mapping(n_channels, index_to_name)\n",
    "    ch_types = ['eog' if str(n).upper().startswith(\"EOG\") else 'eeg' for n in ch_names]\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    raw = mne.io.RawArray(eeg, info, verbose=False)\n",
    "\n",
    "    montage_applied = False\n",
    "    if use_standard_1010:\n",
    "        try:\n",
    "            mont = mne.channels.make_standard_montage(\"standard_1010\")\n",
    "            raw.set_montage(mont, match_case=False, on_missing=\"ignore\")\n",
    "            montage_applied = True\n",
    "        except Exception:\n",
    "            montage_applied = False\n",
    "\n",
    "    if resample_to is not None and float(resample_to) != float(sfreq):\n",
    "        raw.resample(sfreq=resample_to, npad=\"auto\")\n",
    "    sfreq_out = float(raw.info[\"sfreq\"])\n",
    "\n",
    "    if notch_freqs is not None:\n",
    "        raw.notch_filter(freqs=notch_freqs, verbose=False)\n",
    "    if highpass is not None:\n",
    "        raw.filter(l_freq=highpass, h_freq=None, verbose=False)\n",
    "\n",
    "    X = raw.get_data()\n",
    "    mu = np.mean(X, axis=1, keepdims=True)\n",
    "    sd = np.std(X, axis=1, keepdims=True) + 1e-12\n",
    "    hi = mu + bad_point_z * sd\n",
    "    lo = mu - bad_point_z * sd\n",
    "    bad_idx = (X > hi) | (X < lo)\n",
    "\n",
    "    if np.any(bad_idx):\n",
    "        X_fixed = X.copy()\n",
    "        t = np.arange(X.shape[1], dtype=float)\n",
    "        for ch in range(n_channels):\n",
    "            mask = bad_idx[ch]\n",
    "            if mask.any():\n",
    "                good = ~mask\n",
    "                if good.sum() >= 2:\n",
    "                    X_fixed[ch, mask] = np.interp(t[mask], t[good], X_fixed[ch, good])\n",
    "        raw._data = X_fixed\n",
    "\n",
    "    if interpolate_bad_channels and montage_applied:\n",
    "        X = raw.get_data(picks=\"eeg\")\n",
    "        if X.size > 0:\n",
    "            ch_std = X.std(axis=1)\n",
    "            med = np.median(ch_std)\n",
    "            mad = np.median(np.abs(ch_std - med)) + 1e-12\n",
    "            z = 0.6745 * (ch_std - med) / mad\n",
    "            eeg_names = mne.pick_info(raw.info, mne.pick_types(raw.info, eeg=True)).ch_names\n",
    "            bads = [eeg_names[i] for i in np.where(np.abs(z) > bad_channel_z)[0]]\n",
    "            raw.info[\"bads\"] = bads\n",
    "            if bads:\n",
    "                raw.interpolate_bads(reset_bads=True, verbose=False)\n",
    "\n",
    "    if car:\n",
    "        raw.set_eeg_reference(\"average\", projection=True)\n",
    "        raw.apply_proj()\n",
    "\n",
    "    if use_wica:\n",
    "        cleaned = wavelet_enhanced_ica(\n",
    "            raw.get_data(),\n",
    "            n_components=wica_components,\n",
    "            wavelet=wica_wavelet,\n",
    "            level=wica_level,\n",
    "            random_state=wica_random_state\n",
    "        )\n",
    "        raw = mne.io.RawArray(cleaned, raw.info, verbose=False)\n",
    "\n",
    "    cleaned = raw.get_data()\n",
    "    return cleaned, sfreq_out\n",
    "\n",
    "print(\"[STEP 3] Preprocessing functions ready ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a9d20-889d-45c4-bbc2-de62216908cd",
   "metadata": {},
   "source": [
    "Cell 4 — Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bfcbde0-1cc2-409c-8627-93da46606661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 4] Starting preprocessing...\n",
      "[INFO] USE_WICA = True\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 01/17: Amin | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 02/17: Amin1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 03/17: Cole | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 04/17: Daniel | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 05/17: Ismayil | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 06/17: James | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 07/17: Josh | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 08/17: Marjan | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 09/17: Max | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 10/17: Mina | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 11/17: Mina 1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 12/17: Mina 3 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 13/17: Mohammad | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 14/17: Mona | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 15/17: Roddy | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 16/17: Sam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 17/17: adam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Test subject: Jack | trials=1 | fs_out=128.0\n",
      "[STEP 4] Preprocessing done ✅\n",
      "  X_train_trials_all: (17, 14, 38400)\n",
      "  y_train_trials_all: (17,) (array([0, 1], dtype=int32), array([9, 8]))\n",
      "  X_test_trials_all : (1, 14, 38400)\n",
      "  y_test_trials_all : (1,) (array([1], dtype=int32), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 4: Apply preprocessing (LOSO)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 4] Starting preprocessing...\")\n",
    "\n",
    "USE_WICA = True\n",
    "print(f\"[INFO] USE_WICA = {USE_WICA}\")\n",
    "\n",
    "X_train_trials_all, y_train_trials_all = [], []\n",
    "X_test_trials_all, y_test_trials_all = [], []\n",
    "\n",
    "def preprocess_trials(X_trials, y_trials):\n",
    "    cleaned_list = []\n",
    "    label_list = []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        trial = X_trials[i]\n",
    "        label = int(y_trials[i])\n",
    "\n",
    "        cleaned, fs_out = preprocess_eeg(\n",
    "            eeg=trial,\n",
    "            sfreq=SFREQ,\n",
    "            use_wica=USE_WICA\n",
    "        )\n",
    "\n",
    "        cleaned_list.append(cleaned.astype(np.float32, copy=False))\n",
    "        label_list.append(label)\n",
    "\n",
    "    return cleaned_list, np.array(label_list, dtype=np.int32), fs_out\n",
    "\n",
    "# Train subjects\n",
    "for idx, subj in enumerate(train_subjects, start=1):\n",
    "    X_trials, y_trials = subject_data[subj]\n",
    "    cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "    X_train_trials_all.extend(cleaned_trials)\n",
    "    y_train_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "    print(f\"[STEP 4] Train {idx:02d}/{len(train_subjects)}: {subj} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Test subject\n",
    "X_trials, y_trials = subject_data[TEST_SUBJECT]\n",
    "cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "X_test_trials_all.extend(cleaned_trials)\n",
    "y_test_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "print(f\"[STEP 4] Test subject: {TEST_SUBJECT} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Convert to arrays\n",
    "X_train_trials_all = np.array(X_train_trials_all, dtype=np.float32)\n",
    "y_train_trials_all = np.array(y_train_trials_all, dtype=np.int32)\n",
    "\n",
    "X_test_trials_all = np.array(X_test_trials_all, dtype=np.float32)\n",
    "y_test_trials_all = np.array(y_test_trials_all, dtype=np.int32)\n",
    "\n",
    "print(\"[STEP 4] Preprocessing done ✅\")\n",
    "print(\"  X_train_trials_all:\", X_train_trials_all.shape)\n",
    "print(\"  y_train_trials_all:\", y_train_trials_all.shape, np.unique(y_train_trials_all, return_counts=True))\n",
    "print(\"  X_test_trials_all :\", X_test_trials_all.shape)\n",
    "print(\"  y_test_trials_all :\", y_test_trials_all.shape, np.unique(y_test_trials_all, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc3da1-2967-4ee3-84cd-bfcf0f98c0b1",
   "metadata": {},
   "source": [
    "Cell 5 — Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c21ad54-7001-4039-9360-29dfa2905127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 5] Starting segmentation...\n",
      "[STEP 5] Segmented 5/17 trials...\n",
      "[STEP 5] Segmented 10/17 trials...\n",
      "[STEP 5] Segmented 15/17 trials...\n",
      "[STEP 5] Segmented 17/17 trials...\n",
      "[STEP 5] Segmented 1/1 trials...\n",
      "[STEP 5] Segmentation done ✅\n",
      "  X_train: (5100, 14, 128, 1)\n",
      "  y_train: (5100,) (array([0, 1], dtype=int32), array([2700, 2400]))\n",
      "  X_test : (300, 14, 128, 1)\n",
      "  y_test : (300,) (array([1], dtype=int32), array([300]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 5: Segmentation\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 5] Starting segmentation...\")\n",
    "\n",
    "def segment_trial(trial_2d: np.ndarray, label: int, segment_len: int = 128):\n",
    "    C, T = trial_2d.shape\n",
    "    n_segs = T // segment_len\n",
    "    use_T = n_segs * segment_len\n",
    "\n",
    "    if n_segs == 0:\n",
    "        return np.empty((0, C, segment_len), dtype=np.float32), np.empty((0,), dtype=np.int32)\n",
    "\n",
    "    trial_trim = trial_2d[:, :use_T]\n",
    "    Xsegs = trial_trim.reshape(C, n_segs, segment_len).transpose(1, 0, 2)\n",
    "    ysegs = np.full((n_segs,), int(label), dtype=np.int32)\n",
    "    return Xsegs.astype(np.float32, copy=False), ysegs\n",
    "\n",
    "def segment_dataset(X_trials: np.ndarray, y_trials: np.ndarray, segment_len: int = 128):\n",
    "    X_all, y_all = [], []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        Xsegs, ysegs = segment_trial(X_trials[i], int(y_trials[i]), segment_len=segment_len)\n",
    "        X_all.append(Xsegs)\n",
    "        y_all.append(ysegs)\n",
    "\n",
    "        if (i + 1) % 5 == 0 or (i + 1) == X_trials.shape[0]:\n",
    "            print(f\"[STEP 5] Segmented {i+1}/{X_trials.shape[0]} trials...\")\n",
    "\n",
    "    X_out = np.concatenate(X_all, axis=0)\n",
    "    y_out = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    X_out = np.expand_dims(X_out, axis=-1)  # (N,14,128,1)\n",
    "    return X_out.astype(np.float32), y_out.astype(np.int32)\n",
    "\n",
    "X_train, y_train = segment_dataset(X_train_trials_all, y_train_trials_all, segment_len=128)\n",
    "X_test,  y_test  = segment_dataset(X_test_trials_all,  y_test_trials_all,  segment_len=128)\n",
    "\n",
    "print(\"[STEP 5] Segmentation done ✅\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  y_train:\", y_train.shape, np.unique(y_train, return_counts=True))\n",
    "print(\"  X_test :\", X_test.shape)\n",
    "print(\"  y_test :\", y_test.shape, np.unique(y_test, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b4ffd5-bf2f-490e-8854-0a2fc747f1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e69ba7-3952-47b1-bd80-f05237b2e018",
   "metadata": {},
   "source": [
    "Cell 6 — Split train/val + Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97fda4ae-e1c8-4fd1-bf0c-c82ae2a85764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 6] Normalizing using ALL training data (no train/val split)...\n",
      "[STEP 6] Normalization done ✅\n",
      "  X_train_norm: (5100, 14, 128, 1)\n",
      "  X_test_norm : (300, 14, 128, 1)\n",
      "  y_train_norm: (5100,)\n",
      "  y_test_norm : (300,)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 6: Normalize (ALL training data)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 6] Normalizing using ALL training data (no train/val split)...\")\n",
    "\n",
    "def normalize_eeg(train_data: np.ndarray, test_data: np.ndarray):\n",
    "    if train_data.ndim == 5:\n",
    "        train_flat = train_data.reshape(-1, *train_data.shape[2:])\n",
    "        test_flat  = test_data.reshape(-1, *test_data.shape[2:])\n",
    "    else:\n",
    "        train_flat, test_flat = train_data, test_data\n",
    "\n",
    "    # Compute stats on ALL training data\n",
    "    train_mean = np.mean(train_flat, axis=(0, 2, 3), keepdims=True)\n",
    "    train_std  = np.std(train_flat,  axis=(0, 2, 3), keepdims=True) + 1e-8\n",
    "\n",
    "    train_norm = (train_flat - train_mean) / train_std\n",
    "    test_norm  = (test_flat  - train_mean) / train_std\n",
    "\n",
    "    if train_data.ndim == 5:\n",
    "        train_norm = train_norm.reshape(train_data.shape)\n",
    "        test_norm  = test_norm.reshape(test_data.shape)\n",
    "\n",
    "    return (\n",
    "        train_norm.astype(np.float32),\n",
    "        test_norm.astype(np.float32),\n",
    "        train_mean,\n",
    "        train_std\n",
    "    )\n",
    "\n",
    "# Normalize ALL training data\n",
    "X_train_norm, X_test_norm, mean, std = normalize_eeg(X_train, X_test)\n",
    "\n",
    "y_train_norm = y_train.astype(np.float32)\n",
    "y_test_norm  = y_test.astype(np.float32)\n",
    "\n",
    "print(\"[STEP 6] Normalization done ✅\")\n",
    "print(\"  X_train_norm:\", X_train_norm.shape)\n",
    "print(\"  X_test_norm :\", X_test_norm.shape)\n",
    "print(\"  y_train_norm:\", y_train_norm.shape)\n",
    "print(\"  y_test_norm :\", y_test_norm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f527c-34db-4c23-bdbd-c8ce03b519f0",
   "metadata": {},
   "source": [
    "Cell 7 — EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ea4279-53c2-49c1-b834-7b1e1c20775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\DL\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Xtr: (5100, 14, 128, 1), ytr: (5100,)\n",
      "[INFO] Test / LOSO subject is NOT used here.\n",
      "[SPLIT] Train: (4080, 14, 128, 1), Val: (1020, 14, 128, 1)\n",
      "Epoch 1/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 129ms/step - accuracy: 0.4998 - loss: 0.7434 - val_accuracy: 0.5294 - val_loss: 0.6914 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.5456 - loss: 0.6860 - val_accuracy: 0.5284 - val_loss: 0.6910 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.5794 - loss: 0.6676 - val_accuracy: 0.5304 - val_loss: 0.6895 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.6135 - loss: 0.6346 - val_accuracy: 0.5451 - val_loss: 0.6860 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.6324 - loss: 0.6117 - val_accuracy: 0.5618 - val_loss: 0.6796 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.6755 - loss: 0.5851 - val_accuracy: 0.5706 - val_loss: 0.6682 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.6961 - loss: 0.5637 - val_accuracy: 0.5873 - val_loss: 0.6582 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.7025 - loss: 0.5461 - val_accuracy: 0.6000 - val_loss: 0.6522 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.7164 - loss: 0.5238 - val_accuracy: 0.6078 - val_loss: 0.6466 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.7373 - loss: 0.5109 - val_accuracy: 0.6343 - val_loss: 0.6316 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.7449 - loss: 0.4907 - val_accuracy: 0.6039 - val_loss: 0.6415 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7510 - loss: 0.4864 - val_accuracy: 0.6078 - val_loss: 0.6202 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.7608 - loss: 0.4691 - val_accuracy: 0.6833 - val_loss: 0.5864 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.7809 - loss: 0.4511 - val_accuracy: 0.6167 - val_loss: 0.6259 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7968 - loss: 0.4366 - val_accuracy: 0.6382 - val_loss: 0.6023 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8093 - loss: 0.4169 - val_accuracy: 0.6882 - val_loss: 0.5576 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.8206 - loss: 0.3897 - val_accuracy: 0.6735 - val_loss: 0.5815 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.8426 - loss: 0.3647 - val_accuracy: 0.7108 - val_loss: 0.5238 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8368 - loss: 0.3581 - val_accuracy: 0.7108 - val_loss: 0.5352 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.8485 - loss: 0.3551 - val_accuracy: 0.7127 - val_loss: 0.5450 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8527 - loss: 0.3260 - val_accuracy: 0.7294 - val_loss: 0.5310 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8576 - loss: 0.3251 - val_accuracy: 0.7373 - val_loss: 0.4700 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8681 - loss: 0.3194 - val_accuracy: 0.7980 - val_loss: 0.4328 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.8760 - loss: 0.2987 - val_accuracy: 0.7951 - val_loss: 0.4171 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.8767 - loss: 0.3081 - val_accuracy: 0.7931 - val_loss: 0.4100 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.8838 - loss: 0.2778 - val_accuracy: 0.7667 - val_loss: 0.4448 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8897 - loss: 0.2713 - val_accuracy: 0.8363 - val_loss: 0.3569 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9029 - loss: 0.2412 - val_accuracy: 0.8108 - val_loss: 0.3688 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9010 - loss: 0.2511 - val_accuracy: 0.8265 - val_loss: 0.3705 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9088 - loss: 0.2337 - val_accuracy: 0.8392 - val_loss: 0.3170 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9179 - loss: 0.2172 - val_accuracy: 0.8912 - val_loss: 0.2680 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9169 - loss: 0.2199 - val_accuracy: 0.9000 - val_loss: 0.2431 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9194 - loss: 0.2178 - val_accuracy: 0.8863 - val_loss: 0.2538 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9230 - loss: 0.1972 - val_accuracy: 0.9127 - val_loss: 0.2245 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9208 - loss: 0.2099 - val_accuracy: 0.9314 - val_loss: 0.1946 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9267 - loss: 0.2045 - val_accuracy: 0.9186 - val_loss: 0.2073 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9240 - loss: 0.2096 - val_accuracy: 0.9324 - val_loss: 0.1834 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9373 - loss: 0.1682 - val_accuracy: 0.9480 - val_loss: 0.1510 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9297 - loss: 0.1855 - val_accuracy: 0.9451 - val_loss: 0.1629 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9341 - loss: 0.1719 - val_accuracy: 0.9549 - val_loss: 0.1309 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9350 - loss: 0.1693 - val_accuracy: 0.9647 - val_loss: 0.1146 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9321 - loss: 0.1808 - val_accuracy: 0.9588 - val_loss: 0.1279 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9304 - loss: 0.1885 - val_accuracy: 0.9559 - val_loss: 0.1324 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9333 - loss: 0.1731 - val_accuracy: 0.9441 - val_loss: 0.1483 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.9353 - loss: 0.1583\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9380 - loss: 0.1624 - val_accuracy: 0.9559 - val_loss: 0.1560 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9341 - loss: 0.1663 - val_accuracy: 0.9569 - val_loss: 0.1352 - learning_rate: 5.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9385 - loss: 0.1564 - val_accuracy: 0.9569 - val_loss: 0.1209 - learning_rate: 5.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9358 - loss: 0.1694 - val_accuracy: 0.9637 - val_loss: 0.1067 - learning_rate: 5.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9360 - loss: 0.1640 - val_accuracy: 0.9686 - val_loss: 0.0978 - learning_rate: 5.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9395 - loss: 0.1592 - val_accuracy: 0.9765 - val_loss: 0.0940 - learning_rate: 5.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9419 - loss: 0.1504 - val_accuracy: 0.9696 - val_loss: 0.0965 - learning_rate: 5.0000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9434 - loss: 0.1577 - val_accuracy: 0.9451 - val_loss: 0.1756 - learning_rate: 5.0000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9417 - loss: 0.1536 - val_accuracy: 0.9578 - val_loss: 0.1206 - learning_rate: 5.0000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.9464 - loss: 0.1320\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9475 - loss: 0.1360 - val_accuracy: 0.9676 - val_loss: 0.1031 - learning_rate: 5.0000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9468 - loss: 0.1335 - val_accuracy: 0.9686 - val_loss: 0.0952 - learning_rate: 2.5000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9483 - loss: 0.1387 - val_accuracy: 0.9667 - val_loss: 0.0905 - learning_rate: 2.5000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9510 - loss: 0.1407 - val_accuracy: 0.9696 - val_loss: 0.0899 - learning_rate: 2.5000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9512 - loss: 0.1355 - val_accuracy: 0.9657 - val_loss: 0.0932 - learning_rate: 2.5000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9517 - loss: 0.1313 - val_accuracy: 0.9696 - val_loss: 0.0881 - learning_rate: 2.5000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9522 - loss: 0.1334 - val_accuracy: 0.9735 - val_loss: 0.0858 - learning_rate: 2.5000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9512 - loss: 0.1314 - val_accuracy: 0.9706 - val_loss: 0.0875 - learning_rate: 2.5000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9473 - loss: 0.1460 - val_accuracy: 0.9735 - val_loss: 0.0827 - learning_rate: 2.5000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9517 - loss: 0.1252 - val_accuracy: 0.9686 - val_loss: 0.0863 - learning_rate: 2.5000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9507 - loss: 0.1329 - val_accuracy: 0.9735 - val_loss: 0.0798 - learning_rate: 2.5000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9537 - loss: 0.1281 - val_accuracy: 0.9686 - val_loss: 0.0828 - learning_rate: 2.5000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9493 - loss: 0.1333 - val_accuracy: 0.9784 - val_loss: 0.0799 - learning_rate: 2.5000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9466 - loss: 0.1342 - val_accuracy: 0.9745 - val_loss: 0.0801 - learning_rate: 2.5000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9489 - loss: 0.1277\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9453 - loss: 0.1398 - val_accuracy: 0.9765 - val_loss: 0.0827 - learning_rate: 2.5000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9512 - loss: 0.1297 - val_accuracy: 0.9696 - val_loss: 0.0818 - learning_rate: 1.2500e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9525 - loss: 0.1307 - val_accuracy: 0.9755 - val_loss: 0.0800 - learning_rate: 1.2500e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9507 - loss: 0.1267 - val_accuracy: 0.9784 - val_loss: 0.0771 - learning_rate: 1.2500e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9480 - loss: 0.1280 - val_accuracy: 0.9755 - val_loss: 0.0769 - learning_rate: 1.2500e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9571 - loss: 0.1178 - val_accuracy: 0.9755 - val_loss: 0.0754 - learning_rate: 1.2500e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9534 - loss: 0.1311 - val_accuracy: 0.9775 - val_loss: 0.0766 - learning_rate: 1.2500e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9463 - loss: 0.1413 - val_accuracy: 0.9745 - val_loss: 0.0780 - learning_rate: 1.2500e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9537 - loss: 0.1319 - val_accuracy: 0.9755 - val_loss: 0.0742 - learning_rate: 1.2500e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9564 - loss: 0.1197 - val_accuracy: 0.9765 - val_loss: 0.0769 - learning_rate: 1.2500e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9542 - loss: 0.1241 - val_accuracy: 0.9794 - val_loss: 0.0763 - learning_rate: 1.2500e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9569 - loss: 0.1184 - val_accuracy: 0.9696 - val_loss: 0.0800 - learning_rate: 1.2500e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9563 - loss: 0.1111\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9534 - loss: 0.1224 - val_accuracy: 0.9755 - val_loss: 0.0755 - learning_rate: 1.2500e-04\n",
      "Epoch 81/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9551 - loss: 0.1255 - val_accuracy: 0.9765 - val_loss: 0.0733 - learning_rate: 6.2500e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9569 - loss: 0.1162 - val_accuracy: 0.9784 - val_loss: 0.0728 - learning_rate: 6.2500e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9576 - loss: 0.1205 - val_accuracy: 0.9745 - val_loss: 0.0759 - learning_rate: 6.2500e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9561 - loss: 0.1261 - val_accuracy: 0.9755 - val_loss: 0.0753 - learning_rate: 6.2500e-05\n",
      "Epoch 85/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9517 - loss: 0.1319 - val_accuracy: 0.9775 - val_loss: 0.0738 - learning_rate: 6.2500e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9603 - loss: 0.1218\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9549 - loss: 0.1311 - val_accuracy: 0.9725 - val_loss: 0.0755 - learning_rate: 6.2500e-05\n",
      "Epoch 87/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.9527 - loss: 0.1255 - val_accuracy: 0.9745 - val_loss: 0.0748 - learning_rate: 3.1250e-05\n",
      "Epoch 88/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9542 - loss: 0.1250 - val_accuracy: 0.9765 - val_loss: 0.0733 - learning_rate: 3.1250e-05\n",
      "Epoch 89/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9608 - loss: 0.1122 - val_accuracy: 0.9745 - val_loss: 0.0748 - learning_rate: 3.1250e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9532 - loss: 0.1155\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.9556 - loss: 0.1137 - val_accuracy: 0.9735 - val_loss: 0.0751 - learning_rate: 3.1250e-05\n",
      "Epoch 91/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 131ms/step - accuracy: 0.9547 - loss: 0.1262 - val_accuracy: 0.9735 - val_loss: 0.0747 - learning_rate: 1.5625e-05\n",
      "Epoch 92/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.9515 - loss: 0.1199 - val_accuracy: 0.9755 - val_loss: 0.0742 - learning_rate: 1.5625e-05\n",
      "Epoch 93/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 129ms/step - accuracy: 0.9556 - loss: 0.1162 - val_accuracy: 0.9745 - val_loss: 0.0745 - learning_rate: 1.5625e-05\n",
      "Epoch 94/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.9540 - loss: 0.1183\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 174ms/step - accuracy: 0.9532 - loss: 0.1264 - val_accuracy: 0.9755 - val_loss: 0.0740 - learning_rate: 1.5625e-05\n",
      "Epoch 95/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.9544 - loss: 0.1229 - val_accuracy: 0.9765 - val_loss: 0.0736 - learning_rate: 7.8125e-06\n",
      "Epoch 96/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 142ms/step - accuracy: 0.9549 - loss: 0.1157 - val_accuracy: 0.9765 - val_loss: 0.0735 - learning_rate: 7.8125e-06\n",
      "Epoch 97/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 138ms/step - accuracy: 0.9547 - loss: 0.1180 - val_accuracy: 0.9765 - val_loss: 0.0734 - learning_rate: 7.8125e-06\n",
      "Epoch 97: early stopping\n",
      "Restoring model weights from the end of the best epoch: 82.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] Model saved as cole-final.h5\n",
      "\n",
      "[CONFUSION MATRIX]\n",
      "[[536   4]\n",
      " [ 18 462]]\n",
      "[VAL] acc=0.9784, prec=0.9914, rec=0.9625, f1=0.9767\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 4: 80:20 Train / Validation Split (EEGNet)\n",
    "# ====================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, SeparableConv2D, BatchNormalization, Activation,\n",
    "    DepthwiseConv2D, Conv2D, AveragePooling2D,\n",
    "    Dropout, Dense, Flatten\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# --------------------------------------------------------------------\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# EEGNet model\n",
    "# --------------------------------------------------------------------\n",
    "def create_eegnet_model(input_shape, dropout_rate=0.5, num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(16, (1, 64), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = DepthwiseConv2D(\n",
    "        (input_shape[0], 1),\n",
    "        depth_multiplier=2,\n",
    "        padding='valid',\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 4))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = SeparableConv2D(16, (1, 16), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 8))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Compile model\n",
    "# --------------------------------------------------------------------\n",
    "def compile_model():\n",
    "    input_shape = (Xtr.shape[1], Xtr.shape[2], Xtr.shape[3])\n",
    "    model = create_eegnet_model(input_shape)\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-3),\n",
    "        loss=BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[BinaryAccuracy(name=\"accuracy\", threshold=0.5)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def make_callbacks():\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\",\n",
    "            factor=0.5,\n",
    "            patience=4,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Use ALL normalized training data\n",
    "# --------------------------------------------------------------------\n",
    "Xtr = X_train_norm\n",
    "ytr = y_train_norm\n",
    "\n",
    "print(f\"[TRAIN] Xtr: {Xtr.shape}, ytr: {ytr.shape}\")\n",
    "print(\"[INFO] Test / LOSO subject is NOT used here.\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 80:20 Stratified Split\n",
    "# --------------------------------------------------------------------\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(sss.split(Xtr, ytr))\n",
    "\n",
    "X_train, X_val = Xtr[train_idx], Xtr[val_idx]\n",
    "y_train, y_val = ytr[train_idx], ytr[val_idx]\n",
    "\n",
    "print(f\"[SPLIT] Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Train model\n",
    "# --------------------------------------------------------------------\n",
    "model = compile_model()\n",
    "callbacks = make_callbacks()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Save final model\n",
    "# --------------------------------------------------------------------\n",
    "model.save(\"jack-final.h5\")\n",
    "print(\"[SAVE] Model saved as cole-final.h5\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Validation Confusion Matrix\n",
    "# --------------------------------------------------------------------\n",
    "y_prob = model.predict(X_val, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "y_true = y_val.astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "eps = 1e-12\n",
    "\n",
    "acc  = (tp + tn) / max(tp + tn + fp + fn, eps)\n",
    "prec = tp / max(tp + fp, eps)\n",
    "rec  = tp / max(tp + fn, eps)\n",
    "f1   = 2 * prec * rec / max(prec + rec, eps)\n",
    "\n",
    "print(\"\\n[CONFUSION MATRIX]\")\n",
    "print(cm)\n",
    "print(f\"[VAL] acc={acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e2a13-6367-41ca-be66-a43594c2940e",
   "metadata": {},
   "source": [
    "Cell 8 — Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ac6662-33c1-4e25-b6c5-d47c860fba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Preparing TEST data...\n",
      "[TEST] Xte shape: (300, 14, 128, 1)\n",
      "[TEST] yte shape: (300,), class distribution: (array([1]), array([300]))\n",
      "[TEST] Loading final model: jack-final.h5\n",
      "[TEST] Running inference on TEST subject...\n",
      "[TEST] Mean predicted probability: 0.4395\n",
      "\n",
      "[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[  0   0]\n",
      " [171 129]]\n",
      "Accuracy : 0.4300\n",
      "Precision: 1.0000\n",
      "Recall   : 0.4300\n",
      "F1-score : 0.6014\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Inference on TEST subject (adam) using FINAL EEGNet\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Prepare TEST data\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Preparing TEST data...\")\n",
    "\n",
    "Xte = X_test_norm\n",
    "yte = y_test_norm.astype(int)\n",
    "\n",
    "print(f\"[TEST] Xte shape: {Xte.shape}\")\n",
    "print(f\"[TEST] yte shape: {yte.shape}, class distribution: {np.unique(yte, return_counts=True)}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Load FINAL trained model\n",
    "# ------------------------------------------------------------------\n",
    "model_path = \"jack-final.h5\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        \"Final model 'cole-final.h5' not found. \"\n",
    "        \"Please run Cell 4 (80:20 training) first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[TEST] Loading final model: {model_path}\")\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Run inference on TEST subject\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Running inference on TEST subject...\")\n",
    "\n",
    "y_prob = model.predict(Xte, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(f\"[TEST] Mean predicted probability: {y_prob.mean():.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Metrics on TEST subject (segment-level)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc = accuracy_score(yte, y_pred)\n",
    "prec = precision_score(yte, y_pred, zero_division=0)\n",
    "rec = recall_score(yte, y_pred, zero_division=0)\n",
    "f1 = f1_score(yte, y_pred, zero_division=0)\n",
    "\n",
    "print(\"\\n[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572dbf3e-bbb5-4865-883f-30e374700949",
   "metadata": {},
   "source": [
    "Cell 9 - Performance Matrices for each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a57240-50ea-4736-a7ac-52ef7878b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] Verifying predictions and labels are available...\n",
      "[CHECK] Found predictions for 300 test segments.\n",
      "\n",
      "[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[  0   0]\n",
      " [171 129]]\n",
      "Accuracy : 0.4300\n",
      "Precision: 1.0000\n",
      "Recall   : 0.4300\n",
      "F1-score : 0.6014\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Final model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================\n",
    "# Cell 6: Confusion Matrix & Metrics on TEST (FINAL EEGNet)\n",
    "# =======================================================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# ------------------------------------------------------------------\n",
    "def compute_metrics_from_cm(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    eps = 1e-12\n",
    "    acc  = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec  = tp / (tp + fn + eps)\n",
    "    f1   = 2 * prec * rec / (prec + rec + eps)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], ['0', '1'])\n",
    "    plt.yticks([0, 1], ['0', '1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(\n",
    "                j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Validate upstream artifacts\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[CHECK] Verifying predictions and labels are available...\")\n",
    "\n",
    "if 'y_pred' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Predictions 'y_pred' not found. \"\n",
    "        \"Please run Cell 5 (final model inference) first.\"\n",
    "    )\n",
    "\n",
    "if 'yte' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Test labels 'yte' not found. \"\n",
    "        \"Please run Cell 5 first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[CHECK] Found predictions for {len(y_pred)} test segments.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Confusion Matrix & Metrics (FINAL model)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc, prec, rec, f1 = compute_metrics_from_cm(cm)\n",
    "\n",
    "print(\"\\n[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(cm, title=\"Confusion Matrix - FINAL EEGNet\")\n",
    "\n",
    "print(\"[DONE] Final model evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0f120-39fd-497e-9098-05e6c82f4807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
