{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af5b471-cea9-4194-bc23-da5cdc2f9c67",
   "metadata": {},
   "source": [
    "Cell 1 — Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64aaa897-ab61-4eb4-a558-4c078acf751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Base path set to:\n",
      "C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\n",
      "[INFO] Found 18 folders:\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Selected test subject (LOSO): Max\n",
      "[INFO] Training subjects (17):\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Sampling rate (SFREQ) = 128.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 1: Setup & discovery\n",
    "# =========================\n",
    "\n",
    "import os  # file ops\n",
    "import glob  # file matching\n",
    "import numpy as np  # arrays\n",
    "import random  # seeds\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data path\n",
    "BASE_PATH = r\"C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\"\n",
    "print(f\"[INFO] Base path set to:\\n{BASE_PATH}\")\n",
    "\n",
    "# List subject folders\n",
    "all_folders = [p for p in sorted(os.listdir(BASE_PATH)) if os.path.isdir(os.path.join(BASE_PATH, p))]\n",
    "print(f\"[INFO] Found {len(all_folders)} folders:\\n{all_folders}\")\n",
    "\n",
    "# Expected count (change if needed)\n",
    "assert len(all_folders) == 18, f\"Expected 18 folders, found {len(all_folders)}.\"\n",
    "\n",
    "# LOSO test subject\n",
    "TEST_SUBJECT = \"Max\"\n",
    "print(f\"[INFO] Selected test subject (LOSO): {TEST_SUBJECT}\")\n",
    "\n",
    "# Train subjects = remaining\n",
    "train_subjects = [s for s in all_folders if s.lower() != TEST_SUBJECT.lower()]\n",
    "print(f\"[INFO] Training subjects ({len(train_subjects)}):\\n{train_subjects}\")\n",
    "\n",
    "# Sampling rate\n",
    "SFREQ = 128.0\n",
    "print(f\"[INFO] Sampling rate (SFREQ) = {SFREQ} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e649681-2861-4ce2-bbd1-0d9dd3359f42",
   "metadata": {},
   "source": [
    "Cell 2 — Load EEG + labels per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943c5c6e-a7aa-4618-899c-daf341c25358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 2] Loading data from folders...\n",
      "[STEP 2]       Amin | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Amin | EEG=          Amin.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Amin1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]      Amin1 | EEG=         Amin1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Cole | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Cole | EEG=          Cole.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Daniel | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]     Daniel | EEG=        Daniel.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]    Ismayil | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]    Ismayil | EEG=       Ismayil.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Jack | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Jack | EEG=          Jack.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]      James | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      James | EEG=         James.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Josh | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Josh | EEG=          Josh.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Marjan | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Marjan | EEG=        Marjan.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]        Max | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Max | EEG=           Max.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Mina | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mina | EEG=          Mina.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 1 | EEG=        Mina 1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 3 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 3 | EEG=        Mina 3.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]   Mohammad | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]   Mohammad | EEG=      Mohammad.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Mona | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mona | EEG=          Mona.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Roddy | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      Roddy | EEG=         Roddy.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]        Sam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Sam | EEG=           Sam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       adam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       adam | EEG=          adam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2] Loading complete ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 2: Load data (EMPTY LABEL => CLASS 0)\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path  # helper\n",
    "\n",
    "def load_subject(subj_dir: str, subj_name: str, expected_channels: int = 14):\n",
    "    \"\"\"\n",
    "    EEG file: {subj_name}.npy\n",
    "    Label file: {subj_name}_target.npy\n",
    "    Label rule:\n",
    "      - empty array -> class 0\n",
    "      - [0] -> class 0\n",
    "      - [1] -> class 1\n",
    "    \"\"\"\n",
    "\n",
    "    # EEG file path\n",
    "    eeg_file = os.path.join(subj_dir, f\"{subj_name}.npy\")\n",
    "    assert os.path.exists(eeg_file), f\"[ERROR] Missing EEG file: {eeg_file}\"\n",
    "\n",
    "    # Label file path\n",
    "    target_file = os.path.join(subj_dir, f\"{subj_name}_target.npy\")\n",
    "    assert os.path.exists(target_file), f\"[ERROR] Missing label file: {target_file}\"\n",
    "\n",
    "    # Load EEG\n",
    "    X_raw = np.load(eeg_file, allow_pickle=True)\n",
    "    X_raw = np.asarray(X_raw)\n",
    "\n",
    "    # Standardize EEG to (n_trials, 14, T)\n",
    "    if X_raw.ndim == 2:\n",
    "        X_trials = X_raw[None, :, :]\n",
    "    elif X_raw.ndim == 3:\n",
    "        dims = list(X_raw.shape)\n",
    "        chan_axes = [i for i, d in enumerate(dims) if d == expected_channels]\n",
    "        chan_axis = chan_axes[0] if len(chan_axes) == 1 else 1\n",
    "        time_axis = int(np.argmax(dims))\n",
    "        trial_axis = [i for i in range(3) if i not in (chan_axis, time_axis)][0]\n",
    "        X_trials = np.moveaxis(X_raw, (trial_axis, chan_axis, time_axis), (0, 1, 2))\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] Unsupported EEG shape {X_raw.shape} in {eeg_file}\")\n",
    "\n",
    "    assert X_trials.shape[1] == expected_channels, f\"[ERROR] Expected {expected_channels} channels, got {X_trials.shape[1]} in {eeg_file}\"\n",
    "\n",
    "    # Load label\n",
    "    y_raw = np.load(target_file, allow_pickle=True)\n",
    "    y_arr = np.asarray(y_raw).ravel()\n",
    "\n",
    "    # Apply your rule\n",
    "    if y_arr.size == 0:\n",
    "        label = 0\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\")\n",
    "    else:\n",
    "        label = int(y_arr[0])\n",
    "        if label not in (0, 1):\n",
    "            raise ValueError(f\"[ERROR] Invalid label {label} in {target_file} (must be 0 or 1)\")\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape={y_arr.shape}, value={y_arr} -> class={label}\")\n",
    "\n",
    "    # Repeat label for all trials\n",
    "    y_trials = np.full((X_trials.shape[0],), label, dtype=np.int32)\n",
    "\n",
    "    return X_trials.astype(np.float32), y_trials.astype(np.int32), eeg_file, target_file\n",
    "\n",
    "\n",
    "# Load all subjects\n",
    "subject_data = {}\n",
    "print(\"[STEP 2] Loading data from folders...\")\n",
    "\n",
    "for subj in all_folders:\n",
    "    subj_dir = os.path.join(BASE_PATH, subj)\n",
    "    X_trials, y_trials, eeg_file, target_file = load_subject(subj_dir, subj)\n",
    "\n",
    "    subject_data[subj] = (X_trials, y_trials)\n",
    "\n",
    "    print(f\"[STEP 2] {subj:>10} | EEG={Path(eeg_file).name:>18} | X={X_trials.shape} | y={y_trials.shape} | class={y_trials[0]}\")\n",
    "\n",
    "print(\"[STEP 2] Loading complete ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd93ca8-a94c-485c-8a61-e6e7441f6299",
   "metadata": {},
   "source": [
    "Cell 3 — Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733e592c-98e1-4cbd-afd2-6fdc8cf407f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 3] Preprocessing functions ready ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3: Preprocessing functions\n",
    "# =========================\n",
    "\n",
    "import mne  # EEG processing\n",
    "import pywt  # wavelets\n",
    "from sklearn.decomposition import FastICA  # ICA\n",
    "from typing import Optional, Tuple, Union, Sequence, Dict, List  # typing\n",
    "\n",
    "def wavelet_enhanced_ica(\n",
    "    data: np.ndarray,\n",
    "    n_components: int = 10,\n",
    "    wavelet: str = \"db4\",\n",
    "    level: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> np.ndarray:\n",
    "    n_ch, n_t = data.shape\n",
    "    n_components = min(n_components, n_ch)\n",
    "\n",
    "    coeffs = pywt.wavedec(data, wavelet=wavelet, level=level, axis=1)\n",
    "    A = coeffs[0]\n",
    "\n",
    "    ica = FastICA(n_components=n_components, random_state=random_state)\n",
    "    S = ica.fit_transform(A.T).T\n",
    "    A_denoised = ica.inverse_transform(S.T).T\n",
    "\n",
    "    coeffs[0] = A_denoised\n",
    "    cleaned = pywt.waverec(coeffs, wavelet=wavelet, axis=1)\n",
    "\n",
    "    if cleaned.shape[1] != n_t:\n",
    "        if cleaned.shape[1] > n_t:\n",
    "            cleaned = cleaned[:, :n_t]\n",
    "        else:\n",
    "            cleaned = np.pad(cleaned, ((0, 0), (0, n_t - cleaned.shape[1])), mode=\"constant\")\n",
    "    return cleaned\n",
    "\n",
    "def _names_from_index_mapping(n_channels: int, index_to_name: Optional[Dict[int, str]]) -> List[str]:\n",
    "    if index_to_name is None:\n",
    "        return [f\"EEG{i+1}\" for i in range(n_channels)]\n",
    "    keys = list(index_to_name.keys())\n",
    "    is_zero_based = (0 in keys) and (1 not in keys)\n",
    "    names = []\n",
    "    for i in range(n_channels):\n",
    "        key = i if is_zero_based else (i + 1)\n",
    "        names.append(index_to_name.get(key, f\"EEG{i+1}\"))\n",
    "    return names\n",
    "\n",
    "def preprocess_eeg(\n",
    "    eeg: np.ndarray,\n",
    "    sfreq: float,\n",
    "    *,\n",
    "    index_to_name: Optional[Dict[int, str]] = None,\n",
    "    use_standard_1010: bool = True,\n",
    "    resample_to: Optional[float] = None,\n",
    "    notch_freqs: Union[None, float, Sequence[float]] = 50.0,\n",
    "    highpass: Optional[float] = 0.05,\n",
    "    bad_point_z: float = 6.0,\n",
    "    bad_channel_z: float = 5.0,\n",
    "    interpolate_bad_channels: bool = True,\n",
    "    car: bool = True,\n",
    "    use_wica: bool = True,\n",
    "    wica_components: int = 10,\n",
    "    wica_wavelet: str = \"db4\",\n",
    "    wica_level: int = 3,\n",
    "    wica_random_state: int = 42,\n",
    "    return_raw: bool = False,\n",
    "):\n",
    "\n",
    "    eeg = np.asarray(eeg, dtype=np.float32)\n",
    "    assert eeg.ndim == 2, \"eeg must be 2D: (n_channels, n_times)\"\n",
    "\n",
    "    n_channels, _ = eeg.shape\n",
    "    ch_names = _names_from_index_mapping(n_channels, index_to_name)\n",
    "    ch_types = ['eog' if str(n).upper().startswith(\"EOG\") else 'eeg' for n in ch_names]\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    raw = mne.io.RawArray(eeg, info, verbose=False)\n",
    "\n",
    "    montage_applied = False\n",
    "    if use_standard_1010:\n",
    "        try:\n",
    "            mont = mne.channels.make_standard_montage(\"standard_1010\")\n",
    "            raw.set_montage(mont, match_case=False, on_missing=\"ignore\")\n",
    "            montage_applied = True\n",
    "        except Exception:\n",
    "            montage_applied = False\n",
    "\n",
    "    if resample_to is not None and float(resample_to) != float(sfreq):\n",
    "        raw.resample(sfreq=resample_to, npad=\"auto\")\n",
    "    sfreq_out = float(raw.info[\"sfreq\"])\n",
    "\n",
    "    if notch_freqs is not None:\n",
    "        raw.notch_filter(freqs=notch_freqs, verbose=False)\n",
    "    if highpass is not None:\n",
    "        raw.filter(l_freq=highpass, h_freq=None, verbose=False)\n",
    "\n",
    "    X = raw.get_data()\n",
    "    mu = np.mean(X, axis=1, keepdims=True)\n",
    "    sd = np.std(X, axis=1, keepdims=True) + 1e-12\n",
    "    hi = mu + bad_point_z * sd\n",
    "    lo = mu - bad_point_z * sd\n",
    "    bad_idx = (X > hi) | (X < lo)\n",
    "\n",
    "    if np.any(bad_idx):\n",
    "        X_fixed = X.copy()\n",
    "        t = np.arange(X.shape[1], dtype=float)\n",
    "        for ch in range(n_channels):\n",
    "            mask = bad_idx[ch]\n",
    "            if mask.any():\n",
    "                good = ~mask\n",
    "                if good.sum() >= 2:\n",
    "                    X_fixed[ch, mask] = np.interp(t[mask], t[good], X_fixed[ch, good])\n",
    "        raw._data = X_fixed\n",
    "\n",
    "    if interpolate_bad_channels and montage_applied:\n",
    "        X = raw.get_data(picks=\"eeg\")\n",
    "        if X.size > 0:\n",
    "            ch_std = X.std(axis=1)\n",
    "            med = np.median(ch_std)\n",
    "            mad = np.median(np.abs(ch_std - med)) + 1e-12\n",
    "            z = 0.6745 * (ch_std - med) / mad\n",
    "            eeg_names = mne.pick_info(raw.info, mne.pick_types(raw.info, eeg=True)).ch_names\n",
    "            bads = [eeg_names[i] for i in np.where(np.abs(z) > bad_channel_z)[0]]\n",
    "            raw.info[\"bads\"] = bads\n",
    "            if bads:\n",
    "                raw.interpolate_bads(reset_bads=True, verbose=False)\n",
    "\n",
    "    if car:\n",
    "        raw.set_eeg_reference(\"average\", projection=True)\n",
    "        raw.apply_proj()\n",
    "\n",
    "    if use_wica:\n",
    "        cleaned = wavelet_enhanced_ica(\n",
    "            raw.get_data(),\n",
    "            n_components=wica_components,\n",
    "            wavelet=wica_wavelet,\n",
    "            level=wica_level,\n",
    "            random_state=wica_random_state\n",
    "        )\n",
    "        raw = mne.io.RawArray(cleaned, raw.info, verbose=False)\n",
    "\n",
    "    cleaned = raw.get_data()\n",
    "    return cleaned, sfreq_out\n",
    "\n",
    "print(\"[STEP 3] Preprocessing functions ready ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a9d20-889d-45c4-bbc2-de62216908cd",
   "metadata": {},
   "source": [
    "Cell 4 — Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bfcbde0-1cc2-409c-8627-93da46606661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 4] Starting preprocessing...\n",
      "[INFO] USE_WICA = True\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 01/17: Amin | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 02/17: Amin1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 03/17: Cole | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 04/17: Daniel | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 05/17: Ismayil | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 06/17: Jack | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 07/17: James | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 08/17: Josh | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 09/17: Marjan | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 10/17: Mina | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 11/17: Mina 1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 12/17: Mina 3 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 13/17: Mohammad | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 14/17: Mona | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 15/17: Roddy | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 16/17: Sam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 17/17: adam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Test subject: Max | trials=1 | fs_out=128.0\n",
      "[STEP 4] Preprocessing done ✅\n",
      "  X_train_trials_all: (17, 14, 38400)\n",
      "  y_train_trials_all: (17,) (array([0, 1], dtype=int32), array([9, 8]))\n",
      "  X_test_trials_all : (1, 14, 38400)\n",
      "  y_test_trials_all : (1,) (array([1], dtype=int32), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 4: Apply preprocessing (LOSO)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 4] Starting preprocessing...\")\n",
    "\n",
    "USE_WICA = True\n",
    "print(f\"[INFO] USE_WICA = {USE_WICA}\")\n",
    "\n",
    "X_train_trials_all, y_train_trials_all = [], []\n",
    "X_test_trials_all, y_test_trials_all = [], []\n",
    "\n",
    "def preprocess_trials(X_trials, y_trials):\n",
    "    cleaned_list = []\n",
    "    label_list = []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        trial = X_trials[i]\n",
    "        label = int(y_trials[i])\n",
    "\n",
    "        cleaned, fs_out = preprocess_eeg(\n",
    "            eeg=trial,\n",
    "            sfreq=SFREQ,\n",
    "            use_wica=USE_WICA\n",
    "        )\n",
    "\n",
    "        cleaned_list.append(cleaned.astype(np.float32, copy=False))\n",
    "        label_list.append(label)\n",
    "\n",
    "    return cleaned_list, np.array(label_list, dtype=np.int32), fs_out\n",
    "\n",
    "# Train subjects\n",
    "for idx, subj in enumerate(train_subjects, start=1):\n",
    "    X_trials, y_trials = subject_data[subj]\n",
    "    cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "    X_train_trials_all.extend(cleaned_trials)\n",
    "    y_train_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "    print(f\"[STEP 4] Train {idx:02d}/{len(train_subjects)}: {subj} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Test subject\n",
    "X_trials, y_trials = subject_data[TEST_SUBJECT]\n",
    "cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "X_test_trials_all.extend(cleaned_trials)\n",
    "y_test_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "print(f\"[STEP 4] Test subject: {TEST_SUBJECT} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Convert to arrays\n",
    "X_train_trials_all = np.array(X_train_trials_all, dtype=np.float32)\n",
    "y_train_trials_all = np.array(y_train_trials_all, dtype=np.int32)\n",
    "\n",
    "X_test_trials_all = np.array(X_test_trials_all, dtype=np.float32)\n",
    "y_test_trials_all = np.array(y_test_trials_all, dtype=np.int32)\n",
    "\n",
    "print(\"[STEP 4] Preprocessing done ✅\")\n",
    "print(\"  X_train_trials_all:\", X_train_trials_all.shape)\n",
    "print(\"  y_train_trials_all:\", y_train_trials_all.shape, np.unique(y_train_trials_all, return_counts=True))\n",
    "print(\"  X_test_trials_all :\", X_test_trials_all.shape)\n",
    "print(\"  y_test_trials_all :\", y_test_trials_all.shape, np.unique(y_test_trials_all, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc3da1-2967-4ee3-84cd-bfcf0f98c0b1",
   "metadata": {},
   "source": [
    "Cell 5 — Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c21ad54-7001-4039-9360-29dfa2905127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 5] Starting segmentation...\n",
      "[STEP 5] Segmented 5/17 trials...\n",
      "[STEP 5] Segmented 10/17 trials...\n",
      "[STEP 5] Segmented 15/17 trials...\n",
      "[STEP 5] Segmented 17/17 trials...\n",
      "[STEP 5] Segmented 1/1 trials...\n",
      "[STEP 5] Segmentation done ✅\n",
      "  X_train: (5100, 14, 128, 1)\n",
      "  y_train: (5100,) (array([0, 1], dtype=int32), array([2700, 2400]))\n",
      "  X_test : (300, 14, 128, 1)\n",
      "  y_test : (300,) (array([1], dtype=int32), array([300]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 5: Segmentation\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 5] Starting segmentation...\")\n",
    "\n",
    "def segment_trial(trial_2d: np.ndarray, label: int, segment_len: int = 128):\n",
    "    C, T = trial_2d.shape\n",
    "    n_segs = T // segment_len\n",
    "    use_T = n_segs * segment_len\n",
    "\n",
    "    if n_segs == 0:\n",
    "        return np.empty((0, C, segment_len), dtype=np.float32), np.empty((0,), dtype=np.int32)\n",
    "\n",
    "    trial_trim = trial_2d[:, :use_T]\n",
    "    Xsegs = trial_trim.reshape(C, n_segs, segment_len).transpose(1, 0, 2)\n",
    "    ysegs = np.full((n_segs,), int(label), dtype=np.int32)\n",
    "    return Xsegs.astype(np.float32, copy=False), ysegs\n",
    "\n",
    "def segment_dataset(X_trials: np.ndarray, y_trials: np.ndarray, segment_len: int = 128):\n",
    "    X_all, y_all = [], []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        Xsegs, ysegs = segment_trial(X_trials[i], int(y_trials[i]), segment_len=segment_len)\n",
    "        X_all.append(Xsegs)\n",
    "        y_all.append(ysegs)\n",
    "\n",
    "        if (i + 1) % 5 == 0 or (i + 1) == X_trials.shape[0]:\n",
    "            print(f\"[STEP 5] Segmented {i+1}/{X_trials.shape[0]} trials...\")\n",
    "\n",
    "    X_out = np.concatenate(X_all, axis=0)\n",
    "    y_out = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    X_out = np.expand_dims(X_out, axis=-1)  # (N,14,128,1)\n",
    "    return X_out.astype(np.float32), y_out.astype(np.int32)\n",
    "\n",
    "X_train, y_train = segment_dataset(X_train_trials_all, y_train_trials_all, segment_len=128)\n",
    "X_test,  y_test  = segment_dataset(X_test_trials_all,  y_test_trials_all,  segment_len=128)\n",
    "\n",
    "print(\"[STEP 5] Segmentation done ✅\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  y_train:\", y_train.shape, np.unique(y_train, return_counts=True))\n",
    "print(\"  X_test :\", X_test.shape)\n",
    "print(\"  y_test :\", y_test.shape, np.unique(y_test, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b4ffd5-bf2f-490e-8854-0a2fc747f1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e69ba7-3952-47b1-bd80-f05237b2e018",
   "metadata": {},
   "source": [
    "Cell 6 — Split train/val + Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97fda4ae-e1c8-4fd1-bf0c-c82ae2a85764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 6] Normalizing using ALL training data (no train/val split)...\n",
      "[STEP 6] Normalization done ✅\n",
      "  X_train_norm: (5100, 14, 128, 1)\n",
      "  X_test_norm : (300, 14, 128, 1)\n",
      "  y_train_norm: (5100,)\n",
      "  y_test_norm : (300,)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 6: Normalize (ALL training data)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 6] Normalizing using ALL training data (no train/val split)...\")\n",
    "\n",
    "def normalize_eeg(train_data: np.ndarray, test_data: np.ndarray):\n",
    "    if train_data.ndim == 5:\n",
    "        train_flat = train_data.reshape(-1, *train_data.shape[2:])\n",
    "        test_flat  = test_data.reshape(-1, *test_data.shape[2:])\n",
    "    else:\n",
    "        train_flat, test_flat = train_data, test_data\n",
    "\n",
    "    # Compute stats on ALL training data\n",
    "    train_mean = np.mean(train_flat, axis=(0, 2, 3), keepdims=True)\n",
    "    train_std  = np.std(train_flat,  axis=(0, 2, 3), keepdims=True) + 1e-8\n",
    "\n",
    "    train_norm = (train_flat - train_mean) / train_std\n",
    "    test_norm  = (test_flat  - train_mean) / train_std\n",
    "\n",
    "    if train_data.ndim == 5:\n",
    "        train_norm = train_norm.reshape(train_data.shape)\n",
    "        test_norm  = test_norm.reshape(test_data.shape)\n",
    "\n",
    "    return (\n",
    "        train_norm.astype(np.float32),\n",
    "        test_norm.astype(np.float32),\n",
    "        train_mean,\n",
    "        train_std\n",
    "    )\n",
    "\n",
    "# Normalize ALL training data\n",
    "X_train_norm, X_test_norm, mean, std = normalize_eeg(X_train, X_test)\n",
    "\n",
    "y_train_norm = y_train.astype(np.float32)\n",
    "y_test_norm  = y_test.astype(np.float32)\n",
    "\n",
    "print(\"[STEP 6] Normalization done ✅\")\n",
    "print(\"  X_train_norm:\", X_train_norm.shape)\n",
    "print(\"  X_test_norm :\", X_test_norm.shape)\n",
    "print(\"  y_train_norm:\", y_train_norm.shape)\n",
    "print(\"  y_test_norm :\", y_test_norm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f527c-34db-4c23-bdbd-c8ce03b519f0",
   "metadata": {},
   "source": [
    "Cell 7 — EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ea4279-53c2-49c1-b834-7b1e1c20775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\DL\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Xtr: (5100, 14, 128, 1), ytr: (5100,)\n",
      "[INFO] Test / LOSO subject is NOT used here.\n",
      "[SPLIT] Train: (4080, 14, 128, 1), Val: (1020, 14, 128, 1)\n",
      "Epoch 1/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 131ms/step - accuracy: 0.4956 - loss: 0.7390 - val_accuracy: 0.5304 - val_loss: 0.6914 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.5586 - loss: 0.6851 - val_accuracy: 0.5294 - val_loss: 0.6912 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.5946 - loss: 0.6653 - val_accuracy: 0.5314 - val_loss: 0.6908 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.6299 - loss: 0.6360 - val_accuracy: 0.5314 - val_loss: 0.6889 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.6502 - loss: 0.6069 - val_accuracy: 0.5480 - val_loss: 0.6829 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 146ms/step - accuracy: 0.6819 - loss: 0.5831 - val_accuracy: 0.5588 - val_loss: 0.6723 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 137ms/step - accuracy: 0.7125 - loss: 0.5510 - val_accuracy: 0.5863 - val_loss: 0.6536 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.7350 - loss: 0.5252 - val_accuracy: 0.6049 - val_loss: 0.6322 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.7390 - loss: 0.5085 - val_accuracy: 0.6147 - val_loss: 0.6259 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.7498 - loss: 0.4961 - val_accuracy: 0.6618 - val_loss: 0.5954 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.7593 - loss: 0.4864 - val_accuracy: 0.6951 - val_loss: 0.5771 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.7848 - loss: 0.4597 - val_accuracy: 0.6775 - val_loss: 0.5749 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.7961 - loss: 0.4386 - val_accuracy: 0.7049 - val_loss: 0.5547 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 135ms/step - accuracy: 0.7978 - loss: 0.4369 - val_accuracy: 0.6843 - val_loss: 0.5627 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 128ms/step - accuracy: 0.7980 - loss: 0.4175 - val_accuracy: 0.7294 - val_loss: 0.5366 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 138ms/step - accuracy: 0.8140 - loss: 0.4120 - val_accuracy: 0.7333 - val_loss: 0.5126 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 127ms/step - accuracy: 0.8299 - loss: 0.3774 - val_accuracy: 0.7706 - val_loss: 0.4772 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 125ms/step - accuracy: 0.8422 - loss: 0.3655 - val_accuracy: 0.7696 - val_loss: 0.4843 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 127ms/step - accuracy: 0.8395 - loss: 0.3626 - val_accuracy: 0.7843 - val_loss: 0.4808 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.8429 - loss: 0.3603 - val_accuracy: 0.8186 - val_loss: 0.4003 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.8593 - loss: 0.3361 - val_accuracy: 0.7990 - val_loss: 0.4205 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.8588 - loss: 0.3298 - val_accuracy: 0.8422 - val_loss: 0.3809 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.8654 - loss: 0.3165 - val_accuracy: 0.8480 - val_loss: 0.3497 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.8632 - loss: 0.3236 - val_accuracy: 0.8353 - val_loss: 0.3628 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.8735 - loss: 0.2987 - val_accuracy: 0.8696 - val_loss: 0.3244 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.8875 - loss: 0.2871 - val_accuracy: 0.8647 - val_loss: 0.3254 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.8745 - loss: 0.2995 - val_accuracy: 0.8382 - val_loss: 0.3793 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.8779 - loss: 0.2969 - val_accuracy: 0.8578 - val_loss: 0.3562 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8831 - loss: 0.2845 - val_accuracy: 0.9167 - val_loss: 0.2618 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.8934 - loss: 0.2744 - val_accuracy: 0.9118 - val_loss: 0.2488 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.8975 - loss: 0.2489 - val_accuracy: 0.9098 - val_loss: 0.2638 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9007 - loss: 0.2612 - val_accuracy: 0.9196 - val_loss: 0.2500 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 137ms/step - accuracy: 0.8988 - loss: 0.2456 - val_accuracy: 0.9402 - val_loss: 0.1890 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9130 - loss: 0.2196 - val_accuracy: 0.9461 - val_loss: 0.1848 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9088 - loss: 0.2231 - val_accuracy: 0.9216 - val_loss: 0.2386 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8924 - loss: 0.2628 - val_accuracy: 0.9245 - val_loss: 0.2320 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9034 - loss: 0.2417 - val_accuracy: 0.9490 - val_loss: 0.1780 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9159 - loss: 0.2157 - val_accuracy: 0.9490 - val_loss: 0.1652 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.9157 - loss: 0.2230 - val_accuracy: 0.9471 - val_loss: 0.1782 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9125 - loss: 0.2130 - val_accuracy: 0.9578 - val_loss: 0.1478 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9233 - loss: 0.1968 - val_accuracy: 0.9461 - val_loss: 0.1926 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9096 - loss: 0.2203 - val_accuracy: 0.9598 - val_loss: 0.1309 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9201 - loss: 0.2031 - val_accuracy: 0.9647 - val_loss: 0.1221 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9194 - loss: 0.1953 - val_accuracy: 0.9657 - val_loss: 0.1193 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9243 - loss: 0.1994 - val_accuracy: 0.9451 - val_loss: 0.1714 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9174 - loss: 0.2209 - val_accuracy: 0.9216 - val_loss: 0.2104 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9123 - loss: 0.2128 - val_accuracy: 0.9480 - val_loss: 0.1517 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9274 - loss: 0.1854\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9213 - loss: 0.1956 - val_accuracy: 0.9539 - val_loss: 0.1290 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.9324 - loss: 0.1701 - val_accuracy: 0.9725 - val_loss: 0.0973 - learning_rate: 5.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9414 - loss: 0.1650 - val_accuracy: 0.9676 - val_loss: 0.0964 - learning_rate: 5.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9395 - loss: 0.1525 - val_accuracy: 0.9657 - val_loss: 0.1033 - learning_rate: 5.0000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9387 - loss: 0.1655 - val_accuracy: 0.9647 - val_loss: 0.1019 - learning_rate: 5.0000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9368 - loss: 0.1627 - val_accuracy: 0.9539 - val_loss: 0.1241 - learning_rate: 5.0000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9306 - loss: 0.1791 - val_accuracy: 0.9676 - val_loss: 0.0983 - learning_rate: 5.0000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9444 - loss: 0.1511 - val_accuracy: 0.9676 - val_loss: 0.0910 - learning_rate: 5.0000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.9429 - loss: 0.1483 - val_accuracy: 0.9696 - val_loss: 0.0862 - learning_rate: 5.0000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.9422 - loss: 0.1520 - val_accuracy: 0.9588 - val_loss: 0.1191 - learning_rate: 5.0000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9395 - loss: 0.1668 - val_accuracy: 0.9676 - val_loss: 0.0983 - learning_rate: 5.0000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9431 - loss: 0.1539 - val_accuracy: 0.9647 - val_loss: 0.0996 - learning_rate: 5.0000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.9353 - loss: 0.1656 \n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9365 - loss: 0.1671 - val_accuracy: 0.9657 - val_loss: 0.1005 - learning_rate: 5.0000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9480 - loss: 0.1453 - val_accuracy: 0.9696 - val_loss: 0.0928 - learning_rate: 2.5000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9368 - loss: 0.1559 - val_accuracy: 0.9735 - val_loss: 0.0887 - learning_rate: 2.5000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9483 - loss: 0.1350 - val_accuracy: 0.9725 - val_loss: 0.0880 - learning_rate: 2.5000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9505 - loss: 0.1354 - val_accuracy: 0.9735 - val_loss: 0.0870 - learning_rate: 2.5000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.9453 - loss: 0.1401 - val_accuracy: 0.9804 - val_loss: 0.0728 - learning_rate: 2.5000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9444 - loss: 0.1429 - val_accuracy: 0.9745 - val_loss: 0.0769 - learning_rate: 2.5000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9543 - loss: 0.1365\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9510 - loss: 0.1398 - val_accuracy: 0.9765 - val_loss: 0.0759 - learning_rate: 2.5000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9444 - loss: 0.1424 - val_accuracy: 0.9735 - val_loss: 0.0784 - learning_rate: 1.2500e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9407 - loss: 0.1449 - val_accuracy: 0.9775 - val_loss: 0.0722 - learning_rate: 1.2500e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9461 - loss: 0.1350 - val_accuracy: 0.9735 - val_loss: 0.0773 - learning_rate: 1.2500e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9493 - loss: 0.1285 - val_accuracy: 0.9745 - val_loss: 0.0750 - learning_rate: 1.2500e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9525 - loss: 0.1265 - val_accuracy: 0.9755 - val_loss: 0.0738 - learning_rate: 1.2500e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9537 - loss: 0.1294 - val_accuracy: 0.9765 - val_loss: 0.0746 - learning_rate: 1.2500e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 125ms/step - accuracy: 0.9512 - loss: 0.1344 - val_accuracy: 0.9775 - val_loss: 0.0746 - learning_rate: 1.2500e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9493 - loss: 0.1346 - val_accuracy: 0.9755 - val_loss: 0.0754 - learning_rate: 1.2500e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9514 - loss: 0.1227\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9483 - loss: 0.1372 - val_accuracy: 0.9765 - val_loss: 0.0719 - learning_rate: 1.2500e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9510 - loss: 0.1224 - val_accuracy: 0.9735 - val_loss: 0.0745 - learning_rate: 6.2500e-05\n",
      "Epoch 78/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9498 - loss: 0.1340 - val_accuracy: 0.9755 - val_loss: 0.0726 - learning_rate: 6.2500e-05\n",
      "Epoch 79/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9505 - loss: 0.1322 - val_accuracy: 0.9745 - val_loss: 0.0744 - learning_rate: 6.2500e-05\n",
      "Epoch 80/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9554 - loss: 0.1252 - val_accuracy: 0.9765 - val_loss: 0.0713 - learning_rate: 6.2500e-05\n",
      "Epoch 81/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9518 - loss: 0.1282\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.9505 - loss: 0.1300 - val_accuracy: 0.9765 - val_loss: 0.0707 - learning_rate: 6.2500e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.9542 - loss: 0.1244 - val_accuracy: 0.9765 - val_loss: 0.0713 - learning_rate: 3.1250e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9483 - loss: 0.1278 - val_accuracy: 0.9755 - val_loss: 0.0710 - learning_rate: 3.1250e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9490 - loss: 0.1365 - val_accuracy: 0.9784 - val_loss: 0.0694 - learning_rate: 3.1250e-05\n",
      "Epoch 85/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9525 - loss: 0.1286\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9515 - loss: 0.1307 - val_accuracy: 0.9775 - val_loss: 0.0710 - learning_rate: 3.1250e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9522 - loss: 0.1319 - val_accuracy: 0.9784 - val_loss: 0.0706 - learning_rate: 1.5625e-05\n",
      "Epoch 87/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9500 - loss: 0.1278 - val_accuracy: 0.9775 - val_loss: 0.0708 - learning_rate: 1.5625e-05\n",
      "Epoch 88/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9525 - loss: 0.1304 - val_accuracy: 0.9775 - val_loss: 0.0700 - learning_rate: 1.5625e-05\n",
      "Epoch 89/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9534 - loss: 0.1199 - val_accuracy: 0.9784 - val_loss: 0.0703 - learning_rate: 1.5625e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.9525 - loss: 0.1169 - val_accuracy: 0.9765 - val_loss: 0.0713 - learning_rate: 1.5625e-05\n",
      "Epoch 91/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9502 - loss: 0.1303 - val_accuracy: 0.9765 - val_loss: 0.0707 - learning_rate: 1.5625e-05\n",
      "Epoch 92/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9542 - loss: 0.1176 - val_accuracy: 0.9784 - val_loss: 0.0708 - learning_rate: 1.5625e-05\n",
      "Epoch 93/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9512 - loss: 0.1304 - val_accuracy: 0.9794 - val_loss: 0.0699 - learning_rate: 1.5625e-05\n",
      "Epoch 94/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9543 - loss: 0.1167\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9527 - loss: 0.1237 - val_accuracy: 0.9794 - val_loss: 0.0705 - learning_rate: 1.5625e-05\n",
      "Epoch 95/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9498 - loss: 0.1314 - val_accuracy: 0.9775 - val_loss: 0.0713 - learning_rate: 7.8125e-06\n",
      "Epoch 96/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9502 - loss: 0.1316 - val_accuracy: 0.9784 - val_loss: 0.0710 - learning_rate: 7.8125e-06\n",
      "Epoch 97/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.9500 - loss: 0.1321 - val_accuracy: 0.9775 - val_loss: 0.0710 - learning_rate: 7.8125e-06\n",
      "Epoch 98/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9506 - loss: 0.1333\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.9468 - loss: 0.1398 - val_accuracy: 0.9794 - val_loss: 0.0703 - learning_rate: 7.8125e-06\n",
      "Epoch 99/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9498 - loss: 0.1233 - val_accuracy: 0.9794 - val_loss: 0.0704 - learning_rate: 3.9063e-06\n",
      "Epoch 99: early stopping\n",
      "Restoring model weights from the end of the best epoch: 84.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] Model saved as cole-final.h5\n",
      "\n",
      "[CONFUSION MATRIX]\n",
      "[[531   9]\n",
      " [ 13 467]]\n",
      "[VAL] acc=0.9784, prec=0.9811, rec=0.9729, f1=0.9770\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 4: 80:20 Train / Validation Split (EEGNet)\n",
    "# ====================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, SeparableConv2D, BatchNormalization, Activation,\n",
    "    DepthwiseConv2D, Conv2D, AveragePooling2D,\n",
    "    Dropout, Dense, Flatten\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# --------------------------------------------------------------------\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# EEGNet model\n",
    "# --------------------------------------------------------------------\n",
    "def create_eegnet_model(input_shape, dropout_rate=0.5, num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(16, (1, 64), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = DepthwiseConv2D(\n",
    "        (input_shape[0], 1),\n",
    "        depth_multiplier=2,\n",
    "        padding='valid',\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 4))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = SeparableConv2D(16, (1, 16), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 8))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Compile model\n",
    "# --------------------------------------------------------------------\n",
    "def compile_model():\n",
    "    input_shape = (Xtr.shape[1], Xtr.shape[2], Xtr.shape[3])\n",
    "    model = create_eegnet_model(input_shape)\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-3),\n",
    "        loss=BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[BinaryAccuracy(name=\"accuracy\", threshold=0.5)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def make_callbacks():\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"loss\",\n",
    "            factor=0.5,\n",
    "            patience=4,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Use ALL normalized training data\n",
    "# --------------------------------------------------------------------\n",
    "Xtr = X_train_norm\n",
    "ytr = y_train_norm\n",
    "\n",
    "print(f\"[TRAIN] Xtr: {Xtr.shape}, ytr: {ytr.shape}\")\n",
    "print(\"[INFO] Test / LOSO subject is NOT used here.\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 80:20 Stratified Split\n",
    "# --------------------------------------------------------------------\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(sss.split(Xtr, ytr))\n",
    "\n",
    "X_train, X_val = Xtr[train_idx], Xtr[val_idx]\n",
    "y_train, y_val = ytr[train_idx], ytr[val_idx]\n",
    "\n",
    "print(f\"[SPLIT] Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Train model\n",
    "# --------------------------------------------------------------------\n",
    "model = compile_model()\n",
    "callbacks = make_callbacks()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Save final model\n",
    "# --------------------------------------------------------------------\n",
    "model.save(\"max-final.h5\")\n",
    "print(\"[SAVE] Model saved as cole-final.h5\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Validation Confusion Matrix\n",
    "# --------------------------------------------------------------------\n",
    "y_prob = model.predict(X_val, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "y_true = y_val.astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "eps = 1e-12\n",
    "\n",
    "acc  = (tp + tn) / max(tp + tn + fp + fn, eps)\n",
    "prec = tp / max(tp + fp, eps)\n",
    "rec  = tp / max(tp + fn, eps)\n",
    "f1   = 2 * prec * rec / max(prec + rec, eps)\n",
    "\n",
    "print(\"\\n[CONFUSION MATRIX]\")\n",
    "print(cm)\n",
    "print(f\"[VAL] acc={acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e2a13-6367-41ca-be66-a43594c2940e",
   "metadata": {},
   "source": [
    "Cell 8 — Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ac6662-33c1-4e25-b6c5-d47c860fba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Preparing TEST data...\n",
      "[TEST] Xte shape: (300, 14, 128, 1)\n",
      "[TEST] yte shape: (300,), class distribution: (array([1]), array([300]))\n",
      "[TEST] Loading final model: max-final.h5\n",
      "[TEST] Running inference on TEST subject...\n",
      "[TEST] Mean predicted probability: 0.2879\n",
      "\n",
      "[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[  0   0]\n",
      " [234  66]]\n",
      "Accuracy : 0.2200\n",
      "Precision: 1.0000\n",
      "Recall   : 0.2200\n",
      "F1-score : 0.3607\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Inference on TEST subject (adam) using FINAL EEGNet\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Prepare TEST data\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Preparing TEST data...\")\n",
    "\n",
    "Xte = X_test_norm\n",
    "yte = y_test_norm.astype(int)\n",
    "\n",
    "print(f\"[TEST] Xte shape: {Xte.shape}\")\n",
    "print(f\"[TEST] yte shape: {yte.shape}, class distribution: {np.unique(yte, return_counts=True)}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Load FINAL trained model\n",
    "# ------------------------------------------------------------------\n",
    "model_path = \"max-final.h5\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        \"Final model 'cole-final.h5' not found. \"\n",
    "        \"Please run Cell 4 (80:20 training) first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[TEST] Loading final model: {model_path}\")\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Run inference on TEST subject\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Running inference on TEST subject...\")\n",
    "\n",
    "y_prob = model.predict(Xte, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(f\"[TEST] Mean predicted probability: {y_prob.mean():.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Metrics on TEST subject (segment-level)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc = accuracy_score(yte, y_pred)\n",
    "prec = precision_score(yte, y_pred, zero_division=0)\n",
    "rec = recall_score(yte, y_pred, zero_division=0)\n",
    "f1 = f1_score(yte, y_pred, zero_division=0)\n",
    "\n",
    "print(\"\\n[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572dbf3e-bbb5-4865-883f-30e374700949",
   "metadata": {},
   "source": [
    "Cell 9 - Performance Matrices for each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a57240-50ea-4736-a7ac-52ef7878b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] Verifying predictions and labels are available...\n",
      "[CHECK] Found predictions for 300 test segments.\n",
      "\n",
      "[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[  0   0]\n",
      " [234  66]]\n",
      "Accuracy : 0.2200\n",
      "Precision: 1.0000\n",
      "Recall   : 0.2200\n",
      "F1-score : 0.3607\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Final model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================\n",
    "# Cell 6: Confusion Matrix & Metrics on TEST (FINAL EEGNet)\n",
    "# =======================================================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# ------------------------------------------------------------------\n",
    "def compute_metrics_from_cm(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    eps = 1e-12\n",
    "    acc  = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec  = tp / (tp + fn + eps)\n",
    "    f1   = 2 * prec * rec / (prec + rec + eps)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], ['0', '1'])\n",
    "    plt.yticks([0, 1], ['0', '1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(\n",
    "                j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Validate upstream artifacts\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[CHECK] Verifying predictions and labels are available...\")\n",
    "\n",
    "if 'y_pred' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Predictions 'y_pred' not found. \"\n",
    "        \"Please run Cell 5 (final model inference) first.\"\n",
    "    )\n",
    "\n",
    "if 'yte' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Test labels 'yte' not found. \"\n",
    "        \"Please run Cell 5 first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[CHECK] Found predictions for {len(y_pred)} test segments.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Confusion Matrix & Metrics (FINAL model)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc, prec, rec, f1 = compute_metrics_from_cm(cm)\n",
    "\n",
    "print(\"\\n[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(cm, title=\"Confusion Matrix - FINAL EEGNet\")\n",
    "\n",
    "print(\"[DONE] Final model evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0f120-39fd-497e-9098-05e6c82f4807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
