{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af5b471-cea9-4194-bc23-da5cdc2f9c67",
   "metadata": {},
   "source": [
    "Cell 1 — Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64aaa897-ab61-4eb4-a558-4c078acf751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Base path set to:\n",
      "C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\n",
      "[INFO] Found 18 folders:\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Selected test subject (LOSO): Amin1\n",
      "[INFO] Training subjects (17):\n",
      "['Amin', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Sampling rate (SFREQ) = 128.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 1: Setup & discovery\n",
    "# =========================\n",
    "\n",
    "import os  # file ops\n",
    "import glob  # file matching\n",
    "import numpy as np  # arrays\n",
    "import random  # seeds\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data path\n",
    "BASE_PATH = r\"C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\"\n",
    "print(f\"[INFO] Base path set to:\\n{BASE_PATH}\")\n",
    "\n",
    "# List subject folders\n",
    "all_folders = [p for p in sorted(os.listdir(BASE_PATH)) if os.path.isdir(os.path.join(BASE_PATH, p))]\n",
    "print(f\"[INFO] Found {len(all_folders)} folders:\\n{all_folders}\")\n",
    "\n",
    "# Expected count (change if needed)\n",
    "assert len(all_folders) == 18, f\"Expected 18 folders, found {len(all_folders)}.\"\n",
    "\n",
    "# LOSO test subject\n",
    "TEST_SUBJECT = \"Amin1\"\n",
    "print(f\"[INFO] Selected test subject (LOSO): {TEST_SUBJECT}\")\n",
    "\n",
    "# Train subjects = remaining\n",
    "train_subjects = [s for s in all_folders if s.lower() != TEST_SUBJECT.lower()]\n",
    "print(f\"[INFO] Training subjects ({len(train_subjects)}):\\n{train_subjects}\")\n",
    "\n",
    "# Sampling rate\n",
    "SFREQ = 128.0\n",
    "print(f\"[INFO] Sampling rate (SFREQ) = {SFREQ} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e649681-2861-4ce2-bbd1-0d9dd3359f42",
   "metadata": {},
   "source": [
    "Cell 2 — Load EEG + labels per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943c5c6e-a7aa-4618-899c-daf341c25358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 2] Loading data from folders...\n",
      "[STEP 2]       Amin | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Amin | EEG=          Amin.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Amin1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]      Amin1 | EEG=         Amin1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Cole | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Cole | EEG=          Cole.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Daniel | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]     Daniel | EEG=        Daniel.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]    Ismayil | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]    Ismayil | EEG=       Ismayil.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Jack | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Jack | EEG=          Jack.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]      James | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      James | EEG=         James.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Josh | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Josh | EEG=          Josh.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Marjan | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Marjan | EEG=        Marjan.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]        Max | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Max | EEG=           Max.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Mina | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mina | EEG=          Mina.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 1 | EEG=        Mina 1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 3 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 3 | EEG=        Mina 3.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]   Mohammad | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]   Mohammad | EEG=      Mohammad.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Mona | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mona | EEG=          Mona.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Roddy | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      Roddy | EEG=         Roddy.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]        Sam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Sam | EEG=           Sam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       adam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       adam | EEG=          adam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2] Loading complete ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 2: Load data (EMPTY LABEL => CLASS 0)\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path  # helper\n",
    "\n",
    "def load_subject(subj_dir: str, subj_name: str, expected_channels: int = 14):\n",
    "    \"\"\"\n",
    "    EEG file: {subj_name}.npy\n",
    "    Label file: {subj_name}_target.npy\n",
    "    Label rule:\n",
    "      - empty array -> class 0\n",
    "      - [0] -> class 0\n",
    "      - [1] -> class 1\n",
    "    \"\"\"\n",
    "\n",
    "    # EEG file path\n",
    "    eeg_file = os.path.join(subj_dir, f\"{subj_name}.npy\")\n",
    "    assert os.path.exists(eeg_file), f\"[ERROR] Missing EEG file: {eeg_file}\"\n",
    "\n",
    "    # Label file path\n",
    "    target_file = os.path.join(subj_dir, f\"{subj_name}_target.npy\")\n",
    "    assert os.path.exists(target_file), f\"[ERROR] Missing label file: {target_file}\"\n",
    "\n",
    "    # Load EEG\n",
    "    X_raw = np.load(eeg_file, allow_pickle=True)\n",
    "    X_raw = np.asarray(X_raw)\n",
    "\n",
    "    # Standardize EEG to (n_trials, 14, T)\n",
    "    if X_raw.ndim == 2:\n",
    "        X_trials = X_raw[None, :, :]\n",
    "    elif X_raw.ndim == 3:\n",
    "        dims = list(X_raw.shape)\n",
    "        chan_axes = [i for i, d in enumerate(dims) if d == expected_channels]\n",
    "        chan_axis = chan_axes[0] if len(chan_axes) == 1 else 1\n",
    "        time_axis = int(np.argmax(dims))\n",
    "        trial_axis = [i for i in range(3) if i not in (chan_axis, time_axis)][0]\n",
    "        X_trials = np.moveaxis(X_raw, (trial_axis, chan_axis, time_axis), (0, 1, 2))\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] Unsupported EEG shape {X_raw.shape} in {eeg_file}\")\n",
    "\n",
    "    assert X_trials.shape[1] == expected_channels, f\"[ERROR] Expected {expected_channels} channels, got {X_trials.shape[1]} in {eeg_file}\"\n",
    "\n",
    "    # Load label\n",
    "    y_raw = np.load(target_file, allow_pickle=True)\n",
    "    y_arr = np.asarray(y_raw).ravel()\n",
    "\n",
    "    # Apply your rule\n",
    "    if y_arr.size == 0:\n",
    "        label = 0\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\")\n",
    "    else:\n",
    "        label = int(y_arr[0])\n",
    "        if label not in (0, 1):\n",
    "            raise ValueError(f\"[ERROR] Invalid label {label} in {target_file} (must be 0 or 1)\")\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape={y_arr.shape}, value={y_arr} -> class={label}\")\n",
    "\n",
    "    # Repeat label for all trials\n",
    "    y_trials = np.full((X_trials.shape[0],), label, dtype=np.int32)\n",
    "\n",
    "    return X_trials.astype(np.float32), y_trials.astype(np.int32), eeg_file, target_file\n",
    "\n",
    "\n",
    "# Load all subjects\n",
    "subject_data = {}\n",
    "print(\"[STEP 2] Loading data from folders...\")\n",
    "\n",
    "for subj in all_folders:\n",
    "    subj_dir = os.path.join(BASE_PATH, subj)\n",
    "    X_trials, y_trials, eeg_file, target_file = load_subject(subj_dir, subj)\n",
    "\n",
    "    subject_data[subj] = (X_trials, y_trials)\n",
    "\n",
    "    print(f\"[STEP 2] {subj:>10} | EEG={Path(eeg_file).name:>18} | X={X_trials.shape} | y={y_trials.shape} | class={y_trials[0]}\")\n",
    "\n",
    "print(\"[STEP 2] Loading complete ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd93ca8-a94c-485c-8a61-e6e7441f6299",
   "metadata": {},
   "source": [
    "Cell 3 — Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733e592c-98e1-4cbd-afd2-6fdc8cf407f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 3] Preprocessing functions ready ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3: Preprocessing functions\n",
    "# =========================\n",
    "\n",
    "import mne  # EEG processing\n",
    "import pywt  # wavelets\n",
    "from sklearn.decomposition import FastICA  # ICA\n",
    "from typing import Optional, Tuple, Union, Sequence, Dict, List  # typing\n",
    "\n",
    "def wavelet_enhanced_ica(\n",
    "    data: np.ndarray,\n",
    "    n_components: int = 10,\n",
    "    wavelet: str = \"db4\",\n",
    "    level: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> np.ndarray:\n",
    "    n_ch, n_t = data.shape\n",
    "    n_components = min(n_components, n_ch)\n",
    "\n",
    "    coeffs = pywt.wavedec(data, wavelet=wavelet, level=level, axis=1)\n",
    "    A = coeffs[0]\n",
    "\n",
    "    ica = FastICA(n_components=n_components, random_state=random_state)\n",
    "    S = ica.fit_transform(A.T).T\n",
    "    A_denoised = ica.inverse_transform(S.T).T\n",
    "\n",
    "    coeffs[0] = A_denoised\n",
    "    cleaned = pywt.waverec(coeffs, wavelet=wavelet, axis=1)\n",
    "\n",
    "    if cleaned.shape[1] != n_t:\n",
    "        if cleaned.shape[1] > n_t:\n",
    "            cleaned = cleaned[:, :n_t]\n",
    "        else:\n",
    "            cleaned = np.pad(cleaned, ((0, 0), (0, n_t - cleaned.shape[1])), mode=\"constant\")\n",
    "    return cleaned\n",
    "\n",
    "def _names_from_index_mapping(n_channels: int, index_to_name: Optional[Dict[int, str]]) -> List[str]:\n",
    "    if index_to_name is None:\n",
    "        return [f\"EEG{i+1}\" for i in range(n_channels)]\n",
    "    keys = list(index_to_name.keys())\n",
    "    is_zero_based = (0 in keys) and (1 not in keys)\n",
    "    names = []\n",
    "    for i in range(n_channels):\n",
    "        key = i if is_zero_based else (i + 1)\n",
    "        names.append(index_to_name.get(key, f\"EEG{i+1}\"))\n",
    "    return names\n",
    "\n",
    "def preprocess_eeg(\n",
    "    eeg: np.ndarray,\n",
    "    sfreq: float,\n",
    "    *,\n",
    "    index_to_name: Optional[Dict[int, str]] = None,\n",
    "    use_standard_1010: bool = True,\n",
    "    resample_to: Optional[float] = None,\n",
    "    notch_freqs: Union[None, float, Sequence[float]] = 50.0,\n",
    "    highpass: Optional[float] = 0.05,\n",
    "    bad_point_z: float = 6.0,\n",
    "    bad_channel_z: float = 5.0,\n",
    "    interpolate_bad_channels: bool = True,\n",
    "    car: bool = True,\n",
    "    use_wica: bool = True,\n",
    "    wica_components: int = 10,\n",
    "    wica_wavelet: str = \"db4\",\n",
    "    wica_level: int = 3,\n",
    "    wica_random_state: int = 42,\n",
    "    return_raw: bool = False,\n",
    "):\n",
    "\n",
    "    eeg = np.asarray(eeg, dtype=np.float32)\n",
    "    assert eeg.ndim == 2, \"eeg must be 2D: (n_channels, n_times)\"\n",
    "\n",
    "    n_channels, _ = eeg.shape\n",
    "    ch_names = _names_from_index_mapping(n_channels, index_to_name)\n",
    "    ch_types = ['eog' if str(n).upper().startswith(\"EOG\") else 'eeg' for n in ch_names]\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    raw = mne.io.RawArray(eeg, info, verbose=False)\n",
    "\n",
    "    montage_applied = False\n",
    "    if use_standard_1010:\n",
    "        try:\n",
    "            mont = mne.channels.make_standard_montage(\"standard_1010\")\n",
    "            raw.set_montage(mont, match_case=False, on_missing=\"ignore\")\n",
    "            montage_applied = True\n",
    "        except Exception:\n",
    "            montage_applied = False\n",
    "\n",
    "    if resample_to is not None and float(resample_to) != float(sfreq):\n",
    "        raw.resample(sfreq=resample_to, npad=\"auto\")\n",
    "    sfreq_out = float(raw.info[\"sfreq\"])\n",
    "\n",
    "    if notch_freqs is not None:\n",
    "        raw.notch_filter(freqs=notch_freqs, verbose=False)\n",
    "    if highpass is not None:\n",
    "        raw.filter(l_freq=highpass, h_freq=None, verbose=False)\n",
    "\n",
    "    X = raw.get_data()\n",
    "    mu = np.mean(X, axis=1, keepdims=True)\n",
    "    sd = np.std(X, axis=1, keepdims=True) + 1e-12\n",
    "    hi = mu + bad_point_z * sd\n",
    "    lo = mu - bad_point_z * sd\n",
    "    bad_idx = (X > hi) | (X < lo)\n",
    "\n",
    "    if np.any(bad_idx):\n",
    "        X_fixed = X.copy()\n",
    "        t = np.arange(X.shape[1], dtype=float)\n",
    "        for ch in range(n_channels):\n",
    "            mask = bad_idx[ch]\n",
    "            if mask.any():\n",
    "                good = ~mask\n",
    "                if good.sum() >= 2:\n",
    "                    X_fixed[ch, mask] = np.interp(t[mask], t[good], X_fixed[ch, good])\n",
    "        raw._data = X_fixed\n",
    "\n",
    "    if interpolate_bad_channels and montage_applied:\n",
    "        X = raw.get_data(picks=\"eeg\")\n",
    "        if X.size > 0:\n",
    "            ch_std = X.std(axis=1)\n",
    "            med = np.median(ch_std)\n",
    "            mad = np.median(np.abs(ch_std - med)) + 1e-12\n",
    "            z = 0.6745 * (ch_std - med) / mad\n",
    "            eeg_names = mne.pick_info(raw.info, mne.pick_types(raw.info, eeg=True)).ch_names\n",
    "            bads = [eeg_names[i] for i in np.where(np.abs(z) > bad_channel_z)[0]]\n",
    "            raw.info[\"bads\"] = bads\n",
    "            if bads:\n",
    "                raw.interpolate_bads(reset_bads=True, verbose=False)\n",
    "\n",
    "    if car:\n",
    "        raw.set_eeg_reference(\"average\", projection=True)\n",
    "        raw.apply_proj()\n",
    "\n",
    "    if use_wica:\n",
    "        cleaned = wavelet_enhanced_ica(\n",
    "            raw.get_data(),\n",
    "            n_components=wica_components,\n",
    "            wavelet=wica_wavelet,\n",
    "            level=wica_level,\n",
    "            random_state=wica_random_state\n",
    "        )\n",
    "        raw = mne.io.RawArray(cleaned, raw.info, verbose=False)\n",
    "\n",
    "    cleaned = raw.get_data()\n",
    "    return cleaned, sfreq_out\n",
    "\n",
    "print(\"[STEP 3] Preprocessing functions ready ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a9d20-889d-45c4-bbc2-de62216908cd",
   "metadata": {},
   "source": [
    "Cell 4 — Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bfcbde0-1cc2-409c-8627-93da46606661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 4] Starting preprocessing...\n",
      "[INFO] USE_WICA = True\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 01/17: Amin | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 02/17: Cole | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 03/17: Daniel | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 04/17: Ismayil | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 05/17: Jack | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 06/17: James | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 07/17: Josh | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 08/17: Marjan | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 09/17: Max | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 10/17: Mina | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 11/17: Mina 1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 12/17: Mina 3 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 13/17: Mohammad | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 14/17: Mona | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 15/17: Roddy | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 16/17: Sam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 17/17: adam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Test subject: Amin1 | trials=1 | fs_out=128.0\n",
      "[STEP 4] Preprocessing done ✅\n",
      "  X_train_trials_all: (17, 14, 38400)\n",
      "  y_train_trials_all: (17,) (array([0, 1], dtype=int32), array([8, 9]))\n",
      "  X_test_trials_all : (1, 14, 38400)\n",
      "  y_test_trials_all : (1,) (array([0], dtype=int32), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 4: Apply preprocessing (LOSO)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 4] Starting preprocessing...\")\n",
    "\n",
    "USE_WICA = True\n",
    "print(f\"[INFO] USE_WICA = {USE_WICA}\")\n",
    "\n",
    "X_train_trials_all, y_train_trials_all = [], []\n",
    "X_test_trials_all, y_test_trials_all = [], []\n",
    "\n",
    "def preprocess_trials(X_trials, y_trials):\n",
    "    cleaned_list = []\n",
    "    label_list = []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        trial = X_trials[i]\n",
    "        label = int(y_trials[i])\n",
    "\n",
    "        cleaned, fs_out = preprocess_eeg(\n",
    "            eeg=trial,\n",
    "            sfreq=SFREQ,\n",
    "            use_wica=USE_WICA\n",
    "        )\n",
    "\n",
    "        cleaned_list.append(cleaned.astype(np.float32, copy=False))\n",
    "        label_list.append(label)\n",
    "\n",
    "    return cleaned_list, np.array(label_list, dtype=np.int32), fs_out\n",
    "\n",
    "# Train subjects\n",
    "for idx, subj in enumerate(train_subjects, start=1):\n",
    "    X_trials, y_trials = subject_data[subj]\n",
    "    cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "    X_train_trials_all.extend(cleaned_trials)\n",
    "    y_train_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "    print(f\"[STEP 4] Train {idx:02d}/{len(train_subjects)}: {subj} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Test subject\n",
    "X_trials, y_trials = subject_data[TEST_SUBJECT]\n",
    "cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "X_test_trials_all.extend(cleaned_trials)\n",
    "y_test_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "print(f\"[STEP 4] Test subject: {TEST_SUBJECT} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Convert to arrays\n",
    "X_train_trials_all = np.array(X_train_trials_all, dtype=np.float32)\n",
    "y_train_trials_all = np.array(y_train_trials_all, dtype=np.int32)\n",
    "\n",
    "X_test_trials_all = np.array(X_test_trials_all, dtype=np.float32)\n",
    "y_test_trials_all = np.array(y_test_trials_all, dtype=np.int32)\n",
    "\n",
    "print(\"[STEP 4] Preprocessing done ✅\")\n",
    "print(\"  X_train_trials_all:\", X_train_trials_all.shape)\n",
    "print(\"  y_train_trials_all:\", y_train_trials_all.shape, np.unique(y_train_trials_all, return_counts=True))\n",
    "print(\"  X_test_trials_all :\", X_test_trials_all.shape)\n",
    "print(\"  y_test_trials_all :\", y_test_trials_all.shape, np.unique(y_test_trials_all, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc3da1-2967-4ee3-84cd-bfcf0f98c0b1",
   "metadata": {},
   "source": [
    "Cell 5 — Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c21ad54-7001-4039-9360-29dfa2905127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 5] Starting segmentation...\n",
      "[STEP 5] Segmented 5/17 trials...\n",
      "[STEP 5] Segmented 10/17 trials...\n",
      "[STEP 5] Segmented 15/17 trials...\n",
      "[STEP 5] Segmented 17/17 trials...\n",
      "[STEP 5] Segmented 1/1 trials...\n",
      "[STEP 5] Segmentation done ✅\n",
      "  X_train: (5100, 14, 128, 1)\n",
      "  y_train: (5100,) (array([0, 1], dtype=int32), array([2400, 2700]))\n",
      "  X_test : (300, 14, 128, 1)\n",
      "  y_test : (300,) (array([0], dtype=int32), array([300]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 5: Segmentation\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 5] Starting segmentation...\")\n",
    "\n",
    "def segment_trial(trial_2d: np.ndarray, label: int, segment_len: int = 128):\n",
    "    C, T = trial_2d.shape\n",
    "    n_segs = T // segment_len\n",
    "    use_T = n_segs * segment_len\n",
    "\n",
    "    if n_segs == 0:\n",
    "        return np.empty((0, C, segment_len), dtype=np.float32), np.empty((0,), dtype=np.int32)\n",
    "\n",
    "    trial_trim = trial_2d[:, :use_T]\n",
    "    Xsegs = trial_trim.reshape(C, n_segs, segment_len).transpose(1, 0, 2)\n",
    "    ysegs = np.full((n_segs,), int(label), dtype=np.int32)\n",
    "    return Xsegs.astype(np.float32, copy=False), ysegs\n",
    "\n",
    "def segment_dataset(X_trials: np.ndarray, y_trials: np.ndarray, segment_len: int = 128):\n",
    "    X_all, y_all = [], []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        Xsegs, ysegs = segment_trial(X_trials[i], int(y_trials[i]), segment_len=segment_len)\n",
    "        X_all.append(Xsegs)\n",
    "        y_all.append(ysegs)\n",
    "\n",
    "        if (i + 1) % 5 == 0 or (i + 1) == X_trials.shape[0]:\n",
    "            print(f\"[STEP 5] Segmented {i+1}/{X_trials.shape[0]} trials...\")\n",
    "\n",
    "    X_out = np.concatenate(X_all, axis=0)\n",
    "    y_out = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    X_out = np.expand_dims(X_out, axis=-1)  # (N,14,128,1)\n",
    "    return X_out.astype(np.float32), y_out.astype(np.int32)\n",
    "\n",
    "X_train, y_train = segment_dataset(X_train_trials_all, y_train_trials_all, segment_len=128)\n",
    "X_test,  y_test  = segment_dataset(X_test_trials_all,  y_test_trials_all,  segment_len=128)\n",
    "\n",
    "print(\"[STEP 5] Segmentation done ✅\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  y_train:\", y_train.shape, np.unique(y_train, return_counts=True))\n",
    "print(\"  X_test :\", X_test.shape)\n",
    "print(\"  y_test :\", y_test.shape, np.unique(y_test, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b4ffd5-bf2f-490e-8854-0a2fc747f1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e69ba7-3952-47b1-bd80-f05237b2e018",
   "metadata": {},
   "source": [
    "Cell 6 — Split train/val + Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97fda4ae-e1c8-4fd1-bf0c-c82ae2a85764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 6] Normalizing using ALL training data (no train/val split)...\n",
      "[STEP 6] Normalization done ✅\n",
      "  X_train_norm: (5100, 14, 128, 1)\n",
      "  X_test_norm : (300, 14, 128, 1)\n",
      "  y_train_norm: (5100,)\n",
      "  y_test_norm : (300,)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 6: Normalize (ALL training data)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 6] Normalizing using ALL training data (no train/val split)...\")\n",
    "\n",
    "def normalize_eeg(train_data: np.ndarray, test_data: np.ndarray):\n",
    "    if train_data.ndim == 5:\n",
    "        train_flat = train_data.reshape(-1, *train_data.shape[2:])\n",
    "        test_flat  = test_data.reshape(-1, *test_data.shape[2:])\n",
    "    else:\n",
    "        train_flat, test_flat = train_data, test_data\n",
    "\n",
    "    # Compute stats on ALL training data\n",
    "    train_mean = np.mean(train_flat, axis=(0, 2, 3), keepdims=True)\n",
    "    train_std  = np.std(train_flat,  axis=(0, 2, 3), keepdims=True) + 1e-8\n",
    "\n",
    "    train_norm = (train_flat - train_mean) / train_std\n",
    "    test_norm  = (test_flat  - train_mean) / train_std\n",
    "\n",
    "    if train_data.ndim == 5:\n",
    "        train_norm = train_norm.reshape(train_data.shape)\n",
    "        test_norm  = test_norm.reshape(test_data.shape)\n",
    "\n",
    "    return (\n",
    "        train_norm.astype(np.float32),\n",
    "        test_norm.astype(np.float32),\n",
    "        train_mean,\n",
    "        train_std\n",
    "    )\n",
    "\n",
    "# Normalize ALL training data\n",
    "X_train_norm, X_test_norm, mean, std = normalize_eeg(X_train, X_test)\n",
    "\n",
    "y_train_norm = y_train.astype(np.float32)\n",
    "y_test_norm  = y_test.astype(np.float32)\n",
    "\n",
    "print(\"[STEP 6] Normalization done ✅\")\n",
    "print(\"  X_train_norm:\", X_train_norm.shape)\n",
    "print(\"  X_test_norm :\", X_test_norm.shape)\n",
    "print(\"  y_train_norm:\", y_train_norm.shape)\n",
    "print(\"  y_test_norm :\", y_test_norm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f527c-34db-4c23-bdbd-c8ce03b519f0",
   "metadata": {},
   "source": [
    "Cell 7 — EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ea4279-53c2-49c1-b834-7b1e1c20775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\DL\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Xtr: (5100, 14, 128, 1), ytr: (5100,)\n",
      "[INFO] Test / LOSO subject is NOT used here.\n",
      "[SPLIT] Train: (4080, 14, 128, 1), Val: (1020, 14, 128, 1)\n",
      "Epoch 1/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 135ms/step - accuracy: 0.5238 - loss: 0.7308 - val_accuracy: 0.4775 - val_loss: 0.6952 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.5473 - loss: 0.6809 - val_accuracy: 0.4833 - val_loss: 0.6941 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.5850 - loss: 0.6611 - val_accuracy: 0.5059 - val_loss: 0.6917 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 157ms/step - accuracy: 0.6108 - loss: 0.6430 - val_accuracy: 0.5392 - val_loss: 0.6886 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.6480 - loss: 0.6073 - val_accuracy: 0.6186 - val_loss: 0.6824 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.6775 - loss: 0.5807 - val_accuracy: 0.6176 - val_loss: 0.6736 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.6968 - loss: 0.5634 - val_accuracy: 0.6039 - val_loss: 0.6627 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.7365 - loss: 0.5214 - val_accuracy: 0.6000 - val_loss: 0.6498 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.7500 - loss: 0.5026 - val_accuracy: 0.5824 - val_loss: 0.6534 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.7814 - loss: 0.4657 - val_accuracy: 0.5922 - val_loss: 0.6423 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7902 - loss: 0.4385 - val_accuracy: 0.5794 - val_loss: 0.6551 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8005 - loss: 0.4228 - val_accuracy: 0.5853 - val_loss: 0.6663 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.8157 - loss: 0.4080 - val_accuracy: 0.5833 - val_loss: 0.6487 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8228 - loss: 0.3861 - val_accuracy: 0.6000 - val_loss: 0.6475 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.8270 - loss: 0.3953 - val_accuracy: 0.6147 - val_loss: 0.6319 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8316 - loss: 0.3823 - val_accuracy: 0.6363 - val_loss: 0.5920 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8466 - loss: 0.3432 - val_accuracy: 0.6480 - val_loss: 0.5815 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8490 - loss: 0.3459 - val_accuracy: 0.6520 - val_loss: 0.5723 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8517 - loss: 0.3341 - val_accuracy: 0.6843 - val_loss: 0.5316 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8652 - loss: 0.3239 - val_accuracy: 0.7020 - val_loss: 0.5401 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8667 - loss: 0.3154 - val_accuracy: 0.7363 - val_loss: 0.4764 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.8782 - loss: 0.2978 - val_accuracy: 0.7147 - val_loss: 0.5218 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.8792 - loss: 0.2951 - val_accuracy: 0.7441 - val_loss: 0.4759 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8885 - loss: 0.2810 - val_accuracy: 0.7559 - val_loss: 0.4740 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8860 - loss: 0.2883 - val_accuracy: 0.8265 - val_loss: 0.3526 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8838 - loss: 0.2876 - val_accuracy: 0.8069 - val_loss: 0.3668 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8836 - loss: 0.2696 - val_accuracy: 0.8098 - val_loss: 0.3473 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8934 - loss: 0.2594 - val_accuracy: 0.8255 - val_loss: 0.3288 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8958 - loss: 0.2629 - val_accuracy: 0.8657 - val_loss: 0.2735 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.8926 - loss: 0.2665 - val_accuracy: 0.8578 - val_loss: 0.2927 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.9047 - loss: 0.2383 - val_accuracy: 0.8863 - val_loss: 0.2342 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.8949 - loss: 0.2529 - val_accuracy: 0.9216 - val_loss: 0.2049 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9000 - loss: 0.2445 - val_accuracy: 0.9069 - val_loss: 0.2107 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9098 - loss: 0.2328 - val_accuracy: 0.9373 - val_loss: 0.1736 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9135 - loss: 0.2188 - val_accuracy: 0.9510 - val_loss: 0.1452 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9108 - loss: 0.2279 - val_accuracy: 0.9324 - val_loss: 0.1884 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9132 - loss: 0.2171 - val_accuracy: 0.9608 - val_loss: 0.1194 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9272 - loss: 0.1910 - val_accuracy: 0.9598 - val_loss: 0.1191 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9252 - loss: 0.1959 - val_accuracy: 0.9618 - val_loss: 0.1229 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9321 - loss: 0.1785 - val_accuracy: 0.9706 - val_loss: 0.0998 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9306 - loss: 0.1866 - val_accuracy: 0.9608 - val_loss: 0.1391 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9292 - loss: 0.1806 - val_accuracy: 0.9559 - val_loss: 0.1226 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9360 - loss: 0.1691 - val_accuracy: 0.9647 - val_loss: 0.0956 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9360 - loss: 0.1578 - val_accuracy: 0.9667 - val_loss: 0.1039 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9319 - loss: 0.1738 - val_accuracy: 0.9559 - val_loss: 0.1255 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9255 - loss: 0.1857 - val_accuracy: 0.9529 - val_loss: 0.1294 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9316 - loss: 0.1765 - val_accuracy: 0.9725 - val_loss: 0.0928 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9429 - loss: 0.1482 - val_accuracy: 0.9657 - val_loss: 0.0942 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9390 - loss: 0.1514 - val_accuracy: 0.9598 - val_loss: 0.1347 - learning_rate: 0.0010\n",
      "Epoch 50/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9478 - loss: 0.1424 - val_accuracy: 0.9686 - val_loss: 0.1047 - learning_rate: 0.0010\n",
      "Epoch 51/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9439 - loss: 0.1391 - val_accuracy: 0.9696 - val_loss: 0.0948 - learning_rate: 0.0010\n",
      "Epoch 52/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9363 - loss: 0.1622 - val_accuracy: 0.9598 - val_loss: 0.1210 - learning_rate: 0.0010\n",
      "Epoch 53/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9444 - loss: 0.1360 - val_accuracy: 0.9696 - val_loss: 0.0949 - learning_rate: 0.0010\n",
      "Epoch 54/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9407 - loss: 0.1449 - val_accuracy: 0.9667 - val_loss: 0.0982 - learning_rate: 0.0010\n",
      "Epoch 55/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9434 - loss: 0.1473 - val_accuracy: 0.9667 - val_loss: 0.0918 - learning_rate: 0.0010\n",
      "Epoch 56/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9439 - loss: 0.1401 - val_accuracy: 0.9647 - val_loss: 0.0943 - learning_rate: 0.0010\n",
      "Epoch 57/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9510 - loss: 0.1346 - val_accuracy: 0.9735 - val_loss: 0.0824 - learning_rate: 0.0010\n",
      "Epoch 58/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9480 - loss: 0.1322 - val_accuracy: 0.9569 - val_loss: 0.1124 - learning_rate: 0.0010\n",
      "Epoch 59/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9373 - loss: 0.1582 - val_accuracy: 0.9627 - val_loss: 0.1023 - learning_rate: 0.0010\n",
      "Epoch 60/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9475 - loss: 0.1316 - val_accuracy: 0.9637 - val_loss: 0.1073 - learning_rate: 0.0010\n",
      "Epoch 61/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9424 - loss: 0.1430 - val_accuracy: 0.9647 - val_loss: 0.1035 - learning_rate: 0.0010\n",
      "Epoch 62/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9456 - loss: 0.1377 - val_accuracy: 0.9598 - val_loss: 0.0989 - learning_rate: 0.0010\n",
      "Epoch 63/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9417 - loss: 0.1507 - val_accuracy: 0.9598 - val_loss: 0.1035 - learning_rate: 0.0010\n",
      "Epoch 64/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.9522 - loss: 0.1364\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9517 - loss: 0.1325 - val_accuracy: 0.9696 - val_loss: 0.0893 - learning_rate: 0.0010\n",
      "Epoch 65/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9525 - loss: 0.1228 - val_accuracy: 0.9696 - val_loss: 0.0827 - learning_rate: 5.0000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9512 - loss: 0.1223 - val_accuracy: 0.9706 - val_loss: 0.0865 - learning_rate: 5.0000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9517 - loss: 0.1273 - val_accuracy: 0.9686 - val_loss: 0.0815 - learning_rate: 5.0000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9461 - loss: 0.1232 - val_accuracy: 0.9706 - val_loss: 0.0824 - learning_rate: 5.0000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9561 - loss: 0.1167 - val_accuracy: 0.9696 - val_loss: 0.0891 - learning_rate: 5.0000e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9490 - loss: 0.1265 - val_accuracy: 0.9686 - val_loss: 0.0885 - learning_rate: 5.0000e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9498 - loss: 0.1272 - val_accuracy: 0.9706 - val_loss: 0.0793 - learning_rate: 5.0000e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9551 - loss: 0.1154 - val_accuracy: 0.9735 - val_loss: 0.0771 - learning_rate: 5.0000e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9510 - loss: 0.1290 - val_accuracy: 0.9706 - val_loss: 0.0848 - learning_rate: 5.0000e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.9507 - loss: 0.1209 - val_accuracy: 0.9725 - val_loss: 0.0726 - learning_rate: 5.0000e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9603 - loss: 0.1125 - val_accuracy: 0.9716 - val_loss: 0.0731 - learning_rate: 5.0000e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9566 - loss: 0.1109 - val_accuracy: 0.9775 - val_loss: 0.0725 - learning_rate: 5.0000e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9586 - loss: 0.1102 - val_accuracy: 0.9755 - val_loss: 0.0697 - learning_rate: 5.0000e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9468 - loss: 0.1321 - val_accuracy: 0.9706 - val_loss: 0.0774 - learning_rate: 5.0000e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9532 - loss: 0.1246 - val_accuracy: 0.9686 - val_loss: 0.0800 - learning_rate: 5.0000e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9537 - loss: 0.1180 - val_accuracy: 0.9745 - val_loss: 0.0770 - learning_rate: 5.0000e-04\n",
      "Epoch 81/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.9607 - loss: 0.1138\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9581 - loss: 0.1194 - val_accuracy: 0.9784 - val_loss: 0.0683 - learning_rate: 5.0000e-04\n",
      "Epoch 82/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9569 - loss: 0.1050 - val_accuracy: 0.9755 - val_loss: 0.0705 - learning_rate: 2.5000e-04\n",
      "Epoch 83/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9547 - loss: 0.1100 - val_accuracy: 0.9765 - val_loss: 0.0673 - learning_rate: 2.5000e-04\n",
      "Epoch 84/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9525 - loss: 0.1172 - val_accuracy: 0.9775 - val_loss: 0.0694 - learning_rate: 2.5000e-04\n",
      "Epoch 85/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9571 - loss: 0.1162 - val_accuracy: 0.9745 - val_loss: 0.0688 - learning_rate: 2.5000e-04\n",
      "Epoch 86/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9588 - loss: 0.1068\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9554 - loss: 0.1160 - val_accuracy: 0.9755 - val_loss: 0.0679 - learning_rate: 2.5000e-04\n",
      "Epoch 87/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9564 - loss: 0.1081 - val_accuracy: 0.9765 - val_loss: 0.0691 - learning_rate: 1.2500e-04\n",
      "Epoch 88/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9571 - loss: 0.1140 - val_accuracy: 0.9784 - val_loss: 0.0661 - learning_rate: 1.2500e-04\n",
      "Epoch 89/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9569 - loss: 0.1103 - val_accuracy: 0.9775 - val_loss: 0.0655 - learning_rate: 1.2500e-04\n",
      "Epoch 90/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9626 - loss: 0.1005\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9603 - loss: 0.1091 - val_accuracy: 0.9775 - val_loss: 0.0659 - learning_rate: 1.2500e-04\n",
      "Epoch 91/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9627 - loss: 0.0971 - val_accuracy: 0.9765 - val_loss: 0.0665 - learning_rate: 6.2500e-05\n",
      "Epoch 92/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9620 - loss: 0.1039 - val_accuracy: 0.9784 - val_loss: 0.0647 - learning_rate: 6.2500e-05\n",
      "Epoch 93/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9623 - loss: 0.1004 - val_accuracy: 0.9794 - val_loss: 0.0637 - learning_rate: 6.2500e-05\n",
      "Epoch 94/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9618 - loss: 0.1023 - val_accuracy: 0.9775 - val_loss: 0.0665 - learning_rate: 6.2500e-05\n",
      "Epoch 95/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9654 - loss: 0.0978\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9625 - loss: 0.1042 - val_accuracy: 0.9804 - val_loss: 0.0636 - learning_rate: 6.2500e-05\n",
      "Epoch 96/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9635 - loss: 0.1015 - val_accuracy: 0.9794 - val_loss: 0.0635 - learning_rate: 3.1250e-05\n",
      "Epoch 97/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9598 - loss: 0.1063 - val_accuracy: 0.9794 - val_loss: 0.0634 - learning_rate: 3.1250e-05\n",
      "Epoch 98/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9613 - loss: 0.1060 - val_accuracy: 0.9804 - val_loss: 0.0628 - learning_rate: 3.1250e-05\n",
      "Epoch 99/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.9568 - loss: 0.1020\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9534 - loss: 0.1110 - val_accuracy: 0.9794 - val_loss: 0.0627 - learning_rate: 3.1250e-05\n",
      "Epoch 100/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9632 - loss: 0.1034 - val_accuracy: 0.9784 - val_loss: 0.0629 - learning_rate: 1.5625e-05\n",
      "Epoch 101/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9547 - loss: 0.1193 - val_accuracy: 0.9784 - val_loss: 0.0624 - learning_rate: 1.5625e-05\n",
      "Epoch 102/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9613 - loss: 0.0984 - val_accuracy: 0.9784 - val_loss: 0.0621 - learning_rate: 1.5625e-05\n",
      "Epoch 103/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9650 - loss: 0.0944\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9623 - loss: 0.0982 - val_accuracy: 0.9794 - val_loss: 0.0621 - learning_rate: 1.5625e-05\n",
      "Epoch 104/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9554 - loss: 0.1136 - val_accuracy: 0.9784 - val_loss: 0.0624 - learning_rate: 7.8125e-06\n",
      "Epoch 105/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9618 - loss: 0.0984 - val_accuracy: 0.9794 - val_loss: 0.0625 - learning_rate: 7.8125e-06\n",
      "Epoch 106/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9593 - loss: 0.1035 - val_accuracy: 0.9794 - val_loss: 0.0625 - learning_rate: 7.8125e-06\n",
      "Epoch 107/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9666 - loss: 0.0972\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9610 - loss: 0.1025 - val_accuracy: 0.9794 - val_loss: 0.0624 - learning_rate: 7.8125e-06\n",
      "Epoch 108/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9586 - loss: 0.1014 - val_accuracy: 0.9804 - val_loss: 0.0621 - learning_rate: 3.9063e-06\n",
      "Epoch 109/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9630 - loss: 0.1085 - val_accuracy: 0.9804 - val_loss: 0.0619 - learning_rate: 3.9063e-06\n",
      "Epoch 110/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9637 - loss: 0.1001 - val_accuracy: 0.9804 - val_loss: 0.0617 - learning_rate: 3.9063e-06\n",
      "Epoch 111/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9657 - loss: 0.0928\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9615 - loss: 0.1040 - val_accuracy: 0.9804 - val_loss: 0.0616 - learning_rate: 3.9063e-06\n",
      "Epoch 112/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9574 - loss: 0.1068 - val_accuracy: 0.9804 - val_loss: 0.0617 - learning_rate: 1.9531e-06\n",
      "Epoch 113/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9635 - loss: 0.0987 - val_accuracy: 0.9804 - val_loss: 0.0618 - learning_rate: 1.9531e-06\n",
      "Epoch 114/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9642 - loss: 0.1009 - val_accuracy: 0.9804 - val_loss: 0.0617 - learning_rate: 1.9531e-06\n",
      "Epoch 115/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9631 - loss: 0.0957\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9615 - loss: 0.1008 - val_accuracy: 0.9804 - val_loss: 0.0617 - learning_rate: 1.9531e-06\n",
      "Epoch 116/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9632 - loss: 0.0999 - val_accuracy: 0.9804 - val_loss: 0.0618 - learning_rate: 1.0000e-06\n",
      "Epoch 117/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9657 - loss: 0.0973 - val_accuracy: 0.9804 - val_loss: 0.0618 - learning_rate: 1.0000e-06\n",
      "Epoch 118/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9603 - loss: 0.1064 - val_accuracy: 0.9804 - val_loss: 0.0618 - learning_rate: 1.0000e-06\n",
      "Epoch 119/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9598 - loss: 0.1002 - val_accuracy: 0.9804 - val_loss: 0.0618 - learning_rate: 1.0000e-06\n",
      "Epoch 120/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9615 - loss: 0.1032 - val_accuracy: 0.9804 - val_loss: 0.0617 - learning_rate: 1.0000e-06\n",
      "Epoch 121/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9596 - loss: 0.1086 - val_accuracy: 0.9804 - val_loss: 0.0617 - learning_rate: 1.0000e-06\n",
      "Epoch 122/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9596 - loss: 0.1034 - val_accuracy: 0.9804 - val_loss: 0.0618 - learning_rate: 1.0000e-06\n",
      "Epoch 123/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9591 - loss: 0.1064 - val_accuracy: 0.9804 - val_loss: 0.0618 - learning_rate: 1.0000e-06\n",
      "Epoch 124/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9637 - loss: 0.0959 - val_accuracy: 0.9804 - val_loss: 0.0618 - learning_rate: 1.0000e-06\n",
      "Epoch 125/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9586 - loss: 0.1078 - val_accuracy: 0.9804 - val_loss: 0.0617 - learning_rate: 1.0000e-06\n",
      "Epoch 126/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9596 - loss: 0.1017 - val_accuracy: 0.9804 - val_loss: 0.0617 - learning_rate: 1.0000e-06\n",
      "Epoch 126: early stopping\n",
      "Restoring model weights from the end of the best epoch: 111.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] Model saved as cole-final.h5\n",
      "\n",
      "[CONFUSION MATRIX]\n",
      "[[473   7]\n",
      " [ 13 527]]\n",
      "[VAL] acc=0.9804, prec=0.9869, rec=0.9759, f1=0.9814\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 4: 80:20 Train / Validation Split (EEGNet)\n",
    "# ====================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, SeparableConv2D, BatchNormalization, Activation,\n",
    "    DepthwiseConv2D, Conv2D, AveragePooling2D,\n",
    "    Dropout, Dense, Flatten\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# --------------------------------------------------------------------\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# EEGNet model\n",
    "# --------------------------------------------------------------------\n",
    "def create_eegnet_model(input_shape, dropout_rate=0.5, num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(16, (1, 64), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = DepthwiseConv2D(\n",
    "        (input_shape[0], 1),\n",
    "        depth_multiplier=2,\n",
    "        padding='valid',\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 4))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = SeparableConv2D(16, (1, 16), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 8))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Compile model\n",
    "# --------------------------------------------------------------------\n",
    "def compile_model():\n",
    "    input_shape = (Xtr.shape[1], Xtr.shape[2], Xtr.shape[3])\n",
    "    model = create_eegnet_model(input_shape)\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-3),\n",
    "        loss=BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[BinaryAccuracy(name=\"accuracy\", threshold=0.5)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def make_callbacks():\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"loss\",\n",
    "            factor=0.5,\n",
    "            patience=4,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Use ALL normalized training data\n",
    "# --------------------------------------------------------------------\n",
    "Xtr = X_train_norm\n",
    "ytr = y_train_norm\n",
    "\n",
    "print(f\"[TRAIN] Xtr: {Xtr.shape}, ytr: {ytr.shape}\")\n",
    "print(\"[INFO] Test / LOSO subject is NOT used here.\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 80:20 Stratified Split\n",
    "# --------------------------------------------------------------------\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(sss.split(Xtr, ytr))\n",
    "\n",
    "X_train, X_val = Xtr[train_idx], Xtr[val_idx]\n",
    "y_train, y_val = ytr[train_idx], ytr[val_idx]\n",
    "\n",
    "print(f\"[SPLIT] Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Train model\n",
    "# --------------------------------------------------------------------\n",
    "model = compile_model()\n",
    "callbacks = make_callbacks()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Save final model\n",
    "# --------------------------------------------------------------------\n",
    "model.save(\"amin1-final.h5\")\n",
    "print(\"[SAVE] Model saved as cole-final.h5\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Validation Confusion Matrix\n",
    "# --------------------------------------------------------------------\n",
    "y_prob = model.predict(X_val, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "y_true = y_val.astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "eps = 1e-12\n",
    "\n",
    "acc  = (tp + tn) / max(tp + tn + fp + fn, eps)\n",
    "prec = tp / max(tp + fp, eps)\n",
    "rec  = tp / max(tp + fn, eps)\n",
    "f1   = 2 * prec * rec / max(prec + rec, eps)\n",
    "\n",
    "print(\"\\n[CONFUSION MATRIX]\")\n",
    "print(cm)\n",
    "print(f\"[VAL] acc={acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e2a13-6367-41ca-be66-a43594c2940e",
   "metadata": {},
   "source": [
    "Cell 8 — Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ac6662-33c1-4e25-b6c5-d47c860fba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Preparing TEST data...\n",
      "[TEST] Xte shape: (300, 14, 128, 1)\n",
      "[TEST] yte shape: (300,), class distribution: (array([0]), array([300]))\n",
      "[TEST] Loading final model: amin1-final.h5\n",
      "[TEST] Running inference on TEST subject...\n",
      "[TEST] Mean predicted probability: 0.2844\n",
      "\n",
      "[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[223  77]\n",
      " [  0   0]]\n",
      "Accuracy : 0.7433\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-score : 0.0000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Inference on TEST subject (adam) using FINAL EEGNet\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Prepare TEST data\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Preparing TEST data...\")\n",
    "\n",
    "Xte = X_test_norm\n",
    "yte = y_test_norm.astype(int)\n",
    "\n",
    "print(f\"[TEST] Xte shape: {Xte.shape}\")\n",
    "print(f\"[TEST] yte shape: {yte.shape}, class distribution: {np.unique(yte, return_counts=True)}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Load FINAL trained model\n",
    "# ------------------------------------------------------------------\n",
    "model_path = \"amin1-final.h5\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        \"Final model 'cole-final.h5' not found. \"\n",
    "        \"Please run Cell 4 (80:20 training) first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[TEST] Loading final model: {model_path}\")\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Run inference on TEST subject\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Running inference on TEST subject...\")\n",
    "\n",
    "y_prob = model.predict(Xte, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(f\"[TEST] Mean predicted probability: {y_prob.mean():.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Metrics on TEST subject (segment-level)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc = accuracy_score(yte, y_pred)\n",
    "prec = precision_score(yte, y_pred, zero_division=0)\n",
    "rec = recall_score(yte, y_pred, zero_division=0)\n",
    "f1 = f1_score(yte, y_pred, zero_division=0)\n",
    "\n",
    "print(\"\\n[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572dbf3e-bbb5-4865-883f-30e374700949",
   "metadata": {},
   "source": [
    "Cell 9 - Performance Matrices for each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a57240-50ea-4736-a7ac-52ef7878b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] Verifying predictions and labels are available...\n",
      "[CHECK] Found predictions for 300 test segments.\n",
      "\n",
      "[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[223  77]\n",
      " [  0   0]]\n",
      "Accuracy : 0.7433\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-score : 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Final model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================\n",
    "# Cell 6: Confusion Matrix & Metrics on TEST (FINAL EEGNet)\n",
    "# =======================================================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# ------------------------------------------------------------------\n",
    "def compute_metrics_from_cm(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    eps = 1e-12\n",
    "    acc  = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec  = tp / (tp + fn + eps)\n",
    "    f1   = 2 * prec * rec / (prec + rec + eps)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], ['0', '1'])\n",
    "    plt.yticks([0, 1], ['0', '1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(\n",
    "                j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Validate upstream artifacts\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[CHECK] Verifying predictions and labels are available...\")\n",
    "\n",
    "if 'y_pred' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Predictions 'y_pred' not found. \"\n",
    "        \"Please run Cell 5 (final model inference) first.\"\n",
    "    )\n",
    "\n",
    "if 'yte' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Test labels 'yte' not found. \"\n",
    "        \"Please run Cell 5 first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[CHECK] Found predictions for {len(y_pred)} test segments.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Confusion Matrix & Metrics (FINAL model)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc, prec, rec, f1 = compute_metrics_from_cm(cm)\n",
    "\n",
    "print(\"\\n[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(cm, title=\"Confusion Matrix - FINAL EEGNet\")\n",
    "\n",
    "print(\"[DONE] Final model evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0f120-39fd-497e-9098-05e6c82f4807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
