{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af5b471-cea9-4194-bc23-da5cdc2f9c67",
   "metadata": {},
   "source": [
    "Cell 1 — Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64aaa897-ab61-4eb4-a558-4c078acf751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Base path set to:\n",
      "C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\n",
      "[INFO] Found 18 folders:\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Selected test subject (LOSO): Mina 3\n",
      "[INFO] Training subjects (17):\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Sampling rate (SFREQ) = 128.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 1: Setup & discovery\n",
    "# =========================\n",
    "\n",
    "import os  # file ops\n",
    "import glob  # file matching\n",
    "import numpy as np  # arrays\n",
    "import random  # seeds\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data path\n",
    "BASE_PATH = r\"C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\"\n",
    "print(f\"[INFO] Base path set to:\\n{BASE_PATH}\")\n",
    "\n",
    "# List subject folders\n",
    "all_folders = [p for p in sorted(os.listdir(BASE_PATH)) if os.path.isdir(os.path.join(BASE_PATH, p))]\n",
    "print(f\"[INFO] Found {len(all_folders)} folders:\\n{all_folders}\")\n",
    "\n",
    "# Expected count (change if needed)\n",
    "assert len(all_folders) == 18, f\"Expected 18 folders, found {len(all_folders)}.\"\n",
    "\n",
    "# LOSO test subject\n",
    "TEST_SUBJECT = \"Mina 3\"\n",
    "print(f\"[INFO] Selected test subject (LOSO): {TEST_SUBJECT}\")\n",
    "\n",
    "# Train subjects = remaining\n",
    "train_subjects = [s for s in all_folders if s.lower() != TEST_SUBJECT.lower()]\n",
    "print(f\"[INFO] Training subjects ({len(train_subjects)}):\\n{train_subjects}\")\n",
    "\n",
    "# Sampling rate\n",
    "SFREQ = 128.0\n",
    "print(f\"[INFO] Sampling rate (SFREQ) = {SFREQ} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e649681-2861-4ce2-bbd1-0d9dd3359f42",
   "metadata": {},
   "source": [
    "Cell 2 — Load EEG + labels per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943c5c6e-a7aa-4618-899c-daf341c25358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 2] Loading data from folders...\n",
      "[STEP 2]       Amin | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Amin | EEG=          Amin.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Amin1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]      Amin1 | EEG=         Amin1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Cole | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Cole | EEG=          Cole.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Daniel | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]     Daniel | EEG=        Daniel.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]    Ismayil | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]    Ismayil | EEG=       Ismayil.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Jack | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Jack | EEG=          Jack.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]      James | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      James | EEG=         James.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Josh | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Josh | EEG=          Josh.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Marjan | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Marjan | EEG=        Marjan.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]        Max | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Max | EEG=           Max.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Mina | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mina | EEG=          Mina.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 1 | EEG=        Mina 1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 3 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 3 | EEG=        Mina 3.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]   Mohammad | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]   Mohammad | EEG=      Mohammad.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Mona | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mona | EEG=          Mona.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Roddy | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      Roddy | EEG=         Roddy.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]        Sam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Sam | EEG=           Sam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       adam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       adam | EEG=          adam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2] Loading complete ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 2: Load data (EMPTY LABEL => CLASS 0)\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path  # helper\n",
    "\n",
    "def load_subject(subj_dir: str, subj_name: str, expected_channels: int = 14):\n",
    "    \"\"\"\n",
    "    EEG file: {subj_name}.npy\n",
    "    Label file: {subj_name}_target.npy\n",
    "    Label rule:\n",
    "      - empty array -> class 0\n",
    "      - [0] -> class 0\n",
    "      - [1] -> class 1\n",
    "    \"\"\"\n",
    "\n",
    "    # EEG file path\n",
    "    eeg_file = os.path.join(subj_dir, f\"{subj_name}.npy\")\n",
    "    assert os.path.exists(eeg_file), f\"[ERROR] Missing EEG file: {eeg_file}\"\n",
    "\n",
    "    # Label file path\n",
    "    target_file = os.path.join(subj_dir, f\"{subj_name}_target.npy\")\n",
    "    assert os.path.exists(target_file), f\"[ERROR] Missing label file: {target_file}\"\n",
    "\n",
    "    # Load EEG\n",
    "    X_raw = np.load(eeg_file, allow_pickle=True)\n",
    "    X_raw = np.asarray(X_raw)\n",
    "\n",
    "    # Standardize EEG to (n_trials, 14, T)\n",
    "    if X_raw.ndim == 2:\n",
    "        X_trials = X_raw[None, :, :]\n",
    "    elif X_raw.ndim == 3:\n",
    "        dims = list(X_raw.shape)\n",
    "        chan_axes = [i for i, d in enumerate(dims) if d == expected_channels]\n",
    "        chan_axis = chan_axes[0] if len(chan_axes) == 1 else 1\n",
    "        time_axis = int(np.argmax(dims))\n",
    "        trial_axis = [i for i in range(3) if i not in (chan_axis, time_axis)][0]\n",
    "        X_trials = np.moveaxis(X_raw, (trial_axis, chan_axis, time_axis), (0, 1, 2))\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] Unsupported EEG shape {X_raw.shape} in {eeg_file}\")\n",
    "\n",
    "    assert X_trials.shape[1] == expected_channels, f\"[ERROR] Expected {expected_channels} channels, got {X_trials.shape[1]} in {eeg_file}\"\n",
    "\n",
    "    # Load label\n",
    "    y_raw = np.load(target_file, allow_pickle=True)\n",
    "    y_arr = np.asarray(y_raw).ravel()\n",
    "\n",
    "    # Apply your rule\n",
    "    if y_arr.size == 0:\n",
    "        label = 0\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\")\n",
    "    else:\n",
    "        label = int(y_arr[0])\n",
    "        if label not in (0, 1):\n",
    "            raise ValueError(f\"[ERROR] Invalid label {label} in {target_file} (must be 0 or 1)\")\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape={y_arr.shape}, value={y_arr} -> class={label}\")\n",
    "\n",
    "    # Repeat label for all trials\n",
    "    y_trials = np.full((X_trials.shape[0],), label, dtype=np.int32)\n",
    "\n",
    "    return X_trials.astype(np.float32), y_trials.astype(np.int32), eeg_file, target_file\n",
    "\n",
    "\n",
    "# Load all subjects\n",
    "subject_data = {}\n",
    "print(\"[STEP 2] Loading data from folders...\")\n",
    "\n",
    "for subj in all_folders:\n",
    "    subj_dir = os.path.join(BASE_PATH, subj)\n",
    "    X_trials, y_trials, eeg_file, target_file = load_subject(subj_dir, subj)\n",
    "\n",
    "    subject_data[subj] = (X_trials, y_trials)\n",
    "\n",
    "    print(f\"[STEP 2] {subj:>10} | EEG={Path(eeg_file).name:>18} | X={X_trials.shape} | y={y_trials.shape} | class={y_trials[0]}\")\n",
    "\n",
    "print(\"[STEP 2] Loading complete ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd93ca8-a94c-485c-8a61-e6e7441f6299",
   "metadata": {},
   "source": [
    "Cell 3 — Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733e592c-98e1-4cbd-afd2-6fdc8cf407f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 3] Preprocessing functions ready ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3: Preprocessing functions\n",
    "# =========================\n",
    "\n",
    "import mne  # EEG processing\n",
    "import pywt  # wavelets\n",
    "from sklearn.decomposition import FastICA  # ICA\n",
    "from typing import Optional, Tuple, Union, Sequence, Dict, List  # typing\n",
    "\n",
    "def wavelet_enhanced_ica(\n",
    "    data: np.ndarray,\n",
    "    n_components: int = 10,\n",
    "    wavelet: str = \"db4\",\n",
    "    level: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> np.ndarray:\n",
    "    n_ch, n_t = data.shape\n",
    "    n_components = min(n_components, n_ch)\n",
    "\n",
    "    coeffs = pywt.wavedec(data, wavelet=wavelet, level=level, axis=1)\n",
    "    A = coeffs[0]\n",
    "\n",
    "    ica = FastICA(n_components=n_components, random_state=random_state)\n",
    "    S = ica.fit_transform(A.T).T\n",
    "    A_denoised = ica.inverse_transform(S.T).T\n",
    "\n",
    "    coeffs[0] = A_denoised\n",
    "    cleaned = pywt.waverec(coeffs, wavelet=wavelet, axis=1)\n",
    "\n",
    "    if cleaned.shape[1] != n_t:\n",
    "        if cleaned.shape[1] > n_t:\n",
    "            cleaned = cleaned[:, :n_t]\n",
    "        else:\n",
    "            cleaned = np.pad(cleaned, ((0, 0), (0, n_t - cleaned.shape[1])), mode=\"constant\")\n",
    "    return cleaned\n",
    "\n",
    "def _names_from_index_mapping(n_channels: int, index_to_name: Optional[Dict[int, str]]) -> List[str]:\n",
    "    if index_to_name is None:\n",
    "        return [f\"EEG{i+1}\" for i in range(n_channels)]\n",
    "    keys = list(index_to_name.keys())\n",
    "    is_zero_based = (0 in keys) and (1 not in keys)\n",
    "    names = []\n",
    "    for i in range(n_channels):\n",
    "        key = i if is_zero_based else (i + 1)\n",
    "        names.append(index_to_name.get(key, f\"EEG{i+1}\"))\n",
    "    return names\n",
    "\n",
    "def preprocess_eeg(\n",
    "    eeg: np.ndarray,\n",
    "    sfreq: float,\n",
    "    *,\n",
    "    index_to_name: Optional[Dict[int, str]] = None,\n",
    "    use_standard_1010: bool = True,\n",
    "    resample_to: Optional[float] = None,\n",
    "    notch_freqs: Union[None, float, Sequence[float]] = 50.0,\n",
    "    highpass: Optional[float] = 0.05,\n",
    "    bad_point_z: float = 6.0,\n",
    "    bad_channel_z: float = 5.0,\n",
    "    interpolate_bad_channels: bool = True,\n",
    "    car: bool = True,\n",
    "    use_wica: bool = True,\n",
    "    wica_components: int = 10,\n",
    "    wica_wavelet: str = \"db4\",\n",
    "    wica_level: int = 3,\n",
    "    wica_random_state: int = 42,\n",
    "    return_raw: bool = False,\n",
    "):\n",
    "\n",
    "    eeg = np.asarray(eeg, dtype=np.float32)\n",
    "    assert eeg.ndim == 2, \"eeg must be 2D: (n_channels, n_times)\"\n",
    "\n",
    "    n_channels, _ = eeg.shape\n",
    "    ch_names = _names_from_index_mapping(n_channels, index_to_name)\n",
    "    ch_types = ['eog' if str(n).upper().startswith(\"EOG\") else 'eeg' for n in ch_names]\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    raw = mne.io.RawArray(eeg, info, verbose=False)\n",
    "\n",
    "    montage_applied = False\n",
    "    if use_standard_1010:\n",
    "        try:\n",
    "            mont = mne.channels.make_standard_montage(\"standard_1010\")\n",
    "            raw.set_montage(mont, match_case=False, on_missing=\"ignore\")\n",
    "            montage_applied = True\n",
    "        except Exception:\n",
    "            montage_applied = False\n",
    "\n",
    "    if resample_to is not None and float(resample_to) != float(sfreq):\n",
    "        raw.resample(sfreq=resample_to, npad=\"auto\")\n",
    "    sfreq_out = float(raw.info[\"sfreq\"])\n",
    "\n",
    "    if notch_freqs is not None:\n",
    "        raw.notch_filter(freqs=notch_freqs, verbose=False)\n",
    "    if highpass is not None:\n",
    "        raw.filter(l_freq=highpass, h_freq=None, verbose=False)\n",
    "\n",
    "    X = raw.get_data()\n",
    "    mu = np.mean(X, axis=1, keepdims=True)\n",
    "    sd = np.std(X, axis=1, keepdims=True) + 1e-12\n",
    "    hi = mu + bad_point_z * sd\n",
    "    lo = mu - bad_point_z * sd\n",
    "    bad_idx = (X > hi) | (X < lo)\n",
    "\n",
    "    if np.any(bad_idx):\n",
    "        X_fixed = X.copy()\n",
    "        t = np.arange(X.shape[1], dtype=float)\n",
    "        for ch in range(n_channels):\n",
    "            mask = bad_idx[ch]\n",
    "            if mask.any():\n",
    "                good = ~mask\n",
    "                if good.sum() >= 2:\n",
    "                    X_fixed[ch, mask] = np.interp(t[mask], t[good], X_fixed[ch, good])\n",
    "        raw._data = X_fixed\n",
    "\n",
    "    if interpolate_bad_channels and montage_applied:\n",
    "        X = raw.get_data(picks=\"eeg\")\n",
    "        if X.size > 0:\n",
    "            ch_std = X.std(axis=1)\n",
    "            med = np.median(ch_std)\n",
    "            mad = np.median(np.abs(ch_std - med)) + 1e-12\n",
    "            z = 0.6745 * (ch_std - med) / mad\n",
    "            eeg_names = mne.pick_info(raw.info, mne.pick_types(raw.info, eeg=True)).ch_names\n",
    "            bads = [eeg_names[i] for i in np.where(np.abs(z) > bad_channel_z)[0]]\n",
    "            raw.info[\"bads\"] = bads\n",
    "            if bads:\n",
    "                raw.interpolate_bads(reset_bads=True, verbose=False)\n",
    "\n",
    "    if car:\n",
    "        raw.set_eeg_reference(\"average\", projection=True)\n",
    "        raw.apply_proj()\n",
    "\n",
    "    if use_wica:\n",
    "        cleaned = wavelet_enhanced_ica(\n",
    "            raw.get_data(),\n",
    "            n_components=wica_components,\n",
    "            wavelet=wica_wavelet,\n",
    "            level=wica_level,\n",
    "            random_state=wica_random_state\n",
    "        )\n",
    "        raw = mne.io.RawArray(cleaned, raw.info, verbose=False)\n",
    "\n",
    "    cleaned = raw.get_data()\n",
    "    return cleaned, sfreq_out\n",
    "\n",
    "print(\"[STEP 3] Preprocessing functions ready ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a9d20-889d-45c4-bbc2-de62216908cd",
   "metadata": {},
   "source": [
    "Cell 4 — Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bfcbde0-1cc2-409c-8627-93da46606661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 4] Starting preprocessing...\n",
      "[INFO] USE_WICA = True\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 01/17: Amin | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 02/17: Amin1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 03/17: Cole | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 04/17: Daniel | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 05/17: Ismayil | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 06/17: Jack | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 07/17: James | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 08/17: Josh | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 09/17: Marjan | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 10/17: Max | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 11/17: Mina | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 12/17: Mina 1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 13/17: Mohammad | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 14/17: Mona | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 15/17: Roddy | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 16/17: Sam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 17/17: adam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Test subject: Mina 3 | trials=1 | fs_out=128.0\n",
      "[STEP 4] Preprocessing done ✅\n",
      "  X_train_trials_all: (17, 14, 38400)\n",
      "  y_train_trials_all: (17,) (array([0, 1], dtype=int32), array([8, 9]))\n",
      "  X_test_trials_all : (1, 14, 38400)\n",
      "  y_test_trials_all : (1,) (array([0], dtype=int32), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 4: Apply preprocessing (LOSO)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 4] Starting preprocessing...\")\n",
    "\n",
    "USE_WICA = True\n",
    "print(f\"[INFO] USE_WICA = {USE_WICA}\")\n",
    "\n",
    "X_train_trials_all, y_train_trials_all = [], []\n",
    "X_test_trials_all, y_test_trials_all = [], []\n",
    "\n",
    "def preprocess_trials(X_trials, y_trials):\n",
    "    cleaned_list = []\n",
    "    label_list = []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        trial = X_trials[i]\n",
    "        label = int(y_trials[i])\n",
    "\n",
    "        cleaned, fs_out = preprocess_eeg(\n",
    "            eeg=trial,\n",
    "            sfreq=SFREQ,\n",
    "            use_wica=USE_WICA\n",
    "        )\n",
    "\n",
    "        cleaned_list.append(cleaned.astype(np.float32, copy=False))\n",
    "        label_list.append(label)\n",
    "\n",
    "    return cleaned_list, np.array(label_list, dtype=np.int32), fs_out\n",
    "\n",
    "# Train subjects\n",
    "for idx, subj in enumerate(train_subjects, start=1):\n",
    "    X_trials, y_trials = subject_data[subj]\n",
    "    cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "    X_train_trials_all.extend(cleaned_trials)\n",
    "    y_train_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "    print(f\"[STEP 4] Train {idx:02d}/{len(train_subjects)}: {subj} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Test subject\n",
    "X_trials, y_trials = subject_data[TEST_SUBJECT]\n",
    "cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "X_test_trials_all.extend(cleaned_trials)\n",
    "y_test_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "print(f\"[STEP 4] Test subject: {TEST_SUBJECT} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Convert to arrays\n",
    "X_train_trials_all = np.array(X_train_trials_all, dtype=np.float32)\n",
    "y_train_trials_all = np.array(y_train_trials_all, dtype=np.int32)\n",
    "\n",
    "X_test_trials_all = np.array(X_test_trials_all, dtype=np.float32)\n",
    "y_test_trials_all = np.array(y_test_trials_all, dtype=np.int32)\n",
    "\n",
    "print(\"[STEP 4] Preprocessing done ✅\")\n",
    "print(\"  X_train_trials_all:\", X_train_trials_all.shape)\n",
    "print(\"  y_train_trials_all:\", y_train_trials_all.shape, np.unique(y_train_trials_all, return_counts=True))\n",
    "print(\"  X_test_trials_all :\", X_test_trials_all.shape)\n",
    "print(\"  y_test_trials_all :\", y_test_trials_all.shape, np.unique(y_test_trials_all, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc3da1-2967-4ee3-84cd-bfcf0f98c0b1",
   "metadata": {},
   "source": [
    "Cell 5 — Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c21ad54-7001-4039-9360-29dfa2905127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 5] Starting segmentation...\n",
      "[STEP 5] Segmented 5/17 trials...\n",
      "[STEP 5] Segmented 10/17 trials...\n",
      "[STEP 5] Segmented 15/17 trials...\n",
      "[STEP 5] Segmented 17/17 trials...\n",
      "[STEP 5] Segmented 1/1 trials...\n",
      "[STEP 5] Segmentation done ✅\n",
      "  X_train: (5100, 14, 128, 1)\n",
      "  y_train: (5100,) (array([0, 1], dtype=int32), array([2400, 2700]))\n",
      "  X_test : (300, 14, 128, 1)\n",
      "  y_test : (300,) (array([0], dtype=int32), array([300]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 5: Segmentation\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 5] Starting segmentation...\")\n",
    "\n",
    "def segment_trial(trial_2d: np.ndarray, label: int, segment_len: int = 128):\n",
    "    C, T = trial_2d.shape\n",
    "    n_segs = T // segment_len\n",
    "    use_T = n_segs * segment_len\n",
    "\n",
    "    if n_segs == 0:\n",
    "        return np.empty((0, C, segment_len), dtype=np.float32), np.empty((0,), dtype=np.int32)\n",
    "\n",
    "    trial_trim = trial_2d[:, :use_T]\n",
    "    Xsegs = trial_trim.reshape(C, n_segs, segment_len).transpose(1, 0, 2)\n",
    "    ysegs = np.full((n_segs,), int(label), dtype=np.int32)\n",
    "    return Xsegs.astype(np.float32, copy=False), ysegs\n",
    "\n",
    "def segment_dataset(X_trials: np.ndarray, y_trials: np.ndarray, segment_len: int = 128):\n",
    "    X_all, y_all = [], []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        Xsegs, ysegs = segment_trial(X_trials[i], int(y_trials[i]), segment_len=segment_len)\n",
    "        X_all.append(Xsegs)\n",
    "        y_all.append(ysegs)\n",
    "\n",
    "        if (i + 1) % 5 == 0 or (i + 1) == X_trials.shape[0]:\n",
    "            print(f\"[STEP 5] Segmented {i+1}/{X_trials.shape[0]} trials...\")\n",
    "\n",
    "    X_out = np.concatenate(X_all, axis=0)\n",
    "    y_out = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    X_out = np.expand_dims(X_out, axis=-1)  # (N,14,128,1)\n",
    "    return X_out.astype(np.float32), y_out.astype(np.int32)\n",
    "\n",
    "X_train, y_train = segment_dataset(X_train_trials_all, y_train_trials_all, segment_len=128)\n",
    "X_test,  y_test  = segment_dataset(X_test_trials_all,  y_test_trials_all,  segment_len=128)\n",
    "\n",
    "print(\"[STEP 5] Segmentation done ✅\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  y_train:\", y_train.shape, np.unique(y_train, return_counts=True))\n",
    "print(\"  X_test :\", X_test.shape)\n",
    "print(\"  y_test :\", y_test.shape, np.unique(y_test, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b4ffd5-bf2f-490e-8854-0a2fc747f1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e69ba7-3952-47b1-bd80-f05237b2e018",
   "metadata": {},
   "source": [
    "Cell 6 — Split train/val + Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97fda4ae-e1c8-4fd1-bf0c-c82ae2a85764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 6] Normalizing using ALL training data (no train/val split)...\n",
      "[STEP 6] Normalization done ✅\n",
      "  X_train_norm: (5100, 14, 128, 1)\n",
      "  X_test_norm : (300, 14, 128, 1)\n",
      "  y_train_norm: (5100,)\n",
      "  y_test_norm : (300,)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 6: Normalize (ALL training data)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 6] Normalizing using ALL training data (no train/val split)...\")\n",
    "\n",
    "def normalize_eeg(train_data: np.ndarray, test_data: np.ndarray):\n",
    "    if train_data.ndim == 5:\n",
    "        train_flat = train_data.reshape(-1, *train_data.shape[2:])\n",
    "        test_flat  = test_data.reshape(-1, *test_data.shape[2:])\n",
    "    else:\n",
    "        train_flat, test_flat = train_data, test_data\n",
    "\n",
    "    # Compute stats on ALL training data\n",
    "    train_mean = np.mean(train_flat, axis=(0, 2, 3), keepdims=True)\n",
    "    train_std  = np.std(train_flat,  axis=(0, 2, 3), keepdims=True) + 1e-8\n",
    "\n",
    "    train_norm = (train_flat - train_mean) / train_std\n",
    "    test_norm  = (test_flat  - train_mean) / train_std\n",
    "\n",
    "    if train_data.ndim == 5:\n",
    "        train_norm = train_norm.reshape(train_data.shape)\n",
    "        test_norm  = test_norm.reshape(test_data.shape)\n",
    "\n",
    "    return (\n",
    "        train_norm.astype(np.float32),\n",
    "        test_norm.astype(np.float32),\n",
    "        train_mean,\n",
    "        train_std\n",
    "    )\n",
    "\n",
    "# Normalize ALL training data\n",
    "X_train_norm, X_test_norm, mean, std = normalize_eeg(X_train, X_test)\n",
    "\n",
    "y_train_norm = y_train.astype(np.float32)\n",
    "y_test_norm  = y_test.astype(np.float32)\n",
    "\n",
    "print(\"[STEP 6] Normalization done ✅\")\n",
    "print(\"  X_train_norm:\", X_train_norm.shape)\n",
    "print(\"  X_test_norm :\", X_test_norm.shape)\n",
    "print(\"  y_train_norm:\", y_train_norm.shape)\n",
    "print(\"  y_test_norm :\", y_test_norm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f527c-34db-4c23-bdbd-c8ce03b519f0",
   "metadata": {},
   "source": [
    "Cell 7 — EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ea4279-53c2-49c1-b834-7b1e1c20775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\DL\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Xtr: (5100, 14, 128, 1), ytr: (5100,)\n",
      "[INFO] Test / LOSO subject is NOT used here.\n",
      "[SPLIT] Train: (4080, 14, 128, 1), Val: (1020, 14, 128, 1)\n",
      "Epoch 1/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 148ms/step - accuracy: 0.5343 - loss: 0.7215 - val_accuracy: 0.4725 - val_loss: 0.6956 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.5583 - loss: 0.6807 - val_accuracy: 0.4765 - val_loss: 0.6948 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.5816 - loss: 0.6596 - val_accuracy: 0.4931 - val_loss: 0.6919 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.6275 - loss: 0.6302 - val_accuracy: 0.5529 - val_loss: 0.6862 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.6615 - loss: 0.5994 - val_accuracy: 0.6039 - val_loss: 0.6812 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.6855 - loss: 0.5723 - val_accuracy: 0.6873 - val_loss: 0.6641 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.7181 - loss: 0.5373 - val_accuracy: 0.7176 - val_loss: 0.6512 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.7444 - loss: 0.5144 - val_accuracy: 0.6990 - val_loss: 0.6336 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.7605 - loss: 0.4836 - val_accuracy: 0.6196 - val_loss: 0.6296 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.7686 - loss: 0.4638 - val_accuracy: 0.6373 - val_loss: 0.6122 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7895 - loss: 0.4393 - val_accuracy: 0.6245 - val_loss: 0.6118 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.7971 - loss: 0.4324 - val_accuracy: 0.6451 - val_loss: 0.5959 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8049 - loss: 0.4203 - val_accuracy: 0.6451 - val_loss: 0.5795 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8103 - loss: 0.4060 - val_accuracy: 0.6402 - val_loss: 0.5849 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.8245 - loss: 0.4004 - val_accuracy: 0.6824 - val_loss: 0.5492 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.8306 - loss: 0.3787 - val_accuracy: 0.6745 - val_loss: 0.5364 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.8338 - loss: 0.3687 - val_accuracy: 0.6765 - val_loss: 0.5342 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.8267 - loss: 0.3944 - val_accuracy: 0.6912 - val_loss: 0.5137 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.8449 - loss: 0.3541 - val_accuracy: 0.6912 - val_loss: 0.5200 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.8461 - loss: 0.3398 - val_accuracy: 0.7412 - val_loss: 0.4513 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8566 - loss: 0.3310 - val_accuracy: 0.7294 - val_loss: 0.4451 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8571 - loss: 0.3418 - val_accuracy: 0.7775 - val_loss: 0.4466 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8485 - loss: 0.3468 - val_accuracy: 0.7814 - val_loss: 0.4006 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.8561 - loss: 0.3419 - val_accuracy: 0.7598 - val_loss: 0.4399 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.8679 - loss: 0.3087 - val_accuracy: 0.7804 - val_loss: 0.3994 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.8735 - loss: 0.3110 - val_accuracy: 0.8500 - val_loss: 0.3253 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.8755 - loss: 0.2994 - val_accuracy: 0.8127 - val_loss: 0.3579 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.8755 - loss: 0.3009 - val_accuracy: 0.8745 - val_loss: 0.2872 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.8843 - loss: 0.2860 - val_accuracy: 0.8765 - val_loss: 0.2765 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8993 - loss: 0.2564 - val_accuracy: 0.8725 - val_loss: 0.2638 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8877 - loss: 0.2700 - val_accuracy: 0.8755 - val_loss: 0.2646 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8748 - loss: 0.3098 - val_accuracy: 0.9353 - val_loss: 0.2070 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8926 - loss: 0.2700 - val_accuracy: 0.9088 - val_loss: 0.2258 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8940 - loss: 0.2618\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.8946 - loss: 0.2576 - val_accuracy: 0.9520 - val_loss: 0.1659 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.9105 - loss: 0.2242 - val_accuracy: 0.9559 - val_loss: 0.1529 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9093 - loss: 0.2356 - val_accuracy: 0.9657 - val_loss: 0.1364 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.9118 - loss: 0.2277 - val_accuracy: 0.9686 - val_loss: 0.1303 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9186 - loss: 0.2147 - val_accuracy: 0.9696 - val_loss: 0.1207 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.9169 - loss: 0.2163 - val_accuracy: 0.9676 - val_loss: 0.1252 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9208 - loss: 0.2001 - val_accuracy: 0.9637 - val_loss: 0.1249 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9150 - loss: 0.2166 - val_accuracy: 0.9765 - val_loss: 0.1046 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9225 - loss: 0.1954 - val_accuracy: 0.9794 - val_loss: 0.0953 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9255 - loss: 0.1940 - val_accuracy: 0.9784 - val_loss: 0.0945 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9331 - loss: 0.1784 - val_accuracy: 0.9725 - val_loss: 0.0999 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9279 - loss: 0.1914 - val_accuracy: 0.9775 - val_loss: 0.0906 - learning_rate: 5.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9279 - loss: 0.1910 - val_accuracy: 0.9755 - val_loss: 0.0965 - learning_rate: 5.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9250 - loss: 0.2019 - val_accuracy: 0.9725 - val_loss: 0.0967 - learning_rate: 5.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9118 - loss: 0.2069\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9213 - loss: 0.1906 - val_accuracy: 0.9667 - val_loss: 0.0976 - learning_rate: 5.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9309 - loss: 0.1775 - val_accuracy: 0.9735 - val_loss: 0.0891 - learning_rate: 2.5000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9370 - loss: 0.1663 - val_accuracy: 0.9745 - val_loss: 0.0847 - learning_rate: 2.5000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9370 - loss: 0.1736 - val_accuracy: 0.9745 - val_loss: 0.0867 - learning_rate: 2.5000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9373 - loss: 0.1688 - val_accuracy: 0.9794 - val_loss: 0.0789 - learning_rate: 2.5000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9390 - loss: 0.1578 - val_accuracy: 0.9745 - val_loss: 0.0813 - learning_rate: 2.5000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9341 - loss: 0.1745 - val_accuracy: 0.9745 - val_loss: 0.0838 - learning_rate: 2.5000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9363 - loss: 0.1737 - val_accuracy: 0.9735 - val_loss: 0.0904 - learning_rate: 2.5000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9324 - loss: 0.1749 - val_accuracy: 0.9804 - val_loss: 0.0743 - learning_rate: 2.5000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9424 - loss: 0.1554 - val_accuracy: 0.9814 - val_loss: 0.0729 - learning_rate: 2.5000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9331 - loss: 0.1573 - val_accuracy: 0.9814 - val_loss: 0.0787 - learning_rate: 2.5000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9346 - loss: 0.1635 - val_accuracy: 0.9775 - val_loss: 0.0727 - learning_rate: 2.5000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9444 - loss: 0.1570 - val_accuracy: 0.9716 - val_loss: 0.0800 - learning_rate: 2.5000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9308 - loss: 0.1808\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9358 - loss: 0.1695 - val_accuracy: 0.9745 - val_loss: 0.0780 - learning_rate: 2.5000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9407 - loss: 0.1490 - val_accuracy: 0.9804 - val_loss: 0.0720 - learning_rate: 1.2500e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9461 - loss: 0.1392 - val_accuracy: 0.9804 - val_loss: 0.0722 - learning_rate: 1.2500e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9449 - loss: 0.1520 - val_accuracy: 0.9804 - val_loss: 0.0716 - learning_rate: 1.2500e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9358 - loss: 0.1560 - val_accuracy: 0.9765 - val_loss: 0.0763 - learning_rate: 1.2500e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9390 - loss: 0.1566 - val_accuracy: 0.9765 - val_loss: 0.0743 - learning_rate: 1.2500e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9356 - loss: 0.1564\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9409 - loss: 0.1503 - val_accuracy: 0.9804 - val_loss: 0.0709 - learning_rate: 1.2500e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9424 - loss: 0.1469 - val_accuracy: 0.9804 - val_loss: 0.0700 - learning_rate: 6.2500e-05\n",
      "Epoch 69/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9402 - loss: 0.1469 - val_accuracy: 0.9804 - val_loss: 0.0692 - learning_rate: 6.2500e-05\n",
      "Epoch 70/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9409 - loss: 0.1536 - val_accuracy: 0.9804 - val_loss: 0.0703 - learning_rate: 6.2500e-05\n",
      "Epoch 71/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.9427 - loss: 0.1479\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9431 - loss: 0.1476 - val_accuracy: 0.9804 - val_loss: 0.0699 - learning_rate: 6.2500e-05\n",
      "Epoch 72/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9468 - loss: 0.1431 - val_accuracy: 0.9794 - val_loss: 0.0697 - learning_rate: 3.1250e-05\n",
      "Epoch 73/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9417 - loss: 0.1474 - val_accuracy: 0.9784 - val_loss: 0.0697 - learning_rate: 3.1250e-05\n",
      "Epoch 74/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9409 - loss: 0.1583 - val_accuracy: 0.9794 - val_loss: 0.0714 - learning_rate: 3.1250e-05\n",
      "Epoch 75/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9525 - loss: 0.1374 - val_accuracy: 0.9794 - val_loss: 0.0687 - learning_rate: 3.1250e-05\n",
      "Epoch 76/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9471 - loss: 0.1364 - val_accuracy: 0.9794 - val_loss: 0.0695 - learning_rate: 3.1250e-05\n",
      "Epoch 77/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9419 - loss: 0.1580 - val_accuracy: 0.9794 - val_loss: 0.0682 - learning_rate: 3.1250e-05\n",
      "Epoch 78/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9412 - loss: 0.1496 - val_accuracy: 0.9784 - val_loss: 0.0714 - learning_rate: 3.1250e-05\n",
      "Epoch 79/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9441 - loss: 0.1507 - val_accuracy: 0.9804 - val_loss: 0.0662 - learning_rate: 3.1250e-05\n",
      "Epoch 80/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9505 - loss: 0.1387\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9480 - loss: 0.1416 - val_accuracy: 0.9784 - val_loss: 0.0686 - learning_rate: 3.1250e-05\n",
      "Epoch 81/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9488 - loss: 0.1407 - val_accuracy: 0.9794 - val_loss: 0.0676 - learning_rate: 1.5625e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9463 - loss: 0.1412 - val_accuracy: 0.9794 - val_loss: 0.0690 - learning_rate: 1.5625e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9456 - loss: 0.1444 - val_accuracy: 0.9794 - val_loss: 0.0676 - learning_rate: 1.5625e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.9493 - loss: 0.1492\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9463 - loss: 0.1468 - val_accuracy: 0.9794 - val_loss: 0.0672 - learning_rate: 1.5625e-05\n",
      "Epoch 85/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9436 - loss: 0.1464 - val_accuracy: 0.9794 - val_loss: 0.0685 - learning_rate: 7.8125e-06\n",
      "Epoch 86/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.9461 - loss: 0.1377 - val_accuracy: 0.9794 - val_loss: 0.0691 - learning_rate: 7.8125e-06\n",
      "Epoch 87/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9458 - loss: 0.1442 - val_accuracy: 0.9794 - val_loss: 0.0673 - learning_rate: 7.8125e-06\n",
      "Epoch 88/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9412 - loss: 0.1575\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9444 - loss: 0.1480 - val_accuracy: 0.9784 - val_loss: 0.0670 - learning_rate: 7.8125e-06\n",
      "Epoch 89/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9434 - loss: 0.1497 - val_accuracy: 0.9784 - val_loss: 0.0675 - learning_rate: 3.9063e-06\n",
      "Epoch 90/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9478 - loss: 0.1430 - val_accuracy: 0.9784 - val_loss: 0.0675 - learning_rate: 3.9063e-06\n",
      "Epoch 91/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9458 - loss: 0.1464 - val_accuracy: 0.9794 - val_loss: 0.0677 - learning_rate: 3.9063e-06\n",
      "Epoch 92/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9434 - loss: 0.1455\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9409 - loss: 0.1430 - val_accuracy: 0.9784 - val_loss: 0.0671 - learning_rate: 3.9063e-06\n",
      "Epoch 93/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9471 - loss: 0.1450 - val_accuracy: 0.9794 - val_loss: 0.0668 - learning_rate: 1.9531e-06\n",
      "Epoch 94/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9480 - loss: 0.1415 - val_accuracy: 0.9794 - val_loss: 0.0671 - learning_rate: 1.9531e-06\n",
      "Epoch 94: early stopping\n",
      "Restoring model weights from the end of the best epoch: 79.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] Model saved as cole-final.h5\n",
      "\n",
      "[CONFUSION MATRIX]\n",
      "[[473   7]\n",
      " [ 13 527]]\n",
      "[VAL] acc=0.9804, prec=0.9869, rec=0.9759, f1=0.9814\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 4: 80:20 Train / Validation Split (EEGNet)\n",
    "# ====================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, SeparableConv2D, BatchNormalization, Activation,\n",
    "    DepthwiseConv2D, Conv2D, AveragePooling2D,\n",
    "    Dropout, Dense, Flatten\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# --------------------------------------------------------------------\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# EEGNet model\n",
    "# --------------------------------------------------------------------\n",
    "def create_eegnet_model(input_shape, dropout_rate=0.5, num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(16, (1, 64), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = DepthwiseConv2D(\n",
    "        (input_shape[0], 1),\n",
    "        depth_multiplier=2,\n",
    "        padding='valid',\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 4))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = SeparableConv2D(16, (1, 16), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 8))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Compile model\n",
    "# --------------------------------------------------------------------\n",
    "def compile_model():\n",
    "    input_shape = (Xtr.shape[1], Xtr.shape[2], Xtr.shape[3])\n",
    "    model = create_eegnet_model(input_shape)\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-3),\n",
    "        loss=BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[BinaryAccuracy(name=\"accuracy\", threshold=0.5)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def make_callbacks():\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"loss\",\n",
    "            factor=0.5,\n",
    "            patience=4,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Use ALL normalized training data\n",
    "# --------------------------------------------------------------------\n",
    "Xtr = X_train_norm\n",
    "ytr = y_train_norm\n",
    "\n",
    "print(f\"[TRAIN] Xtr: {Xtr.shape}, ytr: {ytr.shape}\")\n",
    "print(\"[INFO] Test / LOSO subject is NOT used here.\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 80:20 Stratified Split\n",
    "# --------------------------------------------------------------------\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(sss.split(Xtr, ytr))\n",
    "\n",
    "X_train, X_val = Xtr[train_idx], Xtr[val_idx]\n",
    "y_train, y_val = ytr[train_idx], ytr[val_idx]\n",
    "\n",
    "print(f\"[SPLIT] Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Train model\n",
    "# --------------------------------------------------------------------\n",
    "model = compile_model()\n",
    "callbacks = make_callbacks()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Save final model\n",
    "# --------------------------------------------------------------------\n",
    "model.save(\"mina3-final.h5\")\n",
    "print(\"[SAVE] Model saved as cole-final.h5\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Validation Confusion Matrix\n",
    "# --------------------------------------------------------------------\n",
    "y_prob = model.predict(X_val, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "y_true = y_val.astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "eps = 1e-12\n",
    "\n",
    "acc  = (tp + tn) / max(tp + tn + fp + fn, eps)\n",
    "prec = tp / max(tp + fp, eps)\n",
    "rec  = tp / max(tp + fn, eps)\n",
    "f1   = 2 * prec * rec / max(prec + rec, eps)\n",
    "\n",
    "print(\"\\n[CONFUSION MATRIX]\")\n",
    "print(cm)\n",
    "print(f\"[VAL] acc={acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e2a13-6367-41ca-be66-a43594c2940e",
   "metadata": {},
   "source": [
    "Cell 8 — Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ac6662-33c1-4e25-b6c5-d47c860fba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Preparing TEST data...\n",
      "[TEST] Xte shape: (300, 14, 128, 1)\n",
      "[TEST] yte shape: (300,), class distribution: (array([0]), array([300]))\n",
      "[TEST] Loading final model: mina3-final.h5\n",
      "[TEST] Running inference on TEST subject...\n",
      "[TEST] Mean predicted probability: 0.2244\n",
      "\n",
      "[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[252  48]\n",
      " [  0   0]]\n",
      "Accuracy : 0.8400\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-score : 0.0000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Inference on TEST subject (adam) using FINAL EEGNet\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Prepare TEST data\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Preparing TEST data...\")\n",
    "\n",
    "Xte = X_test_norm\n",
    "yte = y_test_norm.astype(int)\n",
    "\n",
    "print(f\"[TEST] Xte shape: {Xte.shape}\")\n",
    "print(f\"[TEST] yte shape: {yte.shape}, class distribution: {np.unique(yte, return_counts=True)}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Load FINAL trained model\n",
    "# ------------------------------------------------------------------\n",
    "model_path = \"mina3-final.h5\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        \"Final model 'cole-final.h5' not found. \"\n",
    "        \"Please run Cell 4 (80:20 training) first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[TEST] Loading final model: {model_path}\")\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Run inference on TEST subject\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Running inference on TEST subject...\")\n",
    "\n",
    "y_prob = model.predict(Xte, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(f\"[TEST] Mean predicted probability: {y_prob.mean():.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Metrics on TEST subject (segment-level)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc = accuracy_score(yte, y_pred)\n",
    "prec = precision_score(yte, y_pred, zero_division=0)\n",
    "rec = recall_score(yte, y_pred, zero_division=0)\n",
    "f1 = f1_score(yte, y_pred, zero_division=0)\n",
    "\n",
    "print(\"\\n[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572dbf3e-bbb5-4865-883f-30e374700949",
   "metadata": {},
   "source": [
    "Cell 9 - Performance Matrices for each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a57240-50ea-4736-a7ac-52ef7878b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] Verifying predictions and labels are available...\n",
      "[CHECK] Found predictions for 300 test segments.\n",
      "\n",
      "[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[252  48]\n",
      " [  0   0]]\n",
      "Accuracy : 0.8400\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-score : 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Final model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================\n",
    "# Cell 6: Confusion Matrix & Metrics on TEST (FINAL EEGNet)\n",
    "# =======================================================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# ------------------------------------------------------------------\n",
    "def compute_metrics_from_cm(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    eps = 1e-12\n",
    "    acc  = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec  = tp / (tp + fn + eps)\n",
    "    f1   = 2 * prec * rec / (prec + rec + eps)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], ['0', '1'])\n",
    "    plt.yticks([0, 1], ['0', '1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(\n",
    "                j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Validate upstream artifacts\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[CHECK] Verifying predictions and labels are available...\")\n",
    "\n",
    "if 'y_pred' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Predictions 'y_pred' not found. \"\n",
    "        \"Please run Cell 5 (final model inference) first.\"\n",
    "    )\n",
    "\n",
    "if 'yte' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Test labels 'yte' not found. \"\n",
    "        \"Please run Cell 5 first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[CHECK] Found predictions for {len(y_pred)} test segments.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Confusion Matrix & Metrics (FINAL model)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc, prec, rec, f1 = compute_metrics_from_cm(cm)\n",
    "\n",
    "print(\"\\n[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(cm, title=\"Confusion Matrix - FINAL EEGNet\")\n",
    "\n",
    "print(\"[DONE] Final model evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0f120-39fd-497e-9098-05e6c82f4807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
