{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af5b471-cea9-4194-bc23-da5cdc2f9c67",
   "metadata": {},
   "source": [
    "Cell 1 — Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64aaa897-ab61-4eb4-a558-4c078acf751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Base path set to:\n",
      "C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\n",
      "[INFO] Found 18 folders:\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Selected test subject (LOSO): Ismayil\n",
      "[INFO] Training subjects (17):\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Sampling rate (SFREQ) = 128.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 1: Setup & discovery\n",
    "# =========================\n",
    "\n",
    "import os  # file ops\n",
    "import glob  # file matching\n",
    "import numpy as np  # arrays\n",
    "import random  # seeds\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data path\n",
    "BASE_PATH = r\"C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\"\n",
    "print(f\"[INFO] Base path set to:\\n{BASE_PATH}\")\n",
    "\n",
    "# List subject folders\n",
    "all_folders = [p for p in sorted(os.listdir(BASE_PATH)) if os.path.isdir(os.path.join(BASE_PATH, p))]\n",
    "print(f\"[INFO] Found {len(all_folders)} folders:\\n{all_folders}\")\n",
    "\n",
    "# Expected count (change if needed)\n",
    "assert len(all_folders) == 18, f\"Expected 18 folders, found {len(all_folders)}.\"\n",
    "\n",
    "# LOSO test subject\n",
    "TEST_SUBJECT = \"Ismayil\"\n",
    "print(f\"[INFO] Selected test subject (LOSO): {TEST_SUBJECT}\")\n",
    "\n",
    "# Train subjects = remaining\n",
    "train_subjects = [s for s in all_folders if s.lower() != TEST_SUBJECT.lower()]\n",
    "print(f\"[INFO] Training subjects ({len(train_subjects)}):\\n{train_subjects}\")\n",
    "\n",
    "# Sampling rate\n",
    "SFREQ = 128.0\n",
    "print(f\"[INFO] Sampling rate (SFREQ) = {SFREQ} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e649681-2861-4ce2-bbd1-0d9dd3359f42",
   "metadata": {},
   "source": [
    "Cell 2 — Load EEG + labels per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943c5c6e-a7aa-4618-899c-daf341c25358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 2] Loading data from folders...\n",
      "[STEP 2]       Amin | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Amin | EEG=          Amin.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Amin1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]      Amin1 | EEG=         Amin1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Cole | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Cole | EEG=          Cole.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Daniel | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]     Daniel | EEG=        Daniel.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]    Ismayil | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]    Ismayil | EEG=       Ismayil.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Jack | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Jack | EEG=          Jack.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]      James | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      James | EEG=         James.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Josh | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Josh | EEG=          Josh.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Marjan | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Marjan | EEG=        Marjan.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]        Max | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Max | EEG=           Max.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Mina | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mina | EEG=          Mina.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 1 | EEG=        Mina 1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 3 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 3 | EEG=        Mina 3.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]   Mohammad | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]   Mohammad | EEG=      Mohammad.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Mona | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mona | EEG=          Mona.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Roddy | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      Roddy | EEG=         Roddy.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]        Sam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Sam | EEG=           Sam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       adam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       adam | EEG=          adam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2] Loading complete ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 2: Load data (EMPTY LABEL => CLASS 0)\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path  # helper\n",
    "\n",
    "def load_subject(subj_dir: str, subj_name: str, expected_channels: int = 14):\n",
    "    \"\"\"\n",
    "    EEG file: {subj_name}.npy\n",
    "    Label file: {subj_name}_target.npy\n",
    "    Label rule:\n",
    "      - empty array -> class 0\n",
    "      - [0] -> class 0\n",
    "      - [1] -> class 1\n",
    "    \"\"\"\n",
    "\n",
    "    # EEG file path\n",
    "    eeg_file = os.path.join(subj_dir, f\"{subj_name}.npy\")\n",
    "    assert os.path.exists(eeg_file), f\"[ERROR] Missing EEG file: {eeg_file}\"\n",
    "\n",
    "    # Label file path\n",
    "    target_file = os.path.join(subj_dir, f\"{subj_name}_target.npy\")\n",
    "    assert os.path.exists(target_file), f\"[ERROR] Missing label file: {target_file}\"\n",
    "\n",
    "    # Load EEG\n",
    "    X_raw = np.load(eeg_file, allow_pickle=True)\n",
    "    X_raw = np.asarray(X_raw)\n",
    "\n",
    "    # Standardize EEG to (n_trials, 14, T)\n",
    "    if X_raw.ndim == 2:\n",
    "        X_trials = X_raw[None, :, :]\n",
    "    elif X_raw.ndim == 3:\n",
    "        dims = list(X_raw.shape)\n",
    "        chan_axes = [i for i, d in enumerate(dims) if d == expected_channels]\n",
    "        chan_axis = chan_axes[0] if len(chan_axes) == 1 else 1\n",
    "        time_axis = int(np.argmax(dims))\n",
    "        trial_axis = [i for i in range(3) if i not in (chan_axis, time_axis)][0]\n",
    "        X_trials = np.moveaxis(X_raw, (trial_axis, chan_axis, time_axis), (0, 1, 2))\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] Unsupported EEG shape {X_raw.shape} in {eeg_file}\")\n",
    "\n",
    "    assert X_trials.shape[1] == expected_channels, f\"[ERROR] Expected {expected_channels} channels, got {X_trials.shape[1]} in {eeg_file}\"\n",
    "\n",
    "    # Load label\n",
    "    y_raw = np.load(target_file, allow_pickle=True)\n",
    "    y_arr = np.asarray(y_raw).ravel()\n",
    "\n",
    "    # Apply your rule\n",
    "    if y_arr.size == 0:\n",
    "        label = 0\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\")\n",
    "    else:\n",
    "        label = int(y_arr[0])\n",
    "        if label not in (0, 1):\n",
    "            raise ValueError(f\"[ERROR] Invalid label {label} in {target_file} (must be 0 or 1)\")\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape={y_arr.shape}, value={y_arr} -> class={label}\")\n",
    "\n",
    "    # Repeat label for all trials\n",
    "    y_trials = np.full((X_trials.shape[0],), label, dtype=np.int32)\n",
    "\n",
    "    return X_trials.astype(np.float32), y_trials.astype(np.int32), eeg_file, target_file\n",
    "\n",
    "\n",
    "# Load all subjects\n",
    "subject_data = {}\n",
    "print(\"[STEP 2] Loading data from folders...\")\n",
    "\n",
    "for subj in all_folders:\n",
    "    subj_dir = os.path.join(BASE_PATH, subj)\n",
    "    X_trials, y_trials, eeg_file, target_file = load_subject(subj_dir, subj)\n",
    "\n",
    "    subject_data[subj] = (X_trials, y_trials)\n",
    "\n",
    "    print(f\"[STEP 2] {subj:>10} | EEG={Path(eeg_file).name:>18} | X={X_trials.shape} | y={y_trials.shape} | class={y_trials[0]}\")\n",
    "\n",
    "print(\"[STEP 2] Loading complete ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd93ca8-a94c-485c-8a61-e6e7441f6299",
   "metadata": {},
   "source": [
    "Cell 3 — Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733e592c-98e1-4cbd-afd2-6fdc8cf407f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 3] Preprocessing functions ready ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3: Preprocessing functions\n",
    "# =========================\n",
    "\n",
    "import mne  # EEG processing\n",
    "import pywt  # wavelets\n",
    "from sklearn.decomposition import FastICA  # ICA\n",
    "from typing import Optional, Tuple, Union, Sequence, Dict, List  # typing\n",
    "\n",
    "def wavelet_enhanced_ica(\n",
    "    data: np.ndarray,\n",
    "    n_components: int = 10,\n",
    "    wavelet: str = \"db4\",\n",
    "    level: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> np.ndarray:\n",
    "    n_ch, n_t = data.shape\n",
    "    n_components = min(n_components, n_ch)\n",
    "\n",
    "    coeffs = pywt.wavedec(data, wavelet=wavelet, level=level, axis=1)\n",
    "    A = coeffs[0]\n",
    "\n",
    "    ica = FastICA(n_components=n_components, random_state=random_state)\n",
    "    S = ica.fit_transform(A.T).T\n",
    "    A_denoised = ica.inverse_transform(S.T).T\n",
    "\n",
    "    coeffs[0] = A_denoised\n",
    "    cleaned = pywt.waverec(coeffs, wavelet=wavelet, axis=1)\n",
    "\n",
    "    if cleaned.shape[1] != n_t:\n",
    "        if cleaned.shape[1] > n_t:\n",
    "            cleaned = cleaned[:, :n_t]\n",
    "        else:\n",
    "            cleaned = np.pad(cleaned, ((0, 0), (0, n_t - cleaned.shape[1])), mode=\"constant\")\n",
    "    return cleaned\n",
    "\n",
    "def _names_from_index_mapping(n_channels: int, index_to_name: Optional[Dict[int, str]]) -> List[str]:\n",
    "    if index_to_name is None:\n",
    "        return [f\"EEG{i+1}\" for i in range(n_channels)]\n",
    "    keys = list(index_to_name.keys())\n",
    "    is_zero_based = (0 in keys) and (1 not in keys)\n",
    "    names = []\n",
    "    for i in range(n_channels):\n",
    "        key = i if is_zero_based else (i + 1)\n",
    "        names.append(index_to_name.get(key, f\"EEG{i+1}\"))\n",
    "    return names\n",
    "\n",
    "def preprocess_eeg(\n",
    "    eeg: np.ndarray,\n",
    "    sfreq: float,\n",
    "    *,\n",
    "    index_to_name: Optional[Dict[int, str]] = None,\n",
    "    use_standard_1010: bool = True,\n",
    "    resample_to: Optional[float] = None,\n",
    "    notch_freqs: Union[None, float, Sequence[float]] = 50.0,\n",
    "    highpass: Optional[float] = 0.05,\n",
    "    bad_point_z: float = 6.0,\n",
    "    bad_channel_z: float = 5.0,\n",
    "    interpolate_bad_channels: bool = True,\n",
    "    car: bool = True,\n",
    "    use_wica: bool = True,\n",
    "    wica_components: int = 10,\n",
    "    wica_wavelet: str = \"db4\",\n",
    "    wica_level: int = 3,\n",
    "    wica_random_state: int = 42,\n",
    "    return_raw: bool = False,\n",
    "):\n",
    "\n",
    "    eeg = np.asarray(eeg, dtype=np.float32)\n",
    "    assert eeg.ndim == 2, \"eeg must be 2D: (n_channels, n_times)\"\n",
    "\n",
    "    n_channels, _ = eeg.shape\n",
    "    ch_names = _names_from_index_mapping(n_channels, index_to_name)\n",
    "    ch_types = ['eog' if str(n).upper().startswith(\"EOG\") else 'eeg' for n in ch_names]\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    raw = mne.io.RawArray(eeg, info, verbose=False)\n",
    "\n",
    "    montage_applied = False\n",
    "    if use_standard_1010:\n",
    "        try:\n",
    "            mont = mne.channels.make_standard_montage(\"standard_1010\")\n",
    "            raw.set_montage(mont, match_case=False, on_missing=\"ignore\")\n",
    "            montage_applied = True\n",
    "        except Exception:\n",
    "            montage_applied = False\n",
    "\n",
    "    if resample_to is not None and float(resample_to) != float(sfreq):\n",
    "        raw.resample(sfreq=resample_to, npad=\"auto\")\n",
    "    sfreq_out = float(raw.info[\"sfreq\"])\n",
    "\n",
    "    if notch_freqs is not None:\n",
    "        raw.notch_filter(freqs=notch_freqs, verbose=False)\n",
    "    if highpass is not None:\n",
    "        raw.filter(l_freq=highpass, h_freq=None, verbose=False)\n",
    "\n",
    "    X = raw.get_data()\n",
    "    mu = np.mean(X, axis=1, keepdims=True)\n",
    "    sd = np.std(X, axis=1, keepdims=True) + 1e-12\n",
    "    hi = mu + bad_point_z * sd\n",
    "    lo = mu - bad_point_z * sd\n",
    "    bad_idx = (X > hi) | (X < lo)\n",
    "\n",
    "    if np.any(bad_idx):\n",
    "        X_fixed = X.copy()\n",
    "        t = np.arange(X.shape[1], dtype=float)\n",
    "        for ch in range(n_channels):\n",
    "            mask = bad_idx[ch]\n",
    "            if mask.any():\n",
    "                good = ~mask\n",
    "                if good.sum() >= 2:\n",
    "                    X_fixed[ch, mask] = np.interp(t[mask], t[good], X_fixed[ch, good])\n",
    "        raw._data = X_fixed\n",
    "\n",
    "    if interpolate_bad_channels and montage_applied:\n",
    "        X = raw.get_data(picks=\"eeg\")\n",
    "        if X.size > 0:\n",
    "            ch_std = X.std(axis=1)\n",
    "            med = np.median(ch_std)\n",
    "            mad = np.median(np.abs(ch_std - med)) + 1e-12\n",
    "            z = 0.6745 * (ch_std - med) / mad\n",
    "            eeg_names = mne.pick_info(raw.info, mne.pick_types(raw.info, eeg=True)).ch_names\n",
    "            bads = [eeg_names[i] for i in np.where(np.abs(z) > bad_channel_z)[0]]\n",
    "            raw.info[\"bads\"] = bads\n",
    "            if bads:\n",
    "                raw.interpolate_bads(reset_bads=True, verbose=False)\n",
    "\n",
    "    if car:\n",
    "        raw.set_eeg_reference(\"average\", projection=True)\n",
    "        raw.apply_proj()\n",
    "\n",
    "    if use_wica:\n",
    "        cleaned = wavelet_enhanced_ica(\n",
    "            raw.get_data(),\n",
    "            n_components=wica_components,\n",
    "            wavelet=wica_wavelet,\n",
    "            level=wica_level,\n",
    "            random_state=wica_random_state\n",
    "        )\n",
    "        raw = mne.io.RawArray(cleaned, raw.info, verbose=False)\n",
    "\n",
    "    cleaned = raw.get_data()\n",
    "    return cleaned, sfreq_out\n",
    "\n",
    "print(\"[STEP 3] Preprocessing functions ready ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a9d20-889d-45c4-bbc2-de62216908cd",
   "metadata": {},
   "source": [
    "Cell 4 — Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bfcbde0-1cc2-409c-8627-93da46606661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 4] Starting preprocessing...\n",
      "[INFO] USE_WICA = True\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 01/17: Amin | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 02/17: Amin1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 03/17: Cole | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 04/17: Daniel | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 05/17: Jack | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 06/17: James | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 07/17: Josh | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 08/17: Marjan | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 09/17: Max | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 10/17: Mina | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 11/17: Mina 1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 12/17: Mina 3 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 13/17: Mohammad | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 14/17: Mona | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 15/17: Roddy | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 16/17: Sam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 17/17: adam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Test subject: Ismayil | trials=1 | fs_out=128.0\n",
      "[STEP 4] Preprocessing done ✅\n",
      "  X_train_trials_all: (17, 14, 38400)\n",
      "  y_train_trials_all: (17,) (array([0, 1], dtype=int32), array([8, 9]))\n",
      "  X_test_trials_all : (1, 14, 38400)\n",
      "  y_test_trials_all : (1,) (array([0], dtype=int32), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 4: Apply preprocessing (LOSO)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 4] Starting preprocessing...\")\n",
    "\n",
    "USE_WICA = True\n",
    "print(f\"[INFO] USE_WICA = {USE_WICA}\")\n",
    "\n",
    "X_train_trials_all, y_train_trials_all = [], []\n",
    "X_test_trials_all, y_test_trials_all = [], []\n",
    "\n",
    "def preprocess_trials(X_trials, y_trials):\n",
    "    cleaned_list = []\n",
    "    label_list = []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        trial = X_trials[i]\n",
    "        label = int(y_trials[i])\n",
    "\n",
    "        cleaned, fs_out = preprocess_eeg(\n",
    "            eeg=trial,\n",
    "            sfreq=SFREQ,\n",
    "            use_wica=USE_WICA\n",
    "        )\n",
    "\n",
    "        cleaned_list.append(cleaned.astype(np.float32, copy=False))\n",
    "        label_list.append(label)\n",
    "\n",
    "    return cleaned_list, np.array(label_list, dtype=np.int32), fs_out\n",
    "\n",
    "# Train subjects\n",
    "for idx, subj in enumerate(train_subjects, start=1):\n",
    "    X_trials, y_trials = subject_data[subj]\n",
    "    cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "    X_train_trials_all.extend(cleaned_trials)\n",
    "    y_train_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "    print(f\"[STEP 4] Train {idx:02d}/{len(train_subjects)}: {subj} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Test subject\n",
    "X_trials, y_trials = subject_data[TEST_SUBJECT]\n",
    "cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "X_test_trials_all.extend(cleaned_trials)\n",
    "y_test_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "print(f\"[STEP 4] Test subject: {TEST_SUBJECT} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Convert to arrays\n",
    "X_train_trials_all = np.array(X_train_trials_all, dtype=np.float32)\n",
    "y_train_trials_all = np.array(y_train_trials_all, dtype=np.int32)\n",
    "\n",
    "X_test_trials_all = np.array(X_test_trials_all, dtype=np.float32)\n",
    "y_test_trials_all = np.array(y_test_trials_all, dtype=np.int32)\n",
    "\n",
    "print(\"[STEP 4] Preprocessing done ✅\")\n",
    "print(\"  X_train_trials_all:\", X_train_trials_all.shape)\n",
    "print(\"  y_train_trials_all:\", y_train_trials_all.shape, np.unique(y_train_trials_all, return_counts=True))\n",
    "print(\"  X_test_trials_all :\", X_test_trials_all.shape)\n",
    "print(\"  y_test_trials_all :\", y_test_trials_all.shape, np.unique(y_test_trials_all, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc3da1-2967-4ee3-84cd-bfcf0f98c0b1",
   "metadata": {},
   "source": [
    "Cell 5 — Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c21ad54-7001-4039-9360-29dfa2905127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 5] Starting segmentation...\n",
      "[STEP 5] Segmented 5/17 trials...\n",
      "[STEP 5] Segmented 10/17 trials...\n",
      "[STEP 5] Segmented 15/17 trials...\n",
      "[STEP 5] Segmented 17/17 trials...\n",
      "[STEP 5] Segmented 1/1 trials...\n",
      "[STEP 5] Segmentation done ✅\n",
      "  X_train: (5100, 14, 128, 1)\n",
      "  y_train: (5100,) (array([0, 1], dtype=int32), array([2400, 2700]))\n",
      "  X_test : (300, 14, 128, 1)\n",
      "  y_test : (300,) (array([0], dtype=int32), array([300]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 5: Segmentation\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 5] Starting segmentation...\")\n",
    "\n",
    "def segment_trial(trial_2d: np.ndarray, label: int, segment_len: int = 128):\n",
    "    C, T = trial_2d.shape\n",
    "    n_segs = T // segment_len\n",
    "    use_T = n_segs * segment_len\n",
    "\n",
    "    if n_segs == 0:\n",
    "        return np.empty((0, C, segment_len), dtype=np.float32), np.empty((0,), dtype=np.int32)\n",
    "\n",
    "    trial_trim = trial_2d[:, :use_T]\n",
    "    Xsegs = trial_trim.reshape(C, n_segs, segment_len).transpose(1, 0, 2)\n",
    "    ysegs = np.full((n_segs,), int(label), dtype=np.int32)\n",
    "    return Xsegs.astype(np.float32, copy=False), ysegs\n",
    "\n",
    "def segment_dataset(X_trials: np.ndarray, y_trials: np.ndarray, segment_len: int = 128):\n",
    "    X_all, y_all = [], []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        Xsegs, ysegs = segment_trial(X_trials[i], int(y_trials[i]), segment_len=segment_len)\n",
    "        X_all.append(Xsegs)\n",
    "        y_all.append(ysegs)\n",
    "\n",
    "        if (i + 1) % 5 == 0 or (i + 1) == X_trials.shape[0]:\n",
    "            print(f\"[STEP 5] Segmented {i+1}/{X_trials.shape[0]} trials...\")\n",
    "\n",
    "    X_out = np.concatenate(X_all, axis=0)\n",
    "    y_out = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    X_out = np.expand_dims(X_out, axis=-1)  # (N,14,128,1)\n",
    "    return X_out.astype(np.float32), y_out.astype(np.int32)\n",
    "\n",
    "X_train, y_train = segment_dataset(X_train_trials_all, y_train_trials_all, segment_len=128)\n",
    "X_test,  y_test  = segment_dataset(X_test_trials_all,  y_test_trials_all,  segment_len=128)\n",
    "\n",
    "print(\"[STEP 5] Segmentation done ✅\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  y_train:\", y_train.shape, np.unique(y_train, return_counts=True))\n",
    "print(\"  X_test :\", X_test.shape)\n",
    "print(\"  y_test :\", y_test.shape, np.unique(y_test, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b4ffd5-bf2f-490e-8854-0a2fc747f1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e69ba7-3952-47b1-bd80-f05237b2e018",
   "metadata": {},
   "source": [
    "Cell 6 — Split train/val + Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97fda4ae-e1c8-4fd1-bf0c-c82ae2a85764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 6] Normalizing using ALL training data (no train/val split)...\n",
      "[STEP 6] Normalization done ✅\n",
      "  X_train_norm: (5100, 14, 128, 1)\n",
      "  X_test_norm : (300, 14, 128, 1)\n",
      "  y_train_norm: (5100,)\n",
      "  y_test_norm : (300,)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 6: Normalize (ALL training data)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 6] Normalizing using ALL training data (no train/val split)...\")\n",
    "\n",
    "def normalize_eeg(train_data: np.ndarray, test_data: np.ndarray):\n",
    "    if train_data.ndim == 5:\n",
    "        train_flat = train_data.reshape(-1, *train_data.shape[2:])\n",
    "        test_flat  = test_data.reshape(-1, *test_data.shape[2:])\n",
    "    else:\n",
    "        train_flat, test_flat = train_data, test_data\n",
    "\n",
    "    # Compute stats on ALL training data\n",
    "    train_mean = np.mean(train_flat, axis=(0, 2, 3), keepdims=True)\n",
    "    train_std  = np.std(train_flat,  axis=(0, 2, 3), keepdims=True) + 1e-8\n",
    "\n",
    "    train_norm = (train_flat - train_mean) / train_std\n",
    "    test_norm  = (test_flat  - train_mean) / train_std\n",
    "\n",
    "    if train_data.ndim == 5:\n",
    "        train_norm = train_norm.reshape(train_data.shape)\n",
    "        test_norm  = test_norm.reshape(test_data.shape)\n",
    "\n",
    "    return (\n",
    "        train_norm.astype(np.float32),\n",
    "        test_norm.astype(np.float32),\n",
    "        train_mean,\n",
    "        train_std\n",
    "    )\n",
    "\n",
    "# Normalize ALL training data\n",
    "X_train_norm, X_test_norm, mean, std = normalize_eeg(X_train, X_test)\n",
    "\n",
    "y_train_norm = y_train.astype(np.float32)\n",
    "y_test_norm  = y_test.astype(np.float32)\n",
    "\n",
    "print(\"[STEP 6] Normalization done ✅\")\n",
    "print(\"  X_train_norm:\", X_train_norm.shape)\n",
    "print(\"  X_test_norm :\", X_test_norm.shape)\n",
    "print(\"  y_train_norm:\", y_train_norm.shape)\n",
    "print(\"  y_test_norm :\", y_test_norm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f527c-34db-4c23-bdbd-c8ce03b519f0",
   "metadata": {},
   "source": [
    "Cell 7 — EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ea4279-53c2-49c1-b834-7b1e1c20775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\DL\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Xtr: (5100, 14, 128, 1), ytr: (5100,)\n",
      "[INFO] Test / LOSO subject is NOT used here.\n",
      "[SPLIT] Train: (4080, 14, 128, 1), Val: (1020, 14, 128, 1)\n",
      "Epoch 1/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 145ms/step - accuracy: 0.5267 - loss: 0.7236 - val_accuracy: 0.4775 - val_loss: 0.6952 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.5539 - loss: 0.6834 - val_accuracy: 0.4755 - val_loss: 0.6948 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 126ms/step - accuracy: 0.5711 - loss: 0.6669 - val_accuracy: 0.4980 - val_loss: 0.6920 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.6105 - loss: 0.6435 - val_accuracy: 0.5598 - val_loss: 0.6859 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.6453 - loss: 0.6112 - val_accuracy: 0.5833 - val_loss: 0.6760 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 134ms/step - accuracy: 0.6659 - loss: 0.5944 - val_accuracy: 0.6412 - val_loss: 0.6625 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 125ms/step - accuracy: 0.7069 - loss: 0.5530 - val_accuracy: 0.6559 - val_loss: 0.6465 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.7301 - loss: 0.5269 - val_accuracy: 0.7147 - val_loss: 0.6281 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.7297 - loss: 0.5246 - val_accuracy: 0.6804 - val_loss: 0.6128 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.7657 - loss: 0.4874 - val_accuracy: 0.7304 - val_loss: 0.5832 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.7689 - loss: 0.4745 - val_accuracy: 0.7235 - val_loss: 0.5614 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.7809 - loss: 0.4570 - val_accuracy: 0.7284 - val_loss: 0.5391 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.7887 - loss: 0.4366 - val_accuracy: 0.7402 - val_loss: 0.5182 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8020 - loss: 0.4162 - val_accuracy: 0.7255 - val_loss: 0.5076 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.8078 - loss: 0.4213 - val_accuracy: 0.6794 - val_loss: 0.5334 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8172 - loss: 0.4040 - val_accuracy: 0.7284 - val_loss: 0.5124 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.8186 - loss: 0.3816 - val_accuracy: 0.7441 - val_loss: 0.4904 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8213 - loss: 0.3848 - val_accuracy: 0.7696 - val_loss: 0.4550 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.8228 - loss: 0.3831 - val_accuracy: 0.7206 - val_loss: 0.5191 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.8311 - loss: 0.3837 - val_accuracy: 0.7775 - val_loss: 0.4366 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8328 - loss: 0.3719 - val_accuracy: 0.8049 - val_loss: 0.4070 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8358 - loss: 0.3672 - val_accuracy: 0.8108 - val_loss: 0.3922 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8480 - loss: 0.3517 - val_accuracy: 0.8020 - val_loss: 0.3898 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.8525 - loss: 0.3376 - val_accuracy: 0.8343 - val_loss: 0.3714 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.8576 - loss: 0.3265 - val_accuracy: 0.8157 - val_loss: 0.3717 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8512 - loss: 0.3467 - val_accuracy: 0.8853 - val_loss: 0.3313 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8701 - loss: 0.3136 - val_accuracy: 0.8588 - val_loss: 0.3250 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.8679 - loss: 0.3018 - val_accuracy: 0.8863 - val_loss: 0.3036 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.8691 - loss: 0.3126 - val_accuracy: 0.9039 - val_loss: 0.2831 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.8743 - loss: 0.2931 - val_accuracy: 0.8549 - val_loss: 0.3185 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.8755 - loss: 0.2933 - val_accuracy: 0.8990 - val_loss: 0.2610 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.8838 - loss: 0.2822 - val_accuracy: 0.8980 - val_loss: 0.2800 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8765 - loss: 0.2918 - val_accuracy: 0.9088 - val_loss: 0.2397 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8875 - loss: 0.2790 - val_accuracy: 0.9088 - val_loss: 0.2652 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8855 - loss: 0.2736 - val_accuracy: 0.9324 - val_loss: 0.2148 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8902 - loss: 0.2569 - val_accuracy: 0.9382 - val_loss: 0.1997 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8870 - loss: 0.2648 - val_accuracy: 0.9412 - val_loss: 0.2066 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.8900 - loss: 0.2674 - val_accuracy: 0.9196 - val_loss: 0.2386 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.8853 - loss: 0.2753 - val_accuracy: 0.9304 - val_loss: 0.2136 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.8993 - loss: 0.2377 - val_accuracy: 0.9392 - val_loss: 0.2003 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9002 - loss: 0.2490 - val_accuracy: 0.9451 - val_loss: 0.1770 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.9076 - loss: 0.2286 - val_accuracy: 0.9500 - val_loss: 0.1669 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.8988 - loss: 0.2546 - val_accuracy: 0.9412 - val_loss: 0.1823 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.9037 - loss: 0.2356 - val_accuracy: 0.9490 - val_loss: 0.1712 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.9086 - loss: 0.2250 - val_accuracy: 0.9402 - val_loss: 0.1879 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.9064 - loss: 0.2221 - val_accuracy: 0.9265 - val_loss: 0.1997 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9140 - loss: 0.2170 - val_accuracy: 0.9275 - val_loss: 0.1858 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9000 - loss: 0.2456 - val_accuracy: 0.9284 - val_loss: 0.1935 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9120 - loss: 0.2295 - val_accuracy: 0.9490 - val_loss: 0.1570 - learning_rate: 0.0010\n",
      "Epoch 50/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9189 - loss: 0.2123 - val_accuracy: 0.9451 - val_loss: 0.1687 - learning_rate: 0.0010\n",
      "Epoch 51/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9147 - loss: 0.2091 - val_accuracy: 0.9490 - val_loss: 0.1637 - learning_rate: 0.0010\n",
      "Epoch 52/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9132 - loss: 0.2199 - val_accuracy: 0.9294 - val_loss: 0.1940 - learning_rate: 0.0010\n",
      "Epoch 53/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9162 - loss: 0.2015 - val_accuracy: 0.9500 - val_loss: 0.1721 - learning_rate: 0.0010\n",
      "Epoch 54/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9127 - loss: 0.2100 - val_accuracy: 0.9373 - val_loss: 0.1937 - learning_rate: 0.0010\n",
      "Epoch 55/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9194 - loss: 0.2057 - val_accuracy: 0.9392 - val_loss: 0.1730 - learning_rate: 0.0010\n",
      "Epoch 56/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9137 - loss: 0.2106 - val_accuracy: 0.9363 - val_loss: 0.1779 - learning_rate: 0.0010\n",
      "Epoch 57/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9228 - loss: 0.1982 - val_accuracy: 0.9529 - val_loss: 0.1560 - learning_rate: 0.0010\n",
      "Epoch 58/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9275 - loss: 0.1932 - val_accuracy: 0.9618 - val_loss: 0.1392 - learning_rate: 0.0010\n",
      "Epoch 59/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9135 - loss: 0.2150 - val_accuracy: 0.9284 - val_loss: 0.2082 - learning_rate: 0.0010\n",
      "Epoch 60/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9056 - loss: 0.2302 - val_accuracy: 0.9431 - val_loss: 0.1735 - learning_rate: 0.0010\n",
      "Epoch 61/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9162 - loss: 0.1974 - val_accuracy: 0.9480 - val_loss: 0.1723 - learning_rate: 0.0010\n",
      "Epoch 62/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9233 - loss: 0.1930 - val_accuracy: 0.9549 - val_loss: 0.1457 - learning_rate: 0.0010\n",
      "Epoch 63/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9289 - loss: 0.1822 - val_accuracy: 0.9569 - val_loss: 0.1452 - learning_rate: 0.0010\n",
      "Epoch 64/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9245 - loss: 0.1966 - val_accuracy: 0.9186 - val_loss: 0.2033 - learning_rate: 0.0010\n",
      "Epoch 65/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9248 - loss: 0.1885 - val_accuracy: 0.9275 - val_loss: 0.1930 - learning_rate: 0.0010\n",
      "Epoch 66/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9272 - loss: 0.1848 - val_accuracy: 0.9412 - val_loss: 0.1656 - learning_rate: 0.0010\n",
      "Epoch 67/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9321 - loss: 0.1721 - val_accuracy: 0.9451 - val_loss: 0.1571 - learning_rate: 0.0010\n",
      "Epoch 68/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9319 - loss: 0.1747 - val_accuracy: 0.9471 - val_loss: 0.1474 - learning_rate: 0.0010\n",
      "Epoch 69/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.9314 - loss: 0.1689 - val_accuracy: 0.9510 - val_loss: 0.1430 - learning_rate: 0.0010\n",
      "Epoch 70/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9196 - loss: 0.1960 - val_accuracy: 0.9294 - val_loss: 0.1968 - learning_rate: 0.0010\n",
      "Epoch 71/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9324 - loss: 0.1676 - val_accuracy: 0.9500 - val_loss: 0.1531 - learning_rate: 0.0010\n",
      "Epoch 72/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.9331 - loss: 0.1653 - val_accuracy: 0.9559 - val_loss: 0.1398 - learning_rate: 0.0010\n",
      "Epoch 73/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9341 - loss: 0.1748 - val_accuracy: 0.9520 - val_loss: 0.1444 - learning_rate: 0.0010\n",
      "Epoch 73: early stopping\n",
      "Restoring model weights from the end of the best epoch: 58.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] Model saved as cole-final.h5\n",
      "\n",
      "[CONFUSION MATRIX]\n",
      "[[464  16]\n",
      " [ 23 517]]\n",
      "[VAL] acc=0.9618, prec=0.9700, rec=0.9574, f1=0.9637\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 4: 80:20 Train / Validation Split (EEGNet)\n",
    "# ====================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, SeparableConv2D, BatchNormalization, Activation,\n",
    "    DepthwiseConv2D, Conv2D, AveragePooling2D,\n",
    "    Dropout, Dense, Flatten\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# --------------------------------------------------------------------\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# EEGNet model\n",
    "# --------------------------------------------------------------------\n",
    "def create_eegnet_model(input_shape, dropout_rate=0.5, num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(16, (1, 64), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = DepthwiseConv2D(\n",
    "        (input_shape[0], 1),\n",
    "        depth_multiplier=2,\n",
    "        padding='valid',\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 4))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = SeparableConv2D(16, (1, 16), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 8))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Compile model\n",
    "# --------------------------------------------------------------------\n",
    "def compile_model():\n",
    "    input_shape = (Xtr.shape[1], Xtr.shape[2], Xtr.shape[3])\n",
    "    model = create_eegnet_model(input_shape)\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-3),\n",
    "        loss=BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[BinaryAccuracy(name=\"accuracy\", threshold=0.5)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def make_callbacks():\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"loss\",\n",
    "            factor=0.5,\n",
    "            patience=4,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Use ALL normalized training data\n",
    "# --------------------------------------------------------------------\n",
    "Xtr = X_train_norm\n",
    "ytr = y_train_norm\n",
    "\n",
    "print(f\"[TRAIN] Xtr: {Xtr.shape}, ytr: {ytr.shape}\")\n",
    "print(\"[INFO] Test / LOSO subject is NOT used here.\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 80:20 Stratified Split\n",
    "# --------------------------------------------------------------------\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(sss.split(Xtr, ytr))\n",
    "\n",
    "X_train, X_val = Xtr[train_idx], Xtr[val_idx]\n",
    "y_train, y_val = ytr[train_idx], ytr[val_idx]\n",
    "\n",
    "print(f\"[SPLIT] Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Train model\n",
    "# --------------------------------------------------------------------\n",
    "model = compile_model()\n",
    "callbacks = make_callbacks()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Save final model\n",
    "# --------------------------------------------------------------------\n",
    "model.save(\"ismayil-final.h5\")\n",
    "print(\"[SAVE] Model saved as cole-final.h5\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Validation Confusion Matrix\n",
    "# --------------------------------------------------------------------\n",
    "y_prob = model.predict(X_val, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "y_true = y_val.astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "eps = 1e-12\n",
    "\n",
    "acc  = (tp + tn) / max(tp + tn + fp + fn, eps)\n",
    "prec = tp / max(tp + fp, eps)\n",
    "rec  = tp / max(tp + fn, eps)\n",
    "f1   = 2 * prec * rec / max(prec + rec, eps)\n",
    "\n",
    "print(\"\\n[CONFUSION MATRIX]\")\n",
    "print(cm)\n",
    "print(f\"[VAL] acc={acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e2a13-6367-41ca-be66-a43594c2940e",
   "metadata": {},
   "source": [
    "Cell 8 — Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ac6662-33c1-4e25-b6c5-d47c860fba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Preparing TEST data...\n",
      "[TEST] Xte shape: (300, 14, 128, 1)\n",
      "[TEST] yte shape: (300,), class distribution: (array([0]), array([300]))\n",
      "[TEST] Loading final model: ismayil-final.h5\n",
      "[TEST] Running inference on TEST subject...\n",
      "[TEST] Mean predicted probability: 0.1603\n",
      "\n",
      "[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[257  43]\n",
      " [  0   0]]\n",
      "Accuracy : 0.8567\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-score : 0.0000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Inference on TEST subject (adam) using FINAL EEGNet\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Prepare TEST data\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Preparing TEST data...\")\n",
    "\n",
    "Xte = X_test_norm\n",
    "yte = y_test_norm.astype(int)\n",
    "\n",
    "print(f\"[TEST] Xte shape: {Xte.shape}\")\n",
    "print(f\"[TEST] yte shape: {yte.shape}, class distribution: {np.unique(yte, return_counts=True)}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Load FINAL trained model\n",
    "# ------------------------------------------------------------------\n",
    "model_path = \"ismayil-final.h5\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        \"Final model 'cole-final.h5' not found. \"\n",
    "        \"Please run Cell 4 (80:20 training) first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[TEST] Loading final model: {model_path}\")\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Run inference on TEST subject\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Running inference on TEST subject...\")\n",
    "\n",
    "y_prob = model.predict(Xte, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(f\"[TEST] Mean predicted probability: {y_prob.mean():.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Metrics on TEST subject (segment-level)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc = accuracy_score(yte, y_pred)\n",
    "prec = precision_score(yte, y_pred, zero_division=0)\n",
    "rec = recall_score(yte, y_pred, zero_division=0)\n",
    "f1 = f1_score(yte, y_pred, zero_division=0)\n",
    "\n",
    "print(\"\\n[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572dbf3e-bbb5-4865-883f-30e374700949",
   "metadata": {},
   "source": [
    "Cell 9 - Performance Matrices for each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a57240-50ea-4736-a7ac-52ef7878b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] Verifying predictions and labels are available...\n",
      "[CHECK] Found predictions for 300 test segments.\n",
      "\n",
      "[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[257  43]\n",
      " [  0   0]]\n",
      "Accuracy : 0.8567\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-score : 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Final model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================\n",
    "# Cell 6: Confusion Matrix & Metrics on TEST (FINAL EEGNet)\n",
    "# =======================================================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# ------------------------------------------------------------------\n",
    "def compute_metrics_from_cm(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    eps = 1e-12\n",
    "    acc  = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec  = tp / (tp + fn + eps)\n",
    "    f1   = 2 * prec * rec / (prec + rec + eps)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], ['0', '1'])\n",
    "    plt.yticks([0, 1], ['0', '1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(\n",
    "                j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Validate upstream artifacts\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[CHECK] Verifying predictions and labels are available...\")\n",
    "\n",
    "if 'y_pred' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Predictions 'y_pred' not found. \"\n",
    "        \"Please run Cell 5 (final model inference) first.\"\n",
    "    )\n",
    "\n",
    "if 'yte' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Test labels 'yte' not found. \"\n",
    "        \"Please run Cell 5 first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[CHECK] Found predictions for {len(y_pred)} test segments.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Confusion Matrix & Metrics (FINAL model)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc, prec, rec, f1 = compute_metrics_from_cm(cm)\n",
    "\n",
    "print(\"\\n[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(cm, title=\"Confusion Matrix - FINAL EEGNet\")\n",
    "\n",
    "print(\"[DONE] Final model evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0f120-39fd-497e-9098-05e6c82f4807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
