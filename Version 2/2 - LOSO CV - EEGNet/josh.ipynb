{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af5b471-cea9-4194-bc23-da5cdc2f9c67",
   "metadata": {},
   "source": [
    "Cell 1 — Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64aaa897-ab61-4eb4-a558-4c078acf751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Base path set to:\n",
      "C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\n",
      "[INFO] Found 18 folders:\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Selected test subject (LOSO): Josh\n",
      "[INFO] Training subjects (17):\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Sampling rate (SFREQ) = 128.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 1: Setup & discovery\n",
    "# =========================\n",
    "\n",
    "import os  # file ops\n",
    "import glob  # file matching\n",
    "import numpy as np  # arrays\n",
    "import random  # seeds\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data path\n",
    "BASE_PATH = r\"C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\"\n",
    "print(f\"[INFO] Base path set to:\\n{BASE_PATH}\")\n",
    "\n",
    "# List subject folders\n",
    "all_folders = [p for p in sorted(os.listdir(BASE_PATH)) if os.path.isdir(os.path.join(BASE_PATH, p))]\n",
    "print(f\"[INFO] Found {len(all_folders)} folders:\\n{all_folders}\")\n",
    "\n",
    "# Expected count (change if needed)\n",
    "assert len(all_folders) == 18, f\"Expected 18 folders, found {len(all_folders)}.\"\n",
    "\n",
    "# LOSO test subject\n",
    "TEST_SUBJECT = \"Josh\"\n",
    "print(f\"[INFO] Selected test subject (LOSO): {TEST_SUBJECT}\")\n",
    "\n",
    "# Train subjects = remaining\n",
    "train_subjects = [s for s in all_folders if s.lower() != TEST_SUBJECT.lower()]\n",
    "print(f\"[INFO] Training subjects ({len(train_subjects)}):\\n{train_subjects}\")\n",
    "\n",
    "# Sampling rate\n",
    "SFREQ = 128.0\n",
    "print(f\"[INFO] Sampling rate (SFREQ) = {SFREQ} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e649681-2861-4ce2-bbd1-0d9dd3359f42",
   "metadata": {},
   "source": [
    "Cell 2 — Load EEG + labels per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943c5c6e-a7aa-4618-899c-daf341c25358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 2] Loading data from folders...\n",
      "[STEP 2]       Amin | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Amin | EEG=          Amin.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Amin1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]      Amin1 | EEG=         Amin1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Cole | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Cole | EEG=          Cole.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Daniel | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]     Daniel | EEG=        Daniel.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]    Ismayil | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]    Ismayil | EEG=       Ismayil.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Jack | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Jack | EEG=          Jack.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]      James | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      James | EEG=         James.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Josh | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Josh | EEG=          Josh.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Marjan | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Marjan | EEG=        Marjan.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]        Max | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Max | EEG=           Max.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Mina | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mina | EEG=          Mina.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 1 | EEG=        Mina 1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 3 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 3 | EEG=        Mina 3.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]   Mohammad | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]   Mohammad | EEG=      Mohammad.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Mona | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mona | EEG=          Mona.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Roddy | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      Roddy | EEG=         Roddy.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]        Sam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Sam | EEG=           Sam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       adam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       adam | EEG=          adam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2] Loading complete ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 2: Load data (EMPTY LABEL => CLASS 0)\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path  # helper\n",
    "\n",
    "def load_subject(subj_dir: str, subj_name: str, expected_channels: int = 14):\n",
    "    \"\"\"\n",
    "    EEG file: {subj_name}.npy\n",
    "    Label file: {subj_name}_target.npy\n",
    "    Label rule:\n",
    "      - empty array -> class 0\n",
    "      - [0] -> class 0\n",
    "      - [1] -> class 1\n",
    "    \"\"\"\n",
    "\n",
    "    # EEG file path\n",
    "    eeg_file = os.path.join(subj_dir, f\"{subj_name}.npy\")\n",
    "    assert os.path.exists(eeg_file), f\"[ERROR] Missing EEG file: {eeg_file}\"\n",
    "\n",
    "    # Label file path\n",
    "    target_file = os.path.join(subj_dir, f\"{subj_name}_target.npy\")\n",
    "    assert os.path.exists(target_file), f\"[ERROR] Missing label file: {target_file}\"\n",
    "\n",
    "    # Load EEG\n",
    "    X_raw = np.load(eeg_file, allow_pickle=True)\n",
    "    X_raw = np.asarray(X_raw)\n",
    "\n",
    "    # Standardize EEG to (n_trials, 14, T)\n",
    "    if X_raw.ndim == 2:\n",
    "        X_trials = X_raw[None, :, :]\n",
    "    elif X_raw.ndim == 3:\n",
    "        dims = list(X_raw.shape)\n",
    "        chan_axes = [i for i, d in enumerate(dims) if d == expected_channels]\n",
    "        chan_axis = chan_axes[0] if len(chan_axes) == 1 else 1\n",
    "        time_axis = int(np.argmax(dims))\n",
    "        trial_axis = [i for i in range(3) if i not in (chan_axis, time_axis)][0]\n",
    "        X_trials = np.moveaxis(X_raw, (trial_axis, chan_axis, time_axis), (0, 1, 2))\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] Unsupported EEG shape {X_raw.shape} in {eeg_file}\")\n",
    "\n",
    "    assert X_trials.shape[1] == expected_channels, f\"[ERROR] Expected {expected_channels} channels, got {X_trials.shape[1]} in {eeg_file}\"\n",
    "\n",
    "    # Load label\n",
    "    y_raw = np.load(target_file, allow_pickle=True)\n",
    "    y_arr = np.asarray(y_raw).ravel()\n",
    "\n",
    "    # Apply your rule\n",
    "    if y_arr.size == 0:\n",
    "        label = 0\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\")\n",
    "    else:\n",
    "        label = int(y_arr[0])\n",
    "        if label not in (0, 1):\n",
    "            raise ValueError(f\"[ERROR] Invalid label {label} in {target_file} (must be 0 or 1)\")\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape={y_arr.shape}, value={y_arr} -> class={label}\")\n",
    "\n",
    "    # Repeat label for all trials\n",
    "    y_trials = np.full((X_trials.shape[0],), label, dtype=np.int32)\n",
    "\n",
    "    return X_trials.astype(np.float32), y_trials.astype(np.int32), eeg_file, target_file\n",
    "\n",
    "\n",
    "# Load all subjects\n",
    "subject_data = {}\n",
    "print(\"[STEP 2] Loading data from folders...\")\n",
    "\n",
    "for subj in all_folders:\n",
    "    subj_dir = os.path.join(BASE_PATH, subj)\n",
    "    X_trials, y_trials, eeg_file, target_file = load_subject(subj_dir, subj)\n",
    "\n",
    "    subject_data[subj] = (X_trials, y_trials)\n",
    "\n",
    "    print(f\"[STEP 2] {subj:>10} | EEG={Path(eeg_file).name:>18} | X={X_trials.shape} | y={y_trials.shape} | class={y_trials[0]}\")\n",
    "\n",
    "print(\"[STEP 2] Loading complete ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd93ca8-a94c-485c-8a61-e6e7441f6299",
   "metadata": {},
   "source": [
    "Cell 3 — Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733e592c-98e1-4cbd-afd2-6fdc8cf407f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 3] Preprocessing functions ready ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3: Preprocessing functions\n",
    "# =========================\n",
    "\n",
    "import mne  # EEG processing\n",
    "import pywt  # wavelets\n",
    "from sklearn.decomposition import FastICA  # ICA\n",
    "from typing import Optional, Tuple, Union, Sequence, Dict, List  # typing\n",
    "\n",
    "def wavelet_enhanced_ica(\n",
    "    data: np.ndarray,\n",
    "    n_components: int = 10,\n",
    "    wavelet: str = \"db4\",\n",
    "    level: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> np.ndarray:\n",
    "    n_ch, n_t = data.shape\n",
    "    n_components = min(n_components, n_ch)\n",
    "\n",
    "    coeffs = pywt.wavedec(data, wavelet=wavelet, level=level, axis=1)\n",
    "    A = coeffs[0]\n",
    "\n",
    "    ica = FastICA(n_components=n_components, random_state=random_state)\n",
    "    S = ica.fit_transform(A.T).T\n",
    "    A_denoised = ica.inverse_transform(S.T).T\n",
    "\n",
    "    coeffs[0] = A_denoised\n",
    "    cleaned = pywt.waverec(coeffs, wavelet=wavelet, axis=1)\n",
    "\n",
    "    if cleaned.shape[1] != n_t:\n",
    "        if cleaned.shape[1] > n_t:\n",
    "            cleaned = cleaned[:, :n_t]\n",
    "        else:\n",
    "            cleaned = np.pad(cleaned, ((0, 0), (0, n_t - cleaned.shape[1])), mode=\"constant\")\n",
    "    return cleaned\n",
    "\n",
    "def _names_from_index_mapping(n_channels: int, index_to_name: Optional[Dict[int, str]]) -> List[str]:\n",
    "    if index_to_name is None:\n",
    "        return [f\"EEG{i+1}\" for i in range(n_channels)]\n",
    "    keys = list(index_to_name.keys())\n",
    "    is_zero_based = (0 in keys) and (1 not in keys)\n",
    "    names = []\n",
    "    for i in range(n_channels):\n",
    "        key = i if is_zero_based else (i + 1)\n",
    "        names.append(index_to_name.get(key, f\"EEG{i+1}\"))\n",
    "    return names\n",
    "\n",
    "def preprocess_eeg(\n",
    "    eeg: np.ndarray,\n",
    "    sfreq: float,\n",
    "    *,\n",
    "    index_to_name: Optional[Dict[int, str]] = None,\n",
    "    use_standard_1010: bool = True,\n",
    "    resample_to: Optional[float] = None,\n",
    "    notch_freqs: Union[None, float, Sequence[float]] = 50.0,\n",
    "    highpass: Optional[float] = 0.05,\n",
    "    bad_point_z: float = 6.0,\n",
    "    bad_channel_z: float = 5.0,\n",
    "    interpolate_bad_channels: bool = True,\n",
    "    car: bool = True,\n",
    "    use_wica: bool = True,\n",
    "    wica_components: int = 10,\n",
    "    wica_wavelet: str = \"db4\",\n",
    "    wica_level: int = 3,\n",
    "    wica_random_state: int = 42,\n",
    "    return_raw: bool = False,\n",
    "):\n",
    "\n",
    "    eeg = np.asarray(eeg, dtype=np.float32)\n",
    "    assert eeg.ndim == 2, \"eeg must be 2D: (n_channels, n_times)\"\n",
    "\n",
    "    n_channels, _ = eeg.shape\n",
    "    ch_names = _names_from_index_mapping(n_channels, index_to_name)\n",
    "    ch_types = ['eog' if str(n).upper().startswith(\"EOG\") else 'eeg' for n in ch_names]\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    raw = mne.io.RawArray(eeg, info, verbose=False)\n",
    "\n",
    "    montage_applied = False\n",
    "    if use_standard_1010:\n",
    "        try:\n",
    "            mont = mne.channels.make_standard_montage(\"standard_1010\")\n",
    "            raw.set_montage(mont, match_case=False, on_missing=\"ignore\")\n",
    "            montage_applied = True\n",
    "        except Exception:\n",
    "            montage_applied = False\n",
    "\n",
    "    if resample_to is not None and float(resample_to) != float(sfreq):\n",
    "        raw.resample(sfreq=resample_to, npad=\"auto\")\n",
    "    sfreq_out = float(raw.info[\"sfreq\"])\n",
    "\n",
    "    if notch_freqs is not None:\n",
    "        raw.notch_filter(freqs=notch_freqs, verbose=False)\n",
    "    if highpass is not None:\n",
    "        raw.filter(l_freq=highpass, h_freq=None, verbose=False)\n",
    "\n",
    "    X = raw.get_data()\n",
    "    mu = np.mean(X, axis=1, keepdims=True)\n",
    "    sd = np.std(X, axis=1, keepdims=True) + 1e-12\n",
    "    hi = mu + bad_point_z * sd\n",
    "    lo = mu - bad_point_z * sd\n",
    "    bad_idx = (X > hi) | (X < lo)\n",
    "\n",
    "    if np.any(bad_idx):\n",
    "        X_fixed = X.copy()\n",
    "        t = np.arange(X.shape[1], dtype=float)\n",
    "        for ch in range(n_channels):\n",
    "            mask = bad_idx[ch]\n",
    "            if mask.any():\n",
    "                good = ~mask\n",
    "                if good.sum() >= 2:\n",
    "                    X_fixed[ch, mask] = np.interp(t[mask], t[good], X_fixed[ch, good])\n",
    "        raw._data = X_fixed\n",
    "\n",
    "    if interpolate_bad_channels and montage_applied:\n",
    "        X = raw.get_data(picks=\"eeg\")\n",
    "        if X.size > 0:\n",
    "            ch_std = X.std(axis=1)\n",
    "            med = np.median(ch_std)\n",
    "            mad = np.median(np.abs(ch_std - med)) + 1e-12\n",
    "            z = 0.6745 * (ch_std - med) / mad\n",
    "            eeg_names = mne.pick_info(raw.info, mne.pick_types(raw.info, eeg=True)).ch_names\n",
    "            bads = [eeg_names[i] for i in np.where(np.abs(z) > bad_channel_z)[0]]\n",
    "            raw.info[\"bads\"] = bads\n",
    "            if bads:\n",
    "                raw.interpolate_bads(reset_bads=True, verbose=False)\n",
    "\n",
    "    if car:\n",
    "        raw.set_eeg_reference(\"average\", projection=True)\n",
    "        raw.apply_proj()\n",
    "\n",
    "    if use_wica:\n",
    "        cleaned = wavelet_enhanced_ica(\n",
    "            raw.get_data(),\n",
    "            n_components=wica_components,\n",
    "            wavelet=wica_wavelet,\n",
    "            level=wica_level,\n",
    "            random_state=wica_random_state\n",
    "        )\n",
    "        raw = mne.io.RawArray(cleaned, raw.info, verbose=False)\n",
    "\n",
    "    cleaned = raw.get_data()\n",
    "    return cleaned, sfreq_out\n",
    "\n",
    "print(\"[STEP 3] Preprocessing functions ready ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a9d20-889d-45c4-bbc2-de62216908cd",
   "metadata": {},
   "source": [
    "Cell 4 — Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bfcbde0-1cc2-409c-8627-93da46606661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 4] Starting preprocessing...\n",
      "[INFO] USE_WICA = True\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 01/17: Amin | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 02/17: Amin1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 03/17: Cole | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 04/17: Daniel | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 05/17: Ismayil | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 06/17: Jack | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 07/17: James | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 08/17: Marjan | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 09/17: Max | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 10/17: Mina | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 11/17: Mina 1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 12/17: Mina 3 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 13/17: Mohammad | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 14/17: Mona | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 15/17: Roddy | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 16/17: Sam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 17/17: adam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Test subject: Josh | trials=1 | fs_out=128.0\n",
      "[STEP 4] Preprocessing done ✅\n",
      "  X_train_trials_all: (17, 14, 38400)\n",
      "  y_train_trials_all: (17,) (array([0, 1], dtype=int32), array([9, 8]))\n",
      "  X_test_trials_all : (1, 14, 38400)\n",
      "  y_test_trials_all : (1,) (array([1], dtype=int32), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 4: Apply preprocessing (LOSO)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 4] Starting preprocessing...\")\n",
    "\n",
    "USE_WICA = True\n",
    "print(f\"[INFO] USE_WICA = {USE_WICA}\")\n",
    "\n",
    "X_train_trials_all, y_train_trials_all = [], []\n",
    "X_test_trials_all, y_test_trials_all = [], []\n",
    "\n",
    "def preprocess_trials(X_trials, y_trials):\n",
    "    cleaned_list = []\n",
    "    label_list = []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        trial = X_trials[i]\n",
    "        label = int(y_trials[i])\n",
    "\n",
    "        cleaned, fs_out = preprocess_eeg(\n",
    "            eeg=trial,\n",
    "            sfreq=SFREQ,\n",
    "            use_wica=USE_WICA\n",
    "        )\n",
    "\n",
    "        cleaned_list.append(cleaned.astype(np.float32, copy=False))\n",
    "        label_list.append(label)\n",
    "\n",
    "    return cleaned_list, np.array(label_list, dtype=np.int32), fs_out\n",
    "\n",
    "# Train subjects\n",
    "for idx, subj in enumerate(train_subjects, start=1):\n",
    "    X_trials, y_trials = subject_data[subj]\n",
    "    cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "    X_train_trials_all.extend(cleaned_trials)\n",
    "    y_train_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "    print(f\"[STEP 4] Train {idx:02d}/{len(train_subjects)}: {subj} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Test subject\n",
    "X_trials, y_trials = subject_data[TEST_SUBJECT]\n",
    "cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "X_test_trials_all.extend(cleaned_trials)\n",
    "y_test_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "print(f\"[STEP 4] Test subject: {TEST_SUBJECT} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Convert to arrays\n",
    "X_train_trials_all = np.array(X_train_trials_all, dtype=np.float32)\n",
    "y_train_trials_all = np.array(y_train_trials_all, dtype=np.int32)\n",
    "\n",
    "X_test_trials_all = np.array(X_test_trials_all, dtype=np.float32)\n",
    "y_test_trials_all = np.array(y_test_trials_all, dtype=np.int32)\n",
    "\n",
    "print(\"[STEP 4] Preprocessing done ✅\")\n",
    "print(\"  X_train_trials_all:\", X_train_trials_all.shape)\n",
    "print(\"  y_train_trials_all:\", y_train_trials_all.shape, np.unique(y_train_trials_all, return_counts=True))\n",
    "print(\"  X_test_trials_all :\", X_test_trials_all.shape)\n",
    "print(\"  y_test_trials_all :\", y_test_trials_all.shape, np.unique(y_test_trials_all, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc3da1-2967-4ee3-84cd-bfcf0f98c0b1",
   "metadata": {},
   "source": [
    "Cell 5 — Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c21ad54-7001-4039-9360-29dfa2905127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 5] Starting segmentation...\n",
      "[STEP 5] Segmented 5/17 trials...\n",
      "[STEP 5] Segmented 10/17 trials...\n",
      "[STEP 5] Segmented 15/17 trials...\n",
      "[STEP 5] Segmented 17/17 trials...\n",
      "[STEP 5] Segmented 1/1 trials...\n",
      "[STEP 5] Segmentation done ✅\n",
      "  X_train: (5100, 14, 128, 1)\n",
      "  y_train: (5100,) (array([0, 1], dtype=int32), array([2700, 2400]))\n",
      "  X_test : (300, 14, 128, 1)\n",
      "  y_test : (300,) (array([1], dtype=int32), array([300]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 5: Segmentation\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 5] Starting segmentation...\")\n",
    "\n",
    "def segment_trial(trial_2d: np.ndarray, label: int, segment_len: int = 128):\n",
    "    C, T = trial_2d.shape\n",
    "    n_segs = T // segment_len\n",
    "    use_T = n_segs * segment_len\n",
    "\n",
    "    if n_segs == 0:\n",
    "        return np.empty((0, C, segment_len), dtype=np.float32), np.empty((0,), dtype=np.int32)\n",
    "\n",
    "    trial_trim = trial_2d[:, :use_T]\n",
    "    Xsegs = trial_trim.reshape(C, n_segs, segment_len).transpose(1, 0, 2)\n",
    "    ysegs = np.full((n_segs,), int(label), dtype=np.int32)\n",
    "    return Xsegs.astype(np.float32, copy=False), ysegs\n",
    "\n",
    "def segment_dataset(X_trials: np.ndarray, y_trials: np.ndarray, segment_len: int = 128):\n",
    "    X_all, y_all = [], []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        Xsegs, ysegs = segment_trial(X_trials[i], int(y_trials[i]), segment_len=segment_len)\n",
    "        X_all.append(Xsegs)\n",
    "        y_all.append(ysegs)\n",
    "\n",
    "        if (i + 1) % 5 == 0 or (i + 1) == X_trials.shape[0]:\n",
    "            print(f\"[STEP 5] Segmented {i+1}/{X_trials.shape[0]} trials...\")\n",
    "\n",
    "    X_out = np.concatenate(X_all, axis=0)\n",
    "    y_out = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    X_out = np.expand_dims(X_out, axis=-1)  # (N,14,128,1)\n",
    "    return X_out.astype(np.float32), y_out.astype(np.int32)\n",
    "\n",
    "X_train, y_train = segment_dataset(X_train_trials_all, y_train_trials_all, segment_len=128)\n",
    "X_test,  y_test  = segment_dataset(X_test_trials_all,  y_test_trials_all,  segment_len=128)\n",
    "\n",
    "print(\"[STEP 5] Segmentation done ✅\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  y_train:\", y_train.shape, np.unique(y_train, return_counts=True))\n",
    "print(\"  X_test :\", X_test.shape)\n",
    "print(\"  y_test :\", y_test.shape, np.unique(y_test, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b4ffd5-bf2f-490e-8854-0a2fc747f1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e69ba7-3952-47b1-bd80-f05237b2e018",
   "metadata": {},
   "source": [
    "Cell 6 — Split train/val + Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97fda4ae-e1c8-4fd1-bf0c-c82ae2a85764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 6] Normalizing using ALL training data (no train/val split)...\n",
      "[STEP 6] Normalization done ✅\n",
      "  X_train_norm: (5100, 14, 128, 1)\n",
      "  X_test_norm : (300, 14, 128, 1)\n",
      "  y_train_norm: (5100,)\n",
      "  y_test_norm : (300,)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 6: Normalize (ALL training data)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 6] Normalizing using ALL training data (no train/val split)...\")\n",
    "\n",
    "def normalize_eeg(train_data: np.ndarray, test_data: np.ndarray):\n",
    "    if train_data.ndim == 5:\n",
    "        train_flat = train_data.reshape(-1, *train_data.shape[2:])\n",
    "        test_flat  = test_data.reshape(-1, *test_data.shape[2:])\n",
    "    else:\n",
    "        train_flat, test_flat = train_data, test_data\n",
    "\n",
    "    # Compute stats on ALL training data\n",
    "    train_mean = np.mean(train_flat, axis=(0, 2, 3), keepdims=True)\n",
    "    train_std  = np.std(train_flat,  axis=(0, 2, 3), keepdims=True) + 1e-8\n",
    "\n",
    "    train_norm = (train_flat - train_mean) / train_std\n",
    "    test_norm  = (test_flat  - train_mean) / train_std\n",
    "\n",
    "    if train_data.ndim == 5:\n",
    "        train_norm = train_norm.reshape(train_data.shape)\n",
    "        test_norm  = test_norm.reshape(test_data.shape)\n",
    "\n",
    "    return (\n",
    "        train_norm.astype(np.float32),\n",
    "        test_norm.astype(np.float32),\n",
    "        train_mean,\n",
    "        train_std\n",
    "    )\n",
    "\n",
    "# Normalize ALL training data\n",
    "X_train_norm, X_test_norm, mean, std = normalize_eeg(X_train, X_test)\n",
    "\n",
    "y_train_norm = y_train.astype(np.float32)\n",
    "y_test_norm  = y_test.astype(np.float32)\n",
    "\n",
    "print(\"[STEP 6] Normalization done ✅\")\n",
    "print(\"  X_train_norm:\", X_train_norm.shape)\n",
    "print(\"  X_test_norm :\", X_test_norm.shape)\n",
    "print(\"  y_train_norm:\", y_train_norm.shape)\n",
    "print(\"  y_test_norm :\", y_test_norm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f527c-34db-4c23-bdbd-c8ce03b519f0",
   "metadata": {},
   "source": [
    "Cell 7 — EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ea4279-53c2-49c1-b834-7b1e1c20775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\DL\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Xtr: (5100, 14, 128, 1), ytr: (5100,)\n",
      "[INFO] Test / LOSO subject is NOT used here.\n",
      "[SPLIT] Train: (4080, 14, 128, 1), Val: (1020, 14, 128, 1)\n",
      "Epoch 1/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 134ms/step - accuracy: 0.4961 - loss: 0.7391 - val_accuracy: 0.5294 - val_loss: 0.6913 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.5478 - loss: 0.6875 - val_accuracy: 0.5284 - val_loss: 0.6910 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.5848 - loss: 0.6704 - val_accuracy: 0.5284 - val_loss: 0.6902 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.6157 - loss: 0.6441 - val_accuracy: 0.5343 - val_loss: 0.6886 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.6355 - loss: 0.6229 - val_accuracy: 0.5480 - val_loss: 0.6842 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.6593 - loss: 0.6024 - val_accuracy: 0.5529 - val_loss: 0.6762 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.6740 - loss: 0.5808 - val_accuracy: 0.5745 - val_loss: 0.6645 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.6968 - loss: 0.5563 - val_accuracy: 0.5931 - val_loss: 0.6490 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.7125 - loss: 0.5424 - val_accuracy: 0.5931 - val_loss: 0.6423 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.7248 - loss: 0.5281 - val_accuracy: 0.6245 - val_loss: 0.6216 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.7321 - loss: 0.5132 - val_accuracy: 0.6637 - val_loss: 0.5985 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.7517 - loss: 0.4925 - val_accuracy: 0.6588 - val_loss: 0.5796 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7679 - loss: 0.4753 - val_accuracy: 0.6843 - val_loss: 0.5587 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7681 - loss: 0.4648 - val_accuracy: 0.7176 - val_loss: 0.5466 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.7765 - loss: 0.4632 - val_accuracy: 0.7304 - val_loss: 0.5214 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 127ms/step - accuracy: 0.8020 - loss: 0.4386 - val_accuracy: 0.7490 - val_loss: 0.5138 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.7958 - loss: 0.4275 - val_accuracy: 0.7412 - val_loss: 0.5264 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.8135 - loss: 0.4123 - val_accuracy: 0.7843 - val_loss: 0.4719 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.8250 - loss: 0.3963 - val_accuracy: 0.7539 - val_loss: 0.5031 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.8142 - loss: 0.3980 - val_accuracy: 0.7147 - val_loss: 0.5463 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.8162 - loss: 0.3995 - val_accuracy: 0.7980 - val_loss: 0.4414 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8363 - loss: 0.3743 - val_accuracy: 0.7990 - val_loss: 0.4371 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8431 - loss: 0.3568 - val_accuracy: 0.8196 - val_loss: 0.4127 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8527 - loss: 0.3478 - val_accuracy: 0.8402 - val_loss: 0.4040 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8618 - loss: 0.3325 - val_accuracy: 0.8275 - val_loss: 0.4153 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8627 - loss: 0.3217 - val_accuracy: 0.8412 - val_loss: 0.3919 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8740 - loss: 0.3114 - val_accuracy: 0.8363 - val_loss: 0.3721 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.8855 - loss: 0.2715 - val_accuracy: 0.8775 - val_loss: 0.3308 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.8801 - loss: 0.2847 - val_accuracy: 0.8814 - val_loss: 0.3137 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.8882 - loss: 0.2726 - val_accuracy: 0.8814 - val_loss: 0.2918 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8961 - loss: 0.2448 - val_accuracy: 0.8765 - val_loss: 0.2915 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.8944 - loss: 0.2589 - val_accuracy: 0.8971 - val_loss: 0.2964 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.8831 - loss: 0.2920 - val_accuracy: 0.8676 - val_loss: 0.3758 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.8958 - loss: 0.2655 - val_accuracy: 0.9147 - val_loss: 0.2395 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9093 - loss: 0.2377 - val_accuracy: 0.9176 - val_loss: 0.2294 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 126ms/step - accuracy: 0.9113 - loss: 0.2291 - val_accuracy: 0.9255 - val_loss: 0.2025 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9137 - loss: 0.2333 - val_accuracy: 0.9245 - val_loss: 0.2101 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.9159 - loss: 0.2143 - val_accuracy: 0.9225 - val_loss: 0.2057 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9167 - loss: 0.2102 - val_accuracy: 0.9314 - val_loss: 0.1883 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.9201 - loss: 0.2064 - val_accuracy: 0.9333 - val_loss: 0.2054 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9081 - loss: 0.2232 - val_accuracy: 0.9520 - val_loss: 0.1551 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9228 - loss: 0.1991 - val_accuracy: 0.9510 - val_loss: 0.1511 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9201 - loss: 0.2012 - val_accuracy: 0.9461 - val_loss: 0.1508 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 127ms/step - accuracy: 0.9287 - loss: 0.1955 - val_accuracy: 0.9471 - val_loss: 0.1487 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 127ms/step - accuracy: 0.9255 - loss: 0.1867 - val_accuracy: 0.9510 - val_loss: 0.1358 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.9230 - loss: 0.1902 - val_accuracy: 0.9461 - val_loss: 0.1789 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9174 - loss: 0.1939 - val_accuracy: 0.9461 - val_loss: 0.1324 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9284 - loss: 0.1906 - val_accuracy: 0.9431 - val_loss: 0.1897 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9241 - loss: 0.1942\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9250 - loss: 0.1911 - val_accuracy: 0.9510 - val_loss: 0.1345 - learning_rate: 0.0010\n",
      "Epoch 50/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9373 - loss: 0.1704 - val_accuracy: 0.9569 - val_loss: 0.1302 - learning_rate: 5.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9346 - loss: 0.1667 - val_accuracy: 0.9539 - val_loss: 0.1347 - learning_rate: 5.0000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9265 - loss: 0.1871 - val_accuracy: 0.9549 - val_loss: 0.1294 - learning_rate: 5.0000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9338 - loss: 0.1729 - val_accuracy: 0.9588 - val_loss: 0.1202 - learning_rate: 5.0000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9387 - loss: 0.1655 - val_accuracy: 0.9549 - val_loss: 0.1187 - learning_rate: 5.0000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9314 - loss: 0.1720 - val_accuracy: 0.9608 - val_loss: 0.1172 - learning_rate: 5.0000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9319 - loss: 0.1662 - val_accuracy: 0.9569 - val_loss: 0.1162 - learning_rate: 5.0000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9365 - loss: 0.1660 - val_accuracy: 0.9627 - val_loss: 0.1254 - learning_rate: 5.0000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9355 - loss: 0.1653 - val_accuracy: 0.9618 - val_loss: 0.1271 - learning_rate: 5.0000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9387 - loss: 0.1611 - val_accuracy: 0.9569 - val_loss: 0.1153 - learning_rate: 5.0000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.9324 - loss: 0.1672 - val_accuracy: 0.9559 - val_loss: 0.1335 - learning_rate: 5.0000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.9414 - loss: 0.1515 - val_accuracy: 0.9598 - val_loss: 0.1131 - learning_rate: 5.0000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.9387 - loss: 0.1644 - val_accuracy: 0.9510 - val_loss: 0.1276 - learning_rate: 5.0000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9365 - loss: 0.1592 - val_accuracy: 0.9588 - val_loss: 0.1233 - learning_rate: 5.0000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9360 - loss: 0.1524 - val_accuracy: 0.9510 - val_loss: 0.1338 - learning_rate: 5.0000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.9403 - loss: 0.1573 \n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9382 - loss: 0.1583 - val_accuracy: 0.9461 - val_loss: 0.1330 - learning_rate: 5.0000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9409 - loss: 0.1546 - val_accuracy: 0.9588 - val_loss: 0.1197 - learning_rate: 2.5000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9461 - loss: 0.1460 - val_accuracy: 0.9588 - val_loss: 0.1231 - learning_rate: 2.5000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9390 - loss: 0.1479 - val_accuracy: 0.9588 - val_loss: 0.1314 - learning_rate: 2.5000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9417 - loss: 0.1594 - val_accuracy: 0.9539 - val_loss: 0.1325 - learning_rate: 2.5000e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9373 - loss: 0.1591 - val_accuracy: 0.9588 - val_loss: 0.1228 - learning_rate: 2.5000e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9346 - loss: 0.1580\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9333 - loss: 0.1648 - val_accuracy: 0.9618 - val_loss: 0.1136 - learning_rate: 2.5000e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9355 - loss: 0.1625 - val_accuracy: 0.9657 - val_loss: 0.1117 - learning_rate: 1.2500e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9402 - loss: 0.1523 - val_accuracy: 0.9627 - val_loss: 0.1065 - learning_rate: 1.2500e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9417 - loss: 0.1456 - val_accuracy: 0.9647 - val_loss: 0.1135 - learning_rate: 1.2500e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9314 - loss: 0.1626 - val_accuracy: 0.9637 - val_loss: 0.1097 - learning_rate: 1.2500e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9441 - loss: 0.1450 - val_accuracy: 0.9647 - val_loss: 0.1097 - learning_rate: 1.2500e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9434 - loss: 0.1437 - val_accuracy: 0.9637 - val_loss: 0.1083 - learning_rate: 1.2500e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9422 - loss: 0.1449 - val_accuracy: 0.9657 - val_loss: 0.1075 - learning_rate: 1.2500e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9463 - loss: 0.1430 - val_accuracy: 0.9637 - val_loss: 0.1104 - learning_rate: 1.2500e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9446 - loss: 0.1411 - val_accuracy: 0.9637 - val_loss: 0.1121 - learning_rate: 1.2500e-04\n",
      "Epoch 81/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9473 - loss: 0.1413 - val_accuracy: 0.9647 - val_loss: 0.1085 - learning_rate: 1.2500e-04\n",
      "Epoch 82/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9451 - loss: 0.1453 - val_accuracy: 0.9637 - val_loss: 0.1070 - learning_rate: 1.2500e-04\n",
      "Epoch 83/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9453 - loss: 0.1468 - val_accuracy: 0.9647 - val_loss: 0.1032 - learning_rate: 1.2500e-04\n",
      "Epoch 84/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9512 - loss: 0.1326 - val_accuracy: 0.9657 - val_loss: 0.1118 - learning_rate: 1.2500e-04\n",
      "Epoch 85/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9456 - loss: 0.1445 - val_accuracy: 0.9627 - val_loss: 0.1112 - learning_rate: 1.2500e-04\n",
      "Epoch 86/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9412 - loss: 0.1466 - val_accuracy: 0.9618 - val_loss: 0.1101 - learning_rate: 1.2500e-04\n",
      "Epoch 87/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9414 - loss: 0.1471 - val_accuracy: 0.9657 - val_loss: 0.1009 - learning_rate: 1.2500e-04\n",
      "Epoch 88/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9411 - loss: 0.1427\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9424 - loss: 0.1469 - val_accuracy: 0.9647 - val_loss: 0.1029 - learning_rate: 1.2500e-04\n",
      "Epoch 89/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9502 - loss: 0.1369 - val_accuracy: 0.9637 - val_loss: 0.1123 - learning_rate: 6.2500e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9456 - loss: 0.1374 - val_accuracy: 0.9637 - val_loss: 0.1059 - learning_rate: 6.2500e-05\n",
      "Epoch 91/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9422 - loss: 0.1422 - val_accuracy: 0.9657 - val_loss: 0.1106 - learning_rate: 6.2500e-05\n",
      "Epoch 92/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9486 - loss: 0.1312\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9461 - loss: 0.1391 - val_accuracy: 0.9637 - val_loss: 0.1042 - learning_rate: 6.2500e-05\n",
      "Epoch 93/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9453 - loss: 0.1458 - val_accuracy: 0.9647 - val_loss: 0.1101 - learning_rate: 3.1250e-05\n",
      "Epoch 94/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9488 - loss: 0.1416 - val_accuracy: 0.9637 - val_loss: 0.1071 - learning_rate: 3.1250e-05\n",
      "Epoch 95/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9424 - loss: 0.1431 - val_accuracy: 0.9647 - val_loss: 0.1088 - learning_rate: 3.1250e-05\n",
      "Epoch 96/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9449 - loss: 0.1433\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9451 - loss: 0.1394 - val_accuracy: 0.9657 - val_loss: 0.1049 - learning_rate: 3.1250e-05\n",
      "Epoch 97/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9449 - loss: 0.1493 - val_accuracy: 0.9657 - val_loss: 0.1044 - learning_rate: 1.5625e-05\n",
      "Epoch 98/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9451 - loss: 0.1480 - val_accuracy: 0.9657 - val_loss: 0.1041 - learning_rate: 1.5625e-05\n",
      "Epoch 99/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9404 - loss: 0.1481 - val_accuracy: 0.9657 - val_loss: 0.1054 - learning_rate: 1.5625e-05\n",
      "Epoch 100/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9503 - loss: 0.1330\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9466 - loss: 0.1417 - val_accuracy: 0.9657 - val_loss: 0.1055 - learning_rate: 1.5625e-05\n",
      "Epoch 101/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9422 - loss: 0.1433 - val_accuracy: 0.9657 - val_loss: 0.1055 - learning_rate: 7.8125e-06\n",
      "Epoch 102/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9488 - loss: 0.1442 - val_accuracy: 0.9657 - val_loss: 0.1054 - learning_rate: 7.8125e-06\n",
      "Epoch 102: early stopping\n",
      "Restoring model weights from the end of the best epoch: 87.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] Model saved as cole-final.h5\n",
      "\n",
      "[CONFUSION MATRIX]\n",
      "[[524  16]\n",
      " [ 19 461]]\n",
      "[VAL] acc=0.9657, prec=0.9665, rec=0.9604, f1=0.9634\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 4: 80:20 Train / Validation Split (EEGNet)\n",
    "# ====================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, SeparableConv2D, BatchNormalization, Activation,\n",
    "    DepthwiseConv2D, Conv2D, AveragePooling2D,\n",
    "    Dropout, Dense, Flatten\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# --------------------------------------------------------------------\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# EEGNet model\n",
    "# --------------------------------------------------------------------\n",
    "def create_eegnet_model(input_shape, dropout_rate=0.5, num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(16, (1, 64), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = DepthwiseConv2D(\n",
    "        (input_shape[0], 1),\n",
    "        depth_multiplier=2,\n",
    "        padding='valid',\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 4))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = SeparableConv2D(16, (1, 16), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 8))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Compile model\n",
    "# --------------------------------------------------------------------\n",
    "def compile_model():\n",
    "    input_shape = (Xtr.shape[1], Xtr.shape[2], Xtr.shape[3])\n",
    "    model = create_eegnet_model(input_shape)\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-3),\n",
    "        loss=BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[BinaryAccuracy(name=\"accuracy\", threshold=0.5)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def make_callbacks():\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"loss\",\n",
    "            factor=0.5,\n",
    "            patience=4,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Use ALL normalized training data\n",
    "# --------------------------------------------------------------------\n",
    "Xtr = X_train_norm\n",
    "ytr = y_train_norm\n",
    "\n",
    "print(f\"[TRAIN] Xtr: {Xtr.shape}, ytr: {ytr.shape}\")\n",
    "print(\"[INFO] Test / LOSO subject is NOT used here.\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 80:20 Stratified Split\n",
    "# --------------------------------------------------------------------\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(sss.split(Xtr, ytr))\n",
    "\n",
    "X_train, X_val = Xtr[train_idx], Xtr[val_idx]\n",
    "y_train, y_val = ytr[train_idx], ytr[val_idx]\n",
    "\n",
    "print(f\"[SPLIT] Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Train model\n",
    "# --------------------------------------------------------------------\n",
    "model = compile_model()\n",
    "callbacks = make_callbacks()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Save final model\n",
    "# --------------------------------------------------------------------\n",
    "model.save(\"josh-final.h5\")\n",
    "print(\"[SAVE] Model saved as cole-final.h5\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Validation Confusion Matrix\n",
    "# --------------------------------------------------------------------\n",
    "y_prob = model.predict(X_val, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "y_true = y_val.astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "eps = 1e-12\n",
    "\n",
    "acc  = (tp + tn) / max(tp + tn + fp + fn, eps)\n",
    "prec = tp / max(tp + fp, eps)\n",
    "rec  = tp / max(tp + fn, eps)\n",
    "f1   = 2 * prec * rec / max(prec + rec, eps)\n",
    "\n",
    "print(\"\\n[CONFUSION MATRIX]\")\n",
    "print(cm)\n",
    "print(f\"[VAL] acc={acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e2a13-6367-41ca-be66-a43594c2940e",
   "metadata": {},
   "source": [
    "Cell 8 — Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ac6662-33c1-4e25-b6c5-d47c860fba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Preparing TEST data...\n",
      "[TEST] Xte shape: (300, 14, 128, 1)\n",
      "[TEST] yte shape: (300,), class distribution: (array([1]), array([300]))\n",
      "[TEST] Loading final model: josh-final.h5\n",
      "[TEST] Running inference on TEST subject...\n",
      "[TEST] Mean predicted probability: 0.9834\n",
      "\n",
      "[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[  0   0]\n",
      " [  1 299]]\n",
      "Accuracy : 0.9967\n",
      "Precision: 1.0000\n",
      "Recall   : 0.9967\n",
      "F1-score : 0.9983\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Inference on TEST subject (adam) using FINAL EEGNet\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Prepare TEST data\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Preparing TEST data...\")\n",
    "\n",
    "Xte = X_test_norm\n",
    "yte = y_test_norm.astype(int)\n",
    "\n",
    "print(f\"[TEST] Xte shape: {Xte.shape}\")\n",
    "print(f\"[TEST] yte shape: {yte.shape}, class distribution: {np.unique(yte, return_counts=True)}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Load FINAL trained model\n",
    "# ------------------------------------------------------------------\n",
    "model_path = \"josh-final.h5\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        \"Final model 'cole-final.h5' not found. \"\n",
    "        \"Please run Cell 4 (80:20 training) first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[TEST] Loading final model: {model_path}\")\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Run inference on TEST subject\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Running inference on TEST subject...\")\n",
    "\n",
    "y_prob = model.predict(Xte, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(f\"[TEST] Mean predicted probability: {y_prob.mean():.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Metrics on TEST subject (segment-level)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc = accuracy_score(yte, y_pred)\n",
    "prec = precision_score(yte, y_pred, zero_division=0)\n",
    "rec = recall_score(yte, y_pred, zero_division=0)\n",
    "f1 = f1_score(yte, y_pred, zero_division=0)\n",
    "\n",
    "print(\"\\n[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572dbf3e-bbb5-4865-883f-30e374700949",
   "metadata": {},
   "source": [
    "Cell 9 - Performance Matrices for each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a57240-50ea-4736-a7ac-52ef7878b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] Verifying predictions and labels are available...\n",
      "[CHECK] Found predictions for 300 test segments.\n",
      "\n",
      "[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[  0   0]\n",
      " [  1 299]]\n",
      "Accuracy : 0.9967\n",
      "Precision: 1.0000\n",
      "Recall   : 0.9967\n",
      "F1-score : 0.9983\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Final model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================\n",
    "# Cell 6: Confusion Matrix & Metrics on TEST (FINAL EEGNet)\n",
    "# =======================================================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# ------------------------------------------------------------------\n",
    "def compute_metrics_from_cm(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    eps = 1e-12\n",
    "    acc  = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec  = tp / (tp + fn + eps)\n",
    "    f1   = 2 * prec * rec / (prec + rec + eps)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], ['0', '1'])\n",
    "    plt.yticks([0, 1], ['0', '1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(\n",
    "                j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Validate upstream artifacts\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[CHECK] Verifying predictions and labels are available...\")\n",
    "\n",
    "if 'y_pred' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Predictions 'y_pred' not found. \"\n",
    "        \"Please run Cell 5 (final model inference) first.\"\n",
    "    )\n",
    "\n",
    "if 'yte' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Test labels 'yte' not found. \"\n",
    "        \"Please run Cell 5 first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[CHECK] Found predictions for {len(y_pred)} test segments.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Confusion Matrix & Metrics (FINAL model)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc, prec, rec, f1 = compute_metrics_from_cm(cm)\n",
    "\n",
    "print(\"\\n[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(cm, title=\"Confusion Matrix - FINAL EEGNet\")\n",
    "\n",
    "print(\"[DONE] Final model evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0f120-39fd-497e-9098-05e6c82f4807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
