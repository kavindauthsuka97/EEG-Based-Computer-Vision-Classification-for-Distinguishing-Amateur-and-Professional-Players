{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af5b471-cea9-4194-bc23-da5cdc2f9c67",
   "metadata": {},
   "source": [
    "Cell 1 — Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64aaa897-ab61-4eb4-a558-4c078acf751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Base path set to:\n",
      "C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\n",
      "[INFO] Found 18 folders:\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Selected test subject (LOSO): Daniel\n",
      "[INFO] Training subjects (17):\n",
      "['Amin', 'Amin1', 'Cole', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Sampling rate (SFREQ) = 128.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 1: Setup & discovery\n",
    "# =========================\n",
    "\n",
    "import os  # file ops\n",
    "import glob  # file matching\n",
    "import numpy as np  # arrays\n",
    "import random  # seeds\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data path\n",
    "BASE_PATH = r\"C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\"\n",
    "print(f\"[INFO] Base path set to:\\n{BASE_PATH}\")\n",
    "\n",
    "# List subject folders\n",
    "all_folders = [p for p in sorted(os.listdir(BASE_PATH)) if os.path.isdir(os.path.join(BASE_PATH, p))]\n",
    "print(f\"[INFO] Found {len(all_folders)} folders:\\n{all_folders}\")\n",
    "\n",
    "# Expected count (change if needed)\n",
    "assert len(all_folders) == 18, f\"Expected 18 folders, found {len(all_folders)}.\"\n",
    "\n",
    "# LOSO test subject\n",
    "TEST_SUBJECT = \"Daniel\"\n",
    "print(f\"[INFO] Selected test subject (LOSO): {TEST_SUBJECT}\")\n",
    "\n",
    "# Train subjects = remaining\n",
    "train_subjects = [s for s in all_folders if s.lower() != TEST_SUBJECT.lower()]\n",
    "print(f\"[INFO] Training subjects ({len(train_subjects)}):\\n{train_subjects}\")\n",
    "\n",
    "# Sampling rate\n",
    "SFREQ = 128.0\n",
    "print(f\"[INFO] Sampling rate (SFREQ) = {SFREQ} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e649681-2861-4ce2-bbd1-0d9dd3359f42",
   "metadata": {},
   "source": [
    "Cell 2 — Load EEG + labels per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943c5c6e-a7aa-4618-899c-daf341c25358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 2] Loading data from folders...\n",
      "[STEP 2]       Amin | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Amin | EEG=          Amin.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Amin1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]      Amin1 | EEG=         Amin1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Cole | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Cole | EEG=          Cole.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Daniel | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]     Daniel | EEG=        Daniel.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]    Ismayil | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]    Ismayil | EEG=       Ismayil.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Jack | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Jack | EEG=          Jack.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]      James | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      James | EEG=         James.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Josh | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Josh | EEG=          Josh.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Marjan | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Marjan | EEG=        Marjan.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]        Max | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Max | EEG=           Max.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Mina | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mina | EEG=          Mina.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 1 | EEG=        Mina 1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 3 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 3 | EEG=        Mina 3.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]   Mohammad | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]   Mohammad | EEG=      Mohammad.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Mona | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mona | EEG=          Mona.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Roddy | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      Roddy | EEG=         Roddy.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]        Sam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Sam | EEG=           Sam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       adam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       adam | EEG=          adam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2] Loading complete ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 2: Load data (EMPTY LABEL => CLASS 0)\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path  # helper\n",
    "\n",
    "def load_subject(subj_dir: str, subj_name: str, expected_channels: int = 14):\n",
    "    \"\"\"\n",
    "    EEG file: {subj_name}.npy\n",
    "    Label file: {subj_name}_target.npy\n",
    "    Label rule:\n",
    "      - empty array -> class 0\n",
    "      - [0] -> class 0\n",
    "      - [1] -> class 1\n",
    "    \"\"\"\n",
    "\n",
    "    # EEG file path\n",
    "    eeg_file = os.path.join(subj_dir, f\"{subj_name}.npy\")\n",
    "    assert os.path.exists(eeg_file), f\"[ERROR] Missing EEG file: {eeg_file}\"\n",
    "\n",
    "    # Label file path\n",
    "    target_file = os.path.join(subj_dir, f\"{subj_name}_target.npy\")\n",
    "    assert os.path.exists(target_file), f\"[ERROR] Missing label file: {target_file}\"\n",
    "\n",
    "    # Load EEG\n",
    "    X_raw = np.load(eeg_file, allow_pickle=True)\n",
    "    X_raw = np.asarray(X_raw)\n",
    "\n",
    "    # Standardize EEG to (n_trials, 14, T)\n",
    "    if X_raw.ndim == 2:\n",
    "        X_trials = X_raw[None, :, :]\n",
    "    elif X_raw.ndim == 3:\n",
    "        dims = list(X_raw.shape)\n",
    "        chan_axes = [i for i, d in enumerate(dims) if d == expected_channels]\n",
    "        chan_axis = chan_axes[0] if len(chan_axes) == 1 else 1\n",
    "        time_axis = int(np.argmax(dims))\n",
    "        trial_axis = [i for i in range(3) if i not in (chan_axis, time_axis)][0]\n",
    "        X_trials = np.moveaxis(X_raw, (trial_axis, chan_axis, time_axis), (0, 1, 2))\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] Unsupported EEG shape {X_raw.shape} in {eeg_file}\")\n",
    "\n",
    "    assert X_trials.shape[1] == expected_channels, f\"[ERROR] Expected {expected_channels} channels, got {X_trials.shape[1]} in {eeg_file}\"\n",
    "\n",
    "    # Load label\n",
    "    y_raw = np.load(target_file, allow_pickle=True)\n",
    "    y_arr = np.asarray(y_raw).ravel()\n",
    "\n",
    "    # Apply your rule\n",
    "    if y_arr.size == 0:\n",
    "        label = 0\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\")\n",
    "    else:\n",
    "        label = int(y_arr[0])\n",
    "        if label not in (0, 1):\n",
    "            raise ValueError(f\"[ERROR] Invalid label {label} in {target_file} (must be 0 or 1)\")\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape={y_arr.shape}, value={y_arr} -> class={label}\")\n",
    "\n",
    "    # Repeat label for all trials\n",
    "    y_trials = np.full((X_trials.shape[0],), label, dtype=np.int32)\n",
    "\n",
    "    return X_trials.astype(np.float32), y_trials.astype(np.int32), eeg_file, target_file\n",
    "\n",
    "\n",
    "# Load all subjects\n",
    "subject_data = {}\n",
    "print(\"[STEP 2] Loading data from folders...\")\n",
    "\n",
    "for subj in all_folders:\n",
    "    subj_dir = os.path.join(BASE_PATH, subj)\n",
    "    X_trials, y_trials, eeg_file, target_file = load_subject(subj_dir, subj)\n",
    "\n",
    "    subject_data[subj] = (X_trials, y_trials)\n",
    "\n",
    "    print(f\"[STEP 2] {subj:>10} | EEG={Path(eeg_file).name:>18} | X={X_trials.shape} | y={y_trials.shape} | class={y_trials[0]}\")\n",
    "\n",
    "print(\"[STEP 2] Loading complete ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd93ca8-a94c-485c-8a61-e6e7441f6299",
   "metadata": {},
   "source": [
    "Cell 3 — Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733e592c-98e1-4cbd-afd2-6fdc8cf407f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 3] Preprocessing functions ready ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3: Preprocessing functions\n",
    "# =========================\n",
    "\n",
    "import mne  # EEG processing\n",
    "import pywt  # wavelets\n",
    "from sklearn.decomposition import FastICA  # ICA\n",
    "from typing import Optional, Tuple, Union, Sequence, Dict, List  # typing\n",
    "\n",
    "def wavelet_enhanced_ica(\n",
    "    data: np.ndarray,\n",
    "    n_components: int = 10,\n",
    "    wavelet: str = \"db4\",\n",
    "    level: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> np.ndarray:\n",
    "    n_ch, n_t = data.shape\n",
    "    n_components = min(n_components, n_ch)\n",
    "\n",
    "    coeffs = pywt.wavedec(data, wavelet=wavelet, level=level, axis=1)\n",
    "    A = coeffs[0]\n",
    "\n",
    "    ica = FastICA(n_components=n_components, random_state=random_state)\n",
    "    S = ica.fit_transform(A.T).T\n",
    "    A_denoised = ica.inverse_transform(S.T).T\n",
    "\n",
    "    coeffs[0] = A_denoised\n",
    "    cleaned = pywt.waverec(coeffs, wavelet=wavelet, axis=1)\n",
    "\n",
    "    if cleaned.shape[1] != n_t:\n",
    "        if cleaned.shape[1] > n_t:\n",
    "            cleaned = cleaned[:, :n_t]\n",
    "        else:\n",
    "            cleaned = np.pad(cleaned, ((0, 0), (0, n_t - cleaned.shape[1])), mode=\"constant\")\n",
    "    return cleaned\n",
    "\n",
    "def _names_from_index_mapping(n_channels: int, index_to_name: Optional[Dict[int, str]]) -> List[str]:\n",
    "    if index_to_name is None:\n",
    "        return [f\"EEG{i+1}\" for i in range(n_channels)]\n",
    "    keys = list(index_to_name.keys())\n",
    "    is_zero_based = (0 in keys) and (1 not in keys)\n",
    "    names = []\n",
    "    for i in range(n_channels):\n",
    "        key = i if is_zero_based else (i + 1)\n",
    "        names.append(index_to_name.get(key, f\"EEG{i+1}\"))\n",
    "    return names\n",
    "\n",
    "def preprocess_eeg(\n",
    "    eeg: np.ndarray,\n",
    "    sfreq: float,\n",
    "    *,\n",
    "    index_to_name: Optional[Dict[int, str]] = None,\n",
    "    use_standard_1010: bool = True,\n",
    "    resample_to: Optional[float] = None,\n",
    "    notch_freqs: Union[None, float, Sequence[float]] = 50.0,\n",
    "    highpass: Optional[float] = 0.05,\n",
    "    bad_point_z: float = 6.0,\n",
    "    bad_channel_z: float = 5.0,\n",
    "    interpolate_bad_channels: bool = True,\n",
    "    car: bool = True,\n",
    "    use_wica: bool = True,\n",
    "    wica_components: int = 10,\n",
    "    wica_wavelet: str = \"db4\",\n",
    "    wica_level: int = 3,\n",
    "    wica_random_state: int = 42,\n",
    "    return_raw: bool = False,\n",
    "):\n",
    "\n",
    "    eeg = np.asarray(eeg, dtype=np.float32)\n",
    "    assert eeg.ndim == 2, \"eeg must be 2D: (n_channels, n_times)\"\n",
    "\n",
    "    n_channels, _ = eeg.shape\n",
    "    ch_names = _names_from_index_mapping(n_channels, index_to_name)\n",
    "    ch_types = ['eog' if str(n).upper().startswith(\"EOG\") else 'eeg' for n in ch_names]\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    raw = mne.io.RawArray(eeg, info, verbose=False)\n",
    "\n",
    "    montage_applied = False\n",
    "    if use_standard_1010:\n",
    "        try:\n",
    "            mont = mne.channels.make_standard_montage(\"standard_1010\")\n",
    "            raw.set_montage(mont, match_case=False, on_missing=\"ignore\")\n",
    "            montage_applied = True\n",
    "        except Exception:\n",
    "            montage_applied = False\n",
    "\n",
    "    if resample_to is not None and float(resample_to) != float(sfreq):\n",
    "        raw.resample(sfreq=resample_to, npad=\"auto\")\n",
    "    sfreq_out = float(raw.info[\"sfreq\"])\n",
    "\n",
    "    if notch_freqs is not None:\n",
    "        raw.notch_filter(freqs=notch_freqs, verbose=False)\n",
    "    if highpass is not None:\n",
    "        raw.filter(l_freq=highpass, h_freq=None, verbose=False)\n",
    "\n",
    "    X = raw.get_data()\n",
    "    mu = np.mean(X, axis=1, keepdims=True)\n",
    "    sd = np.std(X, axis=1, keepdims=True) + 1e-12\n",
    "    hi = mu + bad_point_z * sd\n",
    "    lo = mu - bad_point_z * sd\n",
    "    bad_idx = (X > hi) | (X < lo)\n",
    "\n",
    "    if np.any(bad_idx):\n",
    "        X_fixed = X.copy()\n",
    "        t = np.arange(X.shape[1], dtype=float)\n",
    "        for ch in range(n_channels):\n",
    "            mask = bad_idx[ch]\n",
    "            if mask.any():\n",
    "                good = ~mask\n",
    "                if good.sum() >= 2:\n",
    "                    X_fixed[ch, mask] = np.interp(t[mask], t[good], X_fixed[ch, good])\n",
    "        raw._data = X_fixed\n",
    "\n",
    "    if interpolate_bad_channels and montage_applied:\n",
    "        X = raw.get_data(picks=\"eeg\")\n",
    "        if X.size > 0:\n",
    "            ch_std = X.std(axis=1)\n",
    "            med = np.median(ch_std)\n",
    "            mad = np.median(np.abs(ch_std - med)) + 1e-12\n",
    "            z = 0.6745 * (ch_std - med) / mad\n",
    "            eeg_names = mne.pick_info(raw.info, mne.pick_types(raw.info, eeg=True)).ch_names\n",
    "            bads = [eeg_names[i] for i in np.where(np.abs(z) > bad_channel_z)[0]]\n",
    "            raw.info[\"bads\"] = bads\n",
    "            if bads:\n",
    "                raw.interpolate_bads(reset_bads=True, verbose=False)\n",
    "\n",
    "    if car:\n",
    "        raw.set_eeg_reference(\"average\", projection=True)\n",
    "        raw.apply_proj()\n",
    "\n",
    "    if use_wica:\n",
    "        cleaned = wavelet_enhanced_ica(\n",
    "            raw.get_data(),\n",
    "            n_components=wica_components,\n",
    "            wavelet=wica_wavelet,\n",
    "            level=wica_level,\n",
    "            random_state=wica_random_state\n",
    "        )\n",
    "        raw = mne.io.RawArray(cleaned, raw.info, verbose=False)\n",
    "\n",
    "    cleaned = raw.get_data()\n",
    "    return cleaned, sfreq_out\n",
    "\n",
    "print(\"[STEP 3] Preprocessing functions ready ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a9d20-889d-45c4-bbc2-de62216908cd",
   "metadata": {},
   "source": [
    "Cell 4 — Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bfcbde0-1cc2-409c-8627-93da46606661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 4] Starting preprocessing...\n",
      "[INFO] USE_WICA = True\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 01/17: Amin | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 02/17: Amin1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 03/17: Cole | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 04/17: Ismayil | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 05/17: Jack | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 06/17: James | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 07/17: Josh | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 08/17: Marjan | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 09/17: Max | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 10/17: Mina | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 11/17: Mina 1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 12/17: Mina 3 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 13/17: Mohammad | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 14/17: Mona | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 15/17: Roddy | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 16/17: Sam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 17/17: adam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Test subject: Daniel | trials=1 | fs_out=128.0\n",
      "[STEP 4] Preprocessing done ✅\n",
      "  X_train_trials_all: (17, 14, 38400)\n",
      "  y_train_trials_all: (17,) (array([0, 1], dtype=int32), array([9, 8]))\n",
      "  X_test_trials_all : (1, 14, 38400)\n",
      "  y_test_trials_all : (1,) (array([1], dtype=int32), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 4: Apply preprocessing (LOSO)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 4] Starting preprocessing...\")\n",
    "\n",
    "USE_WICA = True\n",
    "print(f\"[INFO] USE_WICA = {USE_WICA}\")\n",
    "\n",
    "X_train_trials_all, y_train_trials_all = [], []\n",
    "X_test_trials_all, y_test_trials_all = [], []\n",
    "\n",
    "def preprocess_trials(X_trials, y_trials):\n",
    "    cleaned_list = []\n",
    "    label_list = []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        trial = X_trials[i]\n",
    "        label = int(y_trials[i])\n",
    "\n",
    "        cleaned, fs_out = preprocess_eeg(\n",
    "            eeg=trial,\n",
    "            sfreq=SFREQ,\n",
    "            use_wica=USE_WICA\n",
    "        )\n",
    "\n",
    "        cleaned_list.append(cleaned.astype(np.float32, copy=False))\n",
    "        label_list.append(label)\n",
    "\n",
    "    return cleaned_list, np.array(label_list, dtype=np.int32), fs_out\n",
    "\n",
    "# Train subjects\n",
    "for idx, subj in enumerate(train_subjects, start=1):\n",
    "    X_trials, y_trials = subject_data[subj]\n",
    "    cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "    X_train_trials_all.extend(cleaned_trials)\n",
    "    y_train_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "    print(f\"[STEP 4] Train {idx:02d}/{len(train_subjects)}: {subj} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Test subject\n",
    "X_trials, y_trials = subject_data[TEST_SUBJECT]\n",
    "cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "X_test_trials_all.extend(cleaned_trials)\n",
    "y_test_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "print(f\"[STEP 4] Test subject: {TEST_SUBJECT} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Convert to arrays\n",
    "X_train_trials_all = np.array(X_train_trials_all, dtype=np.float32)\n",
    "y_train_trials_all = np.array(y_train_trials_all, dtype=np.int32)\n",
    "\n",
    "X_test_trials_all = np.array(X_test_trials_all, dtype=np.float32)\n",
    "y_test_trials_all = np.array(y_test_trials_all, dtype=np.int32)\n",
    "\n",
    "print(\"[STEP 4] Preprocessing done ✅\")\n",
    "print(\"  X_train_trials_all:\", X_train_trials_all.shape)\n",
    "print(\"  y_train_trials_all:\", y_train_trials_all.shape, np.unique(y_train_trials_all, return_counts=True))\n",
    "print(\"  X_test_trials_all :\", X_test_trials_all.shape)\n",
    "print(\"  y_test_trials_all :\", y_test_trials_all.shape, np.unique(y_test_trials_all, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc3da1-2967-4ee3-84cd-bfcf0f98c0b1",
   "metadata": {},
   "source": [
    "Cell 5 — Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c21ad54-7001-4039-9360-29dfa2905127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 5] Starting segmentation...\n",
      "[STEP 5] Segmented 5/17 trials...\n",
      "[STEP 5] Segmented 10/17 trials...\n",
      "[STEP 5] Segmented 15/17 trials...\n",
      "[STEP 5] Segmented 17/17 trials...\n",
      "[STEP 5] Segmented 1/1 trials...\n",
      "[STEP 5] Segmentation done ✅\n",
      "  X_train: (5100, 14, 128, 1)\n",
      "  y_train: (5100,) (array([0, 1], dtype=int32), array([2700, 2400]))\n",
      "  X_test : (300, 14, 128, 1)\n",
      "  y_test : (300,) (array([1], dtype=int32), array([300]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 5: Segmentation\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 5] Starting segmentation...\")\n",
    "\n",
    "def segment_trial(trial_2d: np.ndarray, label: int, segment_len: int = 128):\n",
    "    C, T = trial_2d.shape\n",
    "    n_segs = T // segment_len\n",
    "    use_T = n_segs * segment_len\n",
    "\n",
    "    if n_segs == 0:\n",
    "        return np.empty((0, C, segment_len), dtype=np.float32), np.empty((0,), dtype=np.int32)\n",
    "\n",
    "    trial_trim = trial_2d[:, :use_T]\n",
    "    Xsegs = trial_trim.reshape(C, n_segs, segment_len).transpose(1, 0, 2)\n",
    "    ysegs = np.full((n_segs,), int(label), dtype=np.int32)\n",
    "    return Xsegs.astype(np.float32, copy=False), ysegs\n",
    "\n",
    "def segment_dataset(X_trials: np.ndarray, y_trials: np.ndarray, segment_len: int = 128):\n",
    "    X_all, y_all = [], []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        Xsegs, ysegs = segment_trial(X_trials[i], int(y_trials[i]), segment_len=segment_len)\n",
    "        X_all.append(Xsegs)\n",
    "        y_all.append(ysegs)\n",
    "\n",
    "        if (i + 1) % 5 == 0 or (i + 1) == X_trials.shape[0]:\n",
    "            print(f\"[STEP 5] Segmented {i+1}/{X_trials.shape[0]} trials...\")\n",
    "\n",
    "    X_out = np.concatenate(X_all, axis=0)\n",
    "    y_out = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    X_out = np.expand_dims(X_out, axis=-1)  # (N,14,128,1)\n",
    "    return X_out.astype(np.float32), y_out.astype(np.int32)\n",
    "\n",
    "X_train, y_train = segment_dataset(X_train_trials_all, y_train_trials_all, segment_len=128)\n",
    "X_test,  y_test  = segment_dataset(X_test_trials_all,  y_test_trials_all,  segment_len=128)\n",
    "\n",
    "print(\"[STEP 5] Segmentation done ✅\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  y_train:\", y_train.shape, np.unique(y_train, return_counts=True))\n",
    "print(\"  X_test :\", X_test.shape)\n",
    "print(\"  y_test :\", y_test.shape, np.unique(y_test, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b4ffd5-bf2f-490e-8854-0a2fc747f1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e69ba7-3952-47b1-bd80-f05237b2e018",
   "metadata": {},
   "source": [
    "Cell 6 — Split train/val + Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97fda4ae-e1c8-4fd1-bf0c-c82ae2a85764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 6] Normalizing using ALL training data (no train/val split)...\n",
      "[STEP 6] Normalization done ✅\n",
      "  X_train_norm: (5100, 14, 128, 1)\n",
      "  X_test_norm : (300, 14, 128, 1)\n",
      "  y_train_norm: (5100,)\n",
      "  y_test_norm : (300,)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 6: Normalize (ALL training data)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 6] Normalizing using ALL training data (no train/val split)...\")\n",
    "\n",
    "def normalize_eeg(train_data: np.ndarray, test_data: np.ndarray):\n",
    "    if train_data.ndim == 5:\n",
    "        train_flat = train_data.reshape(-1, *train_data.shape[2:])\n",
    "        test_flat  = test_data.reshape(-1, *test_data.shape[2:])\n",
    "    else:\n",
    "        train_flat, test_flat = train_data, test_data\n",
    "\n",
    "    # Compute stats on ALL training data\n",
    "    train_mean = np.mean(train_flat, axis=(0, 2, 3), keepdims=True)\n",
    "    train_std  = np.std(train_flat,  axis=(0, 2, 3), keepdims=True) + 1e-8\n",
    "\n",
    "    train_norm = (train_flat - train_mean) / train_std\n",
    "    test_norm  = (test_flat  - train_mean) / train_std\n",
    "\n",
    "    if train_data.ndim == 5:\n",
    "        train_norm = train_norm.reshape(train_data.shape)\n",
    "        test_norm  = test_norm.reshape(test_data.shape)\n",
    "\n",
    "    return (\n",
    "        train_norm.astype(np.float32),\n",
    "        test_norm.astype(np.float32),\n",
    "        train_mean,\n",
    "        train_std\n",
    "    )\n",
    "\n",
    "# Normalize ALL training data\n",
    "X_train_norm, X_test_norm, mean, std = normalize_eeg(X_train, X_test)\n",
    "\n",
    "y_train_norm = y_train.astype(np.float32)\n",
    "y_test_norm  = y_test.astype(np.float32)\n",
    "\n",
    "print(\"[STEP 6] Normalization done ✅\")\n",
    "print(\"  X_train_norm:\", X_train_norm.shape)\n",
    "print(\"  X_test_norm :\", X_test_norm.shape)\n",
    "print(\"  y_train_norm:\", y_train_norm.shape)\n",
    "print(\"  y_test_norm :\", y_test_norm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f527c-34db-4c23-bdbd-c8ce03b519f0",
   "metadata": {},
   "source": [
    "Cell 7 — EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ea4279-53c2-49c1-b834-7b1e1c20775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\DL\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Xtr: (5100, 14, 128, 1), ytr: (5100,)\n",
      "[INFO] Test / LOSO subject is NOT used here.\n",
      "[SPLIT] Train: (4080, 14, 128, 1), Val: (1020, 14, 128, 1)\n",
      "Epoch 1/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 128ms/step - accuracy: 0.5027 - loss: 0.7411 - val_accuracy: 0.5304 - val_loss: 0.6912 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.5542 - loss: 0.6839 - val_accuracy: 0.5294 - val_loss: 0.6911 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.5988 - loss: 0.6631 - val_accuracy: 0.5324 - val_loss: 0.6908 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.6289 - loss: 0.6322 - val_accuracy: 0.5324 - val_loss: 0.6896 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.6488 - loss: 0.6070 - val_accuracy: 0.5422 - val_loss: 0.6852 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.6816 - loss: 0.5813 - val_accuracy: 0.5510 - val_loss: 0.6748 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.7047 - loss: 0.5555 - val_accuracy: 0.5725 - val_loss: 0.6620 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.7213 - loss: 0.5324 - val_accuracy: 0.5882 - val_loss: 0.6392 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.7353 - loss: 0.5136 - val_accuracy: 0.6157 - val_loss: 0.6271 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.7461 - loss: 0.5024 - val_accuracy: 0.6755 - val_loss: 0.5947 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.7586 - loss: 0.4820 - val_accuracy: 0.7225 - val_loss: 0.5651 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.7701 - loss: 0.4721 - val_accuracy: 0.7118 - val_loss: 0.5627 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.7816 - loss: 0.4488 - val_accuracy: 0.7255 - val_loss: 0.5456 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.7865 - loss: 0.4452 - val_accuracy: 0.7549 - val_loss: 0.5247 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.7914 - loss: 0.4316 - val_accuracy: 0.7431 - val_loss: 0.4964 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.8235 - loss: 0.4110 - val_accuracy: 0.7422 - val_loss: 0.4806 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8218 - loss: 0.3959 - val_accuracy: 0.7461 - val_loss: 0.4724 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8350 - loss: 0.3741 - val_accuracy: 0.7392 - val_loss: 0.4717 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8377 - loss: 0.3757 - val_accuracy: 0.7422 - val_loss: 0.4537 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.8373 - loss: 0.3619 - val_accuracy: 0.7735 - val_loss: 0.4299 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8463 - loss: 0.3573 - val_accuracy: 0.7863 - val_loss: 0.4229 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8471 - loss: 0.3555 - val_accuracy: 0.8108 - val_loss: 0.3926 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8657 - loss: 0.3268 - val_accuracy: 0.8088 - val_loss: 0.3923 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.8583 - loss: 0.3327 - val_accuracy: 0.8500 - val_loss: 0.3635 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.8699 - loss: 0.3178 - val_accuracy: 0.8304 - val_loss: 0.3699 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8652 - loss: 0.3240 - val_accuracy: 0.8588 - val_loss: 0.3196 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8777 - loss: 0.2993 - val_accuracy: 0.8794 - val_loss: 0.3062 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8772 - loss: 0.2938 - val_accuracy: 0.8863 - val_loss: 0.2955 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.8806 - loss: 0.2856 - val_accuracy: 0.8971 - val_loss: 0.2833 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.8892 - loss: 0.2791 - val_accuracy: 0.8539 - val_loss: 0.3009 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.8900 - loss: 0.2717 - val_accuracy: 0.9039 - val_loss: 0.2676 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.8890 - loss: 0.2735 - val_accuracy: 0.9049 - val_loss: 0.2519 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.8949 - loss: 0.2553 - val_accuracy: 0.9304 - val_loss: 0.2077 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8988 - loss: 0.2406 - val_accuracy: 0.9294 - val_loss: 0.2081 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8949 - loss: 0.2629 - val_accuracy: 0.9412 - val_loss: 0.1835 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9037 - loss: 0.2360 - val_accuracy: 0.9363 - val_loss: 0.1876 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9083 - loss: 0.2277 - val_accuracy: 0.9559 - val_loss: 0.1720 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9034 - loss: 0.2331 - val_accuracy: 0.9392 - val_loss: 0.1882 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9186 - loss: 0.2116 - val_accuracy: 0.9461 - val_loss: 0.1710 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9127 - loss: 0.2251 - val_accuracy: 0.9422 - val_loss: 0.1797 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9081 - loss: 0.2324 - val_accuracy: 0.8990 - val_loss: 0.2800 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9169 - loss: 0.2094 - val_accuracy: 0.9441 - val_loss: 0.1635 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9086 - loss: 0.2279 - val_accuracy: 0.9265 - val_loss: 0.2117 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9078 - loss: 0.2274 - val_accuracy: 0.9196 - val_loss: 0.2208 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9201 - loss: 0.2019 - val_accuracy: 0.9265 - val_loss: 0.1907 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.9157 - loss: 0.2071\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9078 - loss: 0.2332 - val_accuracy: 0.9147 - val_loss: 0.2010 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9157 - loss: 0.2157 - val_accuracy: 0.9559 - val_loss: 0.1369 - learning_rate: 5.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9270 - loss: 0.1957 - val_accuracy: 0.9480 - val_loss: 0.1465 - learning_rate: 5.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9326 - loss: 0.1800 - val_accuracy: 0.9529 - val_loss: 0.1423 - learning_rate: 5.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9211 - loss: 0.2028 - val_accuracy: 0.9529 - val_loss: 0.1305 - learning_rate: 5.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9331 - loss: 0.1782 - val_accuracy: 0.9637 - val_loss: 0.1196 - learning_rate: 5.0000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9324 - loss: 0.1778 - val_accuracy: 0.9510 - val_loss: 0.1382 - learning_rate: 5.0000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9314 - loss: 0.1790 - val_accuracy: 0.9618 - val_loss: 0.1199 - learning_rate: 5.0000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9360 - loss: 0.1665 - val_accuracy: 0.9520 - val_loss: 0.1367 - learning_rate: 5.0000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9289 - loss: 0.1775\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9324 - loss: 0.1781 - val_accuracy: 0.9549 - val_loss: 0.1264 - learning_rate: 5.0000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9382 - loss: 0.1670 - val_accuracy: 0.9608 - val_loss: 0.1266 - learning_rate: 2.5000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9429 - loss: 0.1686 - val_accuracy: 0.9549 - val_loss: 0.1163 - learning_rate: 2.5000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9355 - loss: 0.1662 - val_accuracy: 0.9549 - val_loss: 0.1167 - learning_rate: 2.5000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9422 - loss: 0.1679 - val_accuracy: 0.9520 - val_loss: 0.1311 - learning_rate: 2.5000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9419 - loss: 0.1631 - val_accuracy: 0.9657 - val_loss: 0.1087 - learning_rate: 2.5000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9397 - loss: 0.1629 - val_accuracy: 0.9529 - val_loss: 0.1251 - learning_rate: 2.5000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9404 - loss: 0.1612 - val_accuracy: 0.9647 - val_loss: 0.1113 - learning_rate: 2.5000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9400 - loss: 0.1601 - val_accuracy: 0.9657 - val_loss: 0.1101 - learning_rate: 2.5000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9341 - loss: 0.1670 - val_accuracy: 0.9647 - val_loss: 0.0980 - learning_rate: 2.5000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9404 - loss: 0.1565 - val_accuracy: 0.9667 - val_loss: 0.1119 - learning_rate: 2.5000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9429 - loss: 0.1573 - val_accuracy: 0.9618 - val_loss: 0.1060 - learning_rate: 2.5000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9395 - loss: 0.1495 - val_accuracy: 0.9657 - val_loss: 0.1025 - learning_rate: 2.5000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9443 - loss: 0.1448\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9358 - loss: 0.1628 - val_accuracy: 0.9637 - val_loss: 0.1088 - learning_rate: 2.5000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9431 - loss: 0.1589 - val_accuracy: 0.9608 - val_loss: 0.1152 - learning_rate: 1.2500e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 127ms/step - accuracy: 0.9434 - loss: 0.1521 - val_accuracy: 0.9686 - val_loss: 0.1031 - learning_rate: 1.2500e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.9397 - loss: 0.1636 - val_accuracy: 0.9725 - val_loss: 0.0981 - learning_rate: 1.2500e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9436 - loss: 0.1481 - val_accuracy: 0.9735 - val_loss: 0.0917 - learning_rate: 1.2500e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9424 - loss: 0.1589 - val_accuracy: 0.9706 - val_loss: 0.0993 - learning_rate: 1.2500e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9402 - loss: 0.1638 - val_accuracy: 0.9696 - val_loss: 0.0960 - learning_rate: 1.2500e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9439 - loss: 0.1587 - val_accuracy: 0.9686 - val_loss: 0.0942 - learning_rate: 1.2500e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9375 - loss: 0.1524 - val_accuracy: 0.9745 - val_loss: 0.0897 - learning_rate: 1.2500e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9426 - loss: 0.1462 - val_accuracy: 0.9667 - val_loss: 0.0981 - learning_rate: 1.2500e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9395 - loss: 0.1549 - val_accuracy: 0.9667 - val_loss: 0.1000 - learning_rate: 1.2500e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9480 - loss: 0.1437 - val_accuracy: 0.9647 - val_loss: 0.1009 - learning_rate: 1.2500e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9426 - loss: 0.1451 - val_accuracy: 0.9765 - val_loss: 0.0853 - learning_rate: 1.2500e-04\n",
      "Epoch 81/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9468 - loss: 0.1470 - val_accuracy: 0.9725 - val_loss: 0.0858 - learning_rate: 1.2500e-04\n",
      "Epoch 82/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9431 - loss: 0.1514 - val_accuracy: 0.9716 - val_loss: 0.0898 - learning_rate: 1.2500e-04\n",
      "Epoch 83/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9412 - loss: 0.1576 - val_accuracy: 0.9716 - val_loss: 0.0899 - learning_rate: 1.2500e-04\n",
      "Epoch 84/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9420 - loss: 0.1535\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9407 - loss: 0.1599 - val_accuracy: 0.9716 - val_loss: 0.0965 - learning_rate: 1.2500e-04\n",
      "Epoch 85/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9397 - loss: 0.1577 - val_accuracy: 0.9716 - val_loss: 0.0927 - learning_rate: 6.2500e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9431 - loss: 0.1571 - val_accuracy: 0.9667 - val_loss: 0.0994 - learning_rate: 6.2500e-05\n",
      "Epoch 87/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9483 - loss: 0.1361 - val_accuracy: 0.9725 - val_loss: 0.0928 - learning_rate: 6.2500e-05\n",
      "Epoch 88/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9432 - loss: 0.1390\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9453 - loss: 0.1406 - val_accuracy: 0.9716 - val_loss: 0.0924 - learning_rate: 6.2500e-05\n",
      "Epoch 89/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9485 - loss: 0.1441 - val_accuracy: 0.9745 - val_loss: 0.0872 - learning_rate: 3.1250e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9446 - loss: 0.1448 - val_accuracy: 0.9725 - val_loss: 0.0915 - learning_rate: 3.1250e-05\n",
      "Epoch 91/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9463 - loss: 0.1402 - val_accuracy: 0.9765 - val_loss: 0.0871 - learning_rate: 3.1250e-05\n",
      "Epoch 92/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9514 - loss: 0.1242\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9456 - loss: 0.1398 - val_accuracy: 0.9735 - val_loss: 0.0891 - learning_rate: 3.1250e-05\n",
      "Epoch 93/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9444 - loss: 0.1465 - val_accuracy: 0.9755 - val_loss: 0.0861 - learning_rate: 1.5625e-05\n",
      "Epoch 94/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9385 - loss: 0.1463 - val_accuracy: 0.9745 - val_loss: 0.0865 - learning_rate: 1.5625e-05\n",
      "Epoch 95/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9483 - loss: 0.1493 - val_accuracy: 0.9745 - val_loss: 0.0880 - learning_rate: 1.5625e-05\n",
      "Epoch 95: early stopping\n",
      "Restoring model weights from the end of the best epoch: 80.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] Model saved as cole-final.h5\n",
      "\n",
      "[CONFUSION MATRIX]\n",
      "[[534   6]\n",
      " [ 18 462]]\n",
      "[VAL] acc=0.9765, prec=0.9872, rec=0.9625, f1=0.9747\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 4: 80:20 Train / Validation Split (EEGNet)\n",
    "# ====================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, SeparableConv2D, BatchNormalization, Activation,\n",
    "    DepthwiseConv2D, Conv2D, AveragePooling2D,\n",
    "    Dropout, Dense, Flatten\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# --------------------------------------------------------------------\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# EEGNet model\n",
    "# --------------------------------------------------------------------\n",
    "def create_eegnet_model(input_shape, dropout_rate=0.5, num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(16, (1, 64), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = DepthwiseConv2D(\n",
    "        (input_shape[0], 1),\n",
    "        depth_multiplier=2,\n",
    "        padding='valid',\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 4))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = SeparableConv2D(16, (1, 16), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 8))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Compile model\n",
    "# --------------------------------------------------------------------\n",
    "def compile_model():\n",
    "    input_shape = (Xtr.shape[1], Xtr.shape[2], Xtr.shape[3])\n",
    "    model = create_eegnet_model(input_shape)\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-3),\n",
    "        loss=BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[BinaryAccuracy(name=\"accuracy\", threshold=0.5)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def make_callbacks():\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\",\n",
    "            factor=0.5,\n",
    "            patience=4,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Use ALL normalized training data\n",
    "# --------------------------------------------------------------------\n",
    "Xtr = X_train_norm\n",
    "ytr = y_train_norm\n",
    "\n",
    "print(f\"[TRAIN] Xtr: {Xtr.shape}, ytr: {ytr.shape}\")\n",
    "print(\"[INFO] Test / LOSO subject is NOT used here.\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 80:20 Stratified Split\n",
    "# --------------------------------------------------------------------\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(sss.split(Xtr, ytr))\n",
    "\n",
    "X_train, X_val = Xtr[train_idx], Xtr[val_idx]\n",
    "y_train, y_val = ytr[train_idx], ytr[val_idx]\n",
    "\n",
    "print(f\"[SPLIT] Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Train model\n",
    "# --------------------------------------------------------------------\n",
    "model = compile_model()\n",
    "callbacks = make_callbacks()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Save final model\n",
    "# --------------------------------------------------------------------\n",
    "model.save(\"daniel-final.h5\")\n",
    "print(\"[SAVE] Model saved as cole-final.h5\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Validation Confusion Matrix\n",
    "# --------------------------------------------------------------------\n",
    "y_prob = model.predict(X_val, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "y_true = y_val.astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "eps = 1e-12\n",
    "\n",
    "acc  = (tp + tn) / max(tp + tn + fp + fn, eps)\n",
    "prec = tp / max(tp + fp, eps)\n",
    "rec  = tp / max(tp + fn, eps)\n",
    "f1   = 2 * prec * rec / max(prec + rec, eps)\n",
    "\n",
    "print(\"\\n[CONFUSION MATRIX]\")\n",
    "print(cm)\n",
    "print(f\"[VAL] acc={acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e2a13-6367-41ca-be66-a43594c2940e",
   "metadata": {},
   "source": [
    "Cell 8 — Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ac6662-33c1-4e25-b6c5-d47c860fba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Preparing TEST data...\n",
      "[TEST] Xte shape: (300, 14, 128, 1)\n",
      "[TEST] yte shape: (300,), class distribution: (array([1]), array([300]))\n",
      "[TEST] Loading final model: daniel-final.h5\n",
      "[TEST] Running inference on TEST subject...\n",
      "[TEST] Mean predicted probability: 0.7693\n",
      "\n",
      "[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[  0   0]\n",
      " [ 46 254]]\n",
      "Accuracy : 0.8467\n",
      "Precision: 1.0000\n",
      "Recall   : 0.8467\n",
      "F1-score : 0.9170\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Inference on TEST subject (adam) using FINAL EEGNet\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Prepare TEST data\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Preparing TEST data...\")\n",
    "\n",
    "Xte = X_test_norm\n",
    "yte = y_test_norm.astype(int)\n",
    "\n",
    "print(f\"[TEST] Xte shape: {Xte.shape}\")\n",
    "print(f\"[TEST] yte shape: {yte.shape}, class distribution: {np.unique(yte, return_counts=True)}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Load FINAL trained model\n",
    "# ------------------------------------------------------------------\n",
    "model_path = \"daniel-final.h5\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        \"Final model 'cole-final.h5' not found. \"\n",
    "        \"Please run Cell 4 (80:20 training) first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[TEST] Loading final model: {model_path}\")\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Run inference on TEST subject\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Running inference on TEST subject...\")\n",
    "\n",
    "y_prob = model.predict(Xte, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(f\"[TEST] Mean predicted probability: {y_prob.mean():.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Metrics on TEST subject (segment-level)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc = accuracy_score(yte, y_pred)\n",
    "prec = precision_score(yte, y_pred, zero_division=0)\n",
    "rec = recall_score(yte, y_pred, zero_division=0)\n",
    "f1 = f1_score(yte, y_pred, zero_division=0)\n",
    "\n",
    "print(\"\\n[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572dbf3e-bbb5-4865-883f-30e374700949",
   "metadata": {},
   "source": [
    "Cell 9 - Performance Matrices for each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a57240-50ea-4736-a7ac-52ef7878b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] Verifying predictions and labels are available...\n",
      "[CHECK] Found predictions for 300 test segments.\n",
      "\n",
      "[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[  0   0]\n",
      " [ 46 254]]\n",
      "Accuracy : 0.8467\n",
      "Precision: 1.0000\n",
      "Recall   : 0.8467\n",
      "F1-score : 0.9170\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Final model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================\n",
    "# Cell 6: Confusion Matrix & Metrics on TEST (FINAL EEGNet)\n",
    "# =======================================================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# ------------------------------------------------------------------\n",
    "def compute_metrics_from_cm(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    eps = 1e-12\n",
    "    acc  = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec  = tp / (tp + fn + eps)\n",
    "    f1   = 2 * prec * rec / (prec + rec + eps)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], ['0', '1'])\n",
    "    plt.yticks([0, 1], ['0', '1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(\n",
    "                j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Validate upstream artifacts\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[CHECK] Verifying predictions and labels are available...\")\n",
    "\n",
    "if 'y_pred' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Predictions 'y_pred' not found. \"\n",
    "        \"Please run Cell 5 (final model inference) first.\"\n",
    "    )\n",
    "\n",
    "if 'yte' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Test labels 'yte' not found. \"\n",
    "        \"Please run Cell 5 first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[CHECK] Found predictions for {len(y_pred)} test segments.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Confusion Matrix & Metrics (FINAL model)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc, prec, rec, f1 = compute_metrics_from_cm(cm)\n",
    "\n",
    "print(\"\\n[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(cm, title=\"Confusion Matrix - FINAL EEGNet\")\n",
    "\n",
    "print(\"[DONE] Final model evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0f120-39fd-497e-9098-05e6c82f4807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
