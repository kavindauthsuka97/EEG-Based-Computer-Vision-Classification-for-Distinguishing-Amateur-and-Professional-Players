{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af5b471-cea9-4194-bc23-da5cdc2f9c67",
   "metadata": {},
   "source": [
    "Cell 1 — Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64aaa897-ab61-4eb4-a558-4c078acf751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Base path set to:\n",
      "C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\n",
      "[INFO] Found 18 folders:\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Selected test subject (LOSO): adam\n",
      "[INFO] Training subjects (17):\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam']\n",
      "[INFO] Sampling rate (SFREQ) = 128.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 1: Setup & discovery\n",
    "# =========================\n",
    "\n",
    "import os  # file ops\n",
    "import glob  # file matching\n",
    "import numpy as np  # arrays\n",
    "import random  # seeds\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data path\n",
    "BASE_PATH = r\"C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\"\n",
    "print(f\"[INFO] Base path set to:\\n{BASE_PATH}\")\n",
    "\n",
    "# List subject folders\n",
    "all_folders = [p for p in sorted(os.listdir(BASE_PATH)) if os.path.isdir(os.path.join(BASE_PATH, p))]\n",
    "print(f\"[INFO] Found {len(all_folders)} folders:\\n{all_folders}\")\n",
    "\n",
    "# Expected count (change if needed)\n",
    "assert len(all_folders) == 18, f\"Expected 18 folders, found {len(all_folders)}.\"\n",
    "\n",
    "# LOSO test subject\n",
    "TEST_SUBJECT = \"adam\"\n",
    "print(f\"[INFO] Selected test subject (LOSO): {TEST_SUBJECT}\")\n",
    "\n",
    "# Train subjects = remaining\n",
    "train_subjects = [s for s in all_folders if s.lower() != TEST_SUBJECT.lower()]\n",
    "print(f\"[INFO] Training subjects ({len(train_subjects)}):\\n{train_subjects}\")\n",
    "\n",
    "# Sampling rate\n",
    "SFREQ = 128.0\n",
    "print(f\"[INFO] Sampling rate (SFREQ) = {SFREQ} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e649681-2861-4ce2-bbd1-0d9dd3359f42",
   "metadata": {},
   "source": [
    "Cell 2 — Load EEG + labels per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "943c5c6e-a7aa-4618-899c-daf341c25358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 2] Loading data from folders...\n",
      "[STEP 2]       Amin | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Amin | EEG=          Amin.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Amin1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]      Amin1 | EEG=         Amin1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Cole | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Cole | EEG=          Cole.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Daniel | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]     Daniel | EEG=        Daniel.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]    Ismayil | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]    Ismayil | EEG=       Ismayil.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Jack | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Jack | EEG=          Jack.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]      James | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      James | EEG=         James.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Josh | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Josh | EEG=          Josh.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Marjan | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Marjan | EEG=        Marjan.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]        Max | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Max | EEG=           Max.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Mina | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mina | EEG=          Mina.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 1 | EEG=        Mina 1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 3 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 3 | EEG=        Mina 3.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]   Mohammad | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]   Mohammad | EEG=      Mohammad.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Mona | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mona | EEG=          Mona.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Roddy | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      Roddy | EEG=         Roddy.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]        Sam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Sam | EEG=           Sam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       adam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       adam | EEG=          adam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2] Loading complete ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 2: Load data (EMPTY LABEL => CLASS 0)\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path  # helper\n",
    "\n",
    "def load_subject(subj_dir: str, subj_name: str, expected_channels: int = 14):\n",
    "    \"\"\"\n",
    "    EEG file: {subj_name}.npy\n",
    "    Label file: {subj_name}_target.npy\n",
    "    Label rule:\n",
    "      - empty array -> class 0\n",
    "      - [0] -> class 0\n",
    "      - [1] -> class 1\n",
    "    \"\"\"\n",
    "\n",
    "    # EEG file path\n",
    "    eeg_file = os.path.join(subj_dir, f\"{subj_name}.npy\")\n",
    "    assert os.path.exists(eeg_file), f\"[ERROR] Missing EEG file: {eeg_file}\"\n",
    "\n",
    "    # Label file path\n",
    "    target_file = os.path.join(subj_dir, f\"{subj_name}_target.npy\")\n",
    "    assert os.path.exists(target_file), f\"[ERROR] Missing label file: {target_file}\"\n",
    "\n",
    "    # Load EEG\n",
    "    X_raw = np.load(eeg_file, allow_pickle=True)\n",
    "    X_raw = np.asarray(X_raw)\n",
    "\n",
    "    # Standardize EEG to (n_trials, 14, T)\n",
    "    if X_raw.ndim == 2:\n",
    "        X_trials = X_raw[None, :, :]\n",
    "    elif X_raw.ndim == 3:\n",
    "        dims = list(X_raw.shape)\n",
    "        chan_axes = [i for i, d in enumerate(dims) if d == expected_channels]\n",
    "        chan_axis = chan_axes[0] if len(chan_axes) == 1 else 1\n",
    "        time_axis = int(np.argmax(dims))\n",
    "        trial_axis = [i for i in range(3) if i not in (chan_axis, time_axis)][0]\n",
    "        X_trials = np.moveaxis(X_raw, (trial_axis, chan_axis, time_axis), (0, 1, 2))\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] Unsupported EEG shape {X_raw.shape} in {eeg_file}\")\n",
    "\n",
    "    assert X_trials.shape[1] == expected_channels, f\"[ERROR] Expected {expected_channels} channels, got {X_trials.shape[1]} in {eeg_file}\"\n",
    "\n",
    "    # Load label\n",
    "    y_raw = np.load(target_file, allow_pickle=True)\n",
    "    y_arr = np.asarray(y_raw).ravel()\n",
    "\n",
    "    # Apply your rule\n",
    "    if y_arr.size == 0:\n",
    "        label = 0\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\")\n",
    "    else:\n",
    "        label = int(y_arr[0])\n",
    "        if label not in (0, 1):\n",
    "            raise ValueError(f\"[ERROR] Invalid label {label} in {target_file} (must be 0 or 1)\")\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape={y_arr.shape}, value={y_arr} -> class={label}\")\n",
    "\n",
    "    # Repeat label for all trials\n",
    "    y_trials = np.full((X_trials.shape[0],), label, dtype=np.int32)\n",
    "\n",
    "    return X_trials.astype(np.float32), y_trials.astype(np.int32), eeg_file, target_file\n",
    "\n",
    "\n",
    "# Load all subjects\n",
    "subject_data = {}\n",
    "print(\"[STEP 2] Loading data from folders...\")\n",
    "\n",
    "for subj in all_folders:\n",
    "    subj_dir = os.path.join(BASE_PATH, subj)\n",
    "    X_trials, y_trials, eeg_file, target_file = load_subject(subj_dir, subj)\n",
    "\n",
    "    subject_data[subj] = (X_trials, y_trials)\n",
    "\n",
    "    print(f\"[STEP 2] {subj:>10} | EEG={Path(eeg_file).name:>18} | X={X_trials.shape} | y={y_trials.shape} | class={y_trials[0]}\")\n",
    "\n",
    "print(\"[STEP 2] Loading complete ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd93ca8-a94c-485c-8a61-e6e7441f6299",
   "metadata": {},
   "source": [
    "Cell 3 — Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "733e592c-98e1-4cbd-afd2-6fdc8cf407f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 3] Preprocessing functions ready ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3: Preprocessing functions\n",
    "# =========================\n",
    "\n",
    "import mne  # EEG processing\n",
    "import pywt  # wavelets\n",
    "from sklearn.decomposition import FastICA  # ICA\n",
    "from typing import Optional, Tuple, Union, Sequence, Dict, List  # typing\n",
    "\n",
    "def wavelet_enhanced_ica(\n",
    "    data: np.ndarray,\n",
    "    n_components: int = 10,\n",
    "    wavelet: str = \"db4\",\n",
    "    level: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> np.ndarray:\n",
    "    n_ch, n_t = data.shape\n",
    "    n_components = min(n_components, n_ch)\n",
    "\n",
    "    coeffs = pywt.wavedec(data, wavelet=wavelet, level=level, axis=1)\n",
    "    A = coeffs[0]\n",
    "\n",
    "    ica = FastICA(n_components=n_components, random_state=random_state)\n",
    "    S = ica.fit_transform(A.T).T\n",
    "    A_denoised = ica.inverse_transform(S.T).T\n",
    "\n",
    "    coeffs[0] = A_denoised\n",
    "    cleaned = pywt.waverec(coeffs, wavelet=wavelet, axis=1)\n",
    "\n",
    "    if cleaned.shape[1] != n_t:\n",
    "        if cleaned.shape[1] > n_t:\n",
    "            cleaned = cleaned[:, :n_t]\n",
    "        else:\n",
    "            cleaned = np.pad(cleaned, ((0, 0), (0, n_t - cleaned.shape[1])), mode=\"constant\")\n",
    "    return cleaned\n",
    "\n",
    "def _names_from_index_mapping(n_channels: int, index_to_name: Optional[Dict[int, str]]) -> List[str]:\n",
    "    if index_to_name is None:\n",
    "        return [f\"EEG{i+1}\" for i in range(n_channels)]\n",
    "    keys = list(index_to_name.keys())\n",
    "    is_zero_based = (0 in keys) and (1 not in keys)\n",
    "    names = []\n",
    "    for i in range(n_channels):\n",
    "        key = i if is_zero_based else (i + 1)\n",
    "        names.append(index_to_name.get(key, f\"EEG{i+1}\"))\n",
    "    return names\n",
    "\n",
    "def preprocess_eeg(\n",
    "    eeg: np.ndarray,\n",
    "    sfreq: float,\n",
    "    *,\n",
    "    index_to_name: Optional[Dict[int, str]] = None,\n",
    "    use_standard_1010: bool = True,\n",
    "    resample_to: Optional[float] = None,\n",
    "    notch_freqs: Union[None, float, Sequence[float]] = 50.0,\n",
    "    highpass: Optional[float] = 0.05,\n",
    "    bad_point_z: float = 6.0,\n",
    "    bad_channel_z: float = 5.0,\n",
    "    interpolate_bad_channels: bool = True,\n",
    "    car: bool = True,\n",
    "    use_wica: bool = True,\n",
    "    wica_components: int = 10,\n",
    "    wica_wavelet: str = \"db4\",\n",
    "    wica_level: int = 3,\n",
    "    wica_random_state: int = 42,\n",
    "    return_raw: bool = False,\n",
    "):\n",
    "\n",
    "    eeg = np.asarray(eeg, dtype=np.float32)\n",
    "    assert eeg.ndim == 2, \"eeg must be 2D: (n_channels, n_times)\"\n",
    "\n",
    "    n_channels, _ = eeg.shape\n",
    "    ch_names = _names_from_index_mapping(n_channels, index_to_name)\n",
    "    ch_types = ['eog' if str(n).upper().startswith(\"EOG\") else 'eeg' for n in ch_names]\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    raw = mne.io.RawArray(eeg, info, verbose=False)\n",
    "\n",
    "    montage_applied = False\n",
    "    if use_standard_1010:\n",
    "        try:\n",
    "            mont = mne.channels.make_standard_montage(\"standard_1010\")\n",
    "            raw.set_montage(mont, match_case=False, on_missing=\"ignore\")\n",
    "            montage_applied = True\n",
    "        except Exception:\n",
    "            montage_applied = False\n",
    "\n",
    "    if resample_to is not None and float(resample_to) != float(sfreq):\n",
    "        raw.resample(sfreq=resample_to, npad=\"auto\")\n",
    "    sfreq_out = float(raw.info[\"sfreq\"])\n",
    "\n",
    "    if notch_freqs is not None:\n",
    "        raw.notch_filter(freqs=notch_freqs, verbose=False)\n",
    "    if highpass is not None:\n",
    "        raw.filter(l_freq=highpass, h_freq=None, verbose=False)\n",
    "\n",
    "    X = raw.get_data()\n",
    "    mu = np.mean(X, axis=1, keepdims=True)\n",
    "    sd = np.std(X, axis=1, keepdims=True) + 1e-12\n",
    "    hi = mu + bad_point_z * sd\n",
    "    lo = mu - bad_point_z * sd\n",
    "    bad_idx = (X > hi) | (X < lo)\n",
    "\n",
    "    if np.any(bad_idx):\n",
    "        X_fixed = X.copy()\n",
    "        t = np.arange(X.shape[1], dtype=float)\n",
    "        for ch in range(n_channels):\n",
    "            mask = bad_idx[ch]\n",
    "            if mask.any():\n",
    "                good = ~mask\n",
    "                if good.sum() >= 2:\n",
    "                    X_fixed[ch, mask] = np.interp(t[mask], t[good], X_fixed[ch, good])\n",
    "        raw._data = X_fixed\n",
    "\n",
    "    if interpolate_bad_channels and montage_applied:\n",
    "        X = raw.get_data(picks=\"eeg\")\n",
    "        if X.size > 0:\n",
    "            ch_std = X.std(axis=1)\n",
    "            med = np.median(ch_std)\n",
    "            mad = np.median(np.abs(ch_std - med)) + 1e-12\n",
    "            z = 0.6745 * (ch_std - med) / mad\n",
    "            eeg_names = mne.pick_info(raw.info, mne.pick_types(raw.info, eeg=True)).ch_names\n",
    "            bads = [eeg_names[i] for i in np.where(np.abs(z) > bad_channel_z)[0]]\n",
    "            raw.info[\"bads\"] = bads\n",
    "            if bads:\n",
    "                raw.interpolate_bads(reset_bads=True, verbose=False)\n",
    "\n",
    "    if car:\n",
    "        raw.set_eeg_reference(\"average\", projection=True)\n",
    "        raw.apply_proj()\n",
    "\n",
    "    if use_wica:\n",
    "        cleaned = wavelet_enhanced_ica(\n",
    "            raw.get_data(),\n",
    "            n_components=wica_components,\n",
    "            wavelet=wica_wavelet,\n",
    "            level=wica_level,\n",
    "            random_state=wica_random_state\n",
    "        )\n",
    "        raw = mne.io.RawArray(cleaned, raw.info, verbose=False)\n",
    "\n",
    "    cleaned = raw.get_data()\n",
    "    return cleaned, sfreq_out\n",
    "\n",
    "print(\"[STEP 3] Preprocessing functions ready ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a9d20-889d-45c4-bbc2-de62216908cd",
   "metadata": {},
   "source": [
    "Cell 4 — Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bfcbde0-1cc2-409c-8627-93da46606661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 4] Starting preprocessing...\n",
      "[INFO] USE_WICA = True\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 01/17: Amin | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 02/17: Amin1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 03/17: Cole | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 04/17: Daniel | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 05/17: Ismayil | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 06/17: Jack | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 07/17: James | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 08/17: Josh | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 09/17: Marjan | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 10/17: Max | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 11/17: Mina | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 12/17: Mina 1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 13/17: Mina 3 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 14/17: Mohammad | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 15/17: Mona | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 16/17: Roddy | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 17/17: Sam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Test subject: adam | trials=1 | fs_out=128.0\n",
      "[STEP 4] Preprocessing done ✅\n",
      "  X_train_trials_all: (17, 14, 38400)\n",
      "  y_train_trials_all: (17,) (array([0, 1], dtype=int32), array([9, 8]))\n",
      "  X_test_trials_all : (1, 14, 38400)\n",
      "  y_test_trials_all : (1,) (array([1], dtype=int32), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 4: Apply preprocessing (LOSO)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 4] Starting preprocessing...\")\n",
    "\n",
    "USE_WICA = True\n",
    "print(f\"[INFO] USE_WICA = {USE_WICA}\")\n",
    "\n",
    "X_train_trials_all, y_train_trials_all = [], []\n",
    "X_test_trials_all, y_test_trials_all = [], []\n",
    "\n",
    "def preprocess_trials(X_trials, y_trials):\n",
    "    cleaned_list = []\n",
    "    label_list = []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        trial = X_trials[i]\n",
    "        label = int(y_trials[i])\n",
    "\n",
    "        cleaned, fs_out = preprocess_eeg(\n",
    "            eeg=trial,\n",
    "            sfreq=SFREQ,\n",
    "            use_wica=USE_WICA\n",
    "        )\n",
    "\n",
    "        cleaned_list.append(cleaned.astype(np.float32, copy=False))\n",
    "        label_list.append(label)\n",
    "\n",
    "    return cleaned_list, np.array(label_list, dtype=np.int32), fs_out\n",
    "\n",
    "# Train subjects\n",
    "for idx, subj in enumerate(train_subjects, start=1):\n",
    "    X_trials, y_trials = subject_data[subj]\n",
    "    cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "    X_train_trials_all.extend(cleaned_trials)\n",
    "    y_train_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "    print(f\"[STEP 4] Train {idx:02d}/{len(train_subjects)}: {subj} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Test subject\n",
    "X_trials, y_trials = subject_data[TEST_SUBJECT]\n",
    "cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "X_test_trials_all.extend(cleaned_trials)\n",
    "y_test_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "print(f\"[STEP 4] Test subject: {TEST_SUBJECT} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Convert to arrays\n",
    "X_train_trials_all = np.array(X_train_trials_all, dtype=np.float32)\n",
    "y_train_trials_all = np.array(y_train_trials_all, dtype=np.int32)\n",
    "\n",
    "X_test_trials_all = np.array(X_test_trials_all, dtype=np.float32)\n",
    "y_test_trials_all = np.array(y_test_trials_all, dtype=np.int32)\n",
    "\n",
    "print(\"[STEP 4] Preprocessing done ✅\")\n",
    "print(\"  X_train_trials_all:\", X_train_trials_all.shape)\n",
    "print(\"  y_train_trials_all:\", y_train_trials_all.shape, np.unique(y_train_trials_all, return_counts=True))\n",
    "print(\"  X_test_trials_all :\", X_test_trials_all.shape)\n",
    "print(\"  y_test_trials_all :\", y_test_trials_all.shape, np.unique(y_test_trials_all, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc3da1-2967-4ee3-84cd-bfcf0f98c0b1",
   "metadata": {},
   "source": [
    "Cell 5 — Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c21ad54-7001-4039-9360-29dfa2905127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 5] Starting segmentation...\n",
      "[STEP 5] Segmented 5/17 trials...\n",
      "[STEP 5] Segmented 10/17 trials...\n",
      "[STEP 5] Segmented 15/17 trials...\n",
      "[STEP 5] Segmented 17/17 trials...\n",
      "[STEP 5] Segmented 1/1 trials...\n",
      "[STEP 5] Segmentation done ✅\n",
      "  X_train: (5100, 14, 128, 1)\n",
      "  y_train: (5100,) (array([0, 1], dtype=int32), array([2700, 2400]))\n",
      "  X_test : (300, 14, 128, 1)\n",
      "  y_test : (300,) (array([1], dtype=int32), array([300]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 5: Segmentation\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 5] Starting segmentation...\")\n",
    "\n",
    "def segment_trial(trial_2d: np.ndarray, label: int, segment_len: int = 128):\n",
    "    C, T = trial_2d.shape\n",
    "    n_segs = T // segment_len\n",
    "    use_T = n_segs * segment_len\n",
    "\n",
    "    if n_segs == 0:\n",
    "        return np.empty((0, C, segment_len), dtype=np.float32), np.empty((0,), dtype=np.int32)\n",
    "\n",
    "    trial_trim = trial_2d[:, :use_T]\n",
    "    Xsegs = trial_trim.reshape(C, n_segs, segment_len).transpose(1, 0, 2)\n",
    "    ysegs = np.full((n_segs,), int(label), dtype=np.int32)\n",
    "    return Xsegs.astype(np.float32, copy=False), ysegs\n",
    "\n",
    "def segment_dataset(X_trials: np.ndarray, y_trials: np.ndarray, segment_len: int = 128):\n",
    "    X_all, y_all = [], []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        Xsegs, ysegs = segment_trial(X_trials[i], int(y_trials[i]), segment_len=segment_len)\n",
    "        X_all.append(Xsegs)\n",
    "        y_all.append(ysegs)\n",
    "\n",
    "        if (i + 1) % 5 == 0 or (i + 1) == X_trials.shape[0]:\n",
    "            print(f\"[STEP 5] Segmented {i+1}/{X_trials.shape[0]} trials...\")\n",
    "\n",
    "    X_out = np.concatenate(X_all, axis=0)\n",
    "    y_out = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    X_out = np.expand_dims(X_out, axis=-1)  # (N,14,128,1)\n",
    "    return X_out.astype(np.float32), y_out.astype(np.int32)\n",
    "\n",
    "X_train, y_train = segment_dataset(X_train_trials_all, y_train_trials_all, segment_len=128)\n",
    "X_test,  y_test  = segment_dataset(X_test_trials_all,  y_test_trials_all,  segment_len=128)\n",
    "\n",
    "print(\"[STEP 5] Segmentation done ✅\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  y_train:\", y_train.shape, np.unique(y_train, return_counts=True))\n",
    "print(\"  X_test :\", X_test.shape)\n",
    "print(\"  y_test :\", y_test.shape, np.unique(y_test, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61b4ffd5-bf2f-490e-8854-0a2fc747f1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e69ba7-3952-47b1-bd80-f05237b2e018",
   "metadata": {},
   "source": [
    "Cell 6 — Split train/val + Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97fda4ae-e1c8-4fd1-bf0c-c82ae2a85764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 6] Normalizing using ALL training data (no train/val split)...\n",
      "[STEP 6] Normalization done ✅\n",
      "  X_train_norm: (5100, 14, 128, 1)\n",
      "  X_test_norm : (300, 14, 128, 1)\n",
      "  y_train_norm: (5100,)\n",
      "  y_test_norm : (300,)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 6: Normalize (ALL training data)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 6] Normalizing using ALL training data (no train/val split)...\")\n",
    "\n",
    "def normalize_eeg(train_data: np.ndarray, test_data: np.ndarray):\n",
    "    if train_data.ndim == 5:\n",
    "        train_flat = train_data.reshape(-1, *train_data.shape[2:])\n",
    "        test_flat  = test_data.reshape(-1, *test_data.shape[2:])\n",
    "    else:\n",
    "        train_flat, test_flat = train_data, test_data\n",
    "\n",
    "    # Compute stats on ALL training data\n",
    "    train_mean = np.mean(train_flat, axis=(0, 2, 3), keepdims=True)\n",
    "    train_std  = np.std(train_flat,  axis=(0, 2, 3), keepdims=True) + 1e-8\n",
    "\n",
    "    train_norm = (train_flat - train_mean) / train_std\n",
    "    test_norm  = (test_flat  - train_mean) / train_std\n",
    "\n",
    "    if train_data.ndim == 5:\n",
    "        train_norm = train_norm.reshape(train_data.shape)\n",
    "        test_norm  = test_norm.reshape(test_data.shape)\n",
    "\n",
    "    return (\n",
    "        train_norm.astype(np.float32),\n",
    "        test_norm.astype(np.float32),\n",
    "        train_mean,\n",
    "        train_std\n",
    "    )\n",
    "\n",
    "# Normalize ALL training data\n",
    "X_train_norm, X_test_norm, mean, std = normalize_eeg(X_train, X_test)\n",
    "\n",
    "y_train_norm = y_train.astype(np.float32)\n",
    "y_test_norm  = y_test.astype(np.float32)\n",
    "\n",
    "print(\"[STEP 6] Normalization done ✅\")\n",
    "print(\"  X_train_norm:\", X_train_norm.shape)\n",
    "print(\"  X_test_norm :\", X_test_norm.shape)\n",
    "print(\"  y_train_norm:\", y_train_norm.shape)\n",
    "print(\"  y_test_norm :\", y_test_norm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f527c-34db-4c23-bdbd-c8ce03b519f0",
   "metadata": {},
   "source": [
    "Cell 7 — EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9ea4279-53c2-49c1-b834-7b1e1c20775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\DL\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Xtr: (5100, 14, 128, 1), ytr: (5100,)\n",
      "[INFO] Test / LOSO subject is NOT used here.\n",
      "[SPLIT] Train: (4080, 14, 128, 1), Val: (1020, 14, 128, 1)\n",
      "Epoch 1/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 122ms/step - accuracy: 0.4985 - loss: 0.7404 - val_accuracy: 0.5294 - val_loss: 0.6914 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.5409 - loss: 0.6864 - val_accuracy: 0.5304 - val_loss: 0.6912 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.5855 - loss: 0.6662 - val_accuracy: 0.5373 - val_loss: 0.6892 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.6132 - loss: 0.6396 - val_accuracy: 0.5451 - val_loss: 0.6865 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.6434 - loss: 0.6131 - val_accuracy: 0.5539 - val_loss: 0.6816 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.6694 - loss: 0.5891 - val_accuracy: 0.5725 - val_loss: 0.6722 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.6973 - loss: 0.5617 - val_accuracy: 0.6118 - val_loss: 0.6540 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7203 - loss: 0.5294 - val_accuracy: 0.6186 - val_loss: 0.6394 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7431 - loss: 0.5100 - val_accuracy: 0.6353 - val_loss: 0.6240 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.7539 - loss: 0.4907 - val_accuracy: 0.7020 - val_loss: 0.6042 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.7637 - loss: 0.4846 - val_accuracy: 0.6824 - val_loss: 0.5986 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.7654 - loss: 0.4707 - val_accuracy: 0.7029 - val_loss: 0.5892 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.7951 - loss: 0.4330 - val_accuracy: 0.7078 - val_loss: 0.5572 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.7993 - loss: 0.4244 - val_accuracy: 0.6804 - val_loss: 0.5688 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8142 - loss: 0.4013 - val_accuracy: 0.6863 - val_loss: 0.5451 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8309 - loss: 0.3827 - val_accuracy: 0.6431 - val_loss: 0.5989 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8480 - loss: 0.3434 - val_accuracy: 0.6402 - val_loss: 0.6166 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8689 - loss: 0.3036 - val_accuracy: 0.6235 - val_loss: 0.6881 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8600 - loss: 0.3204 - val_accuracy: 0.6627 - val_loss: 0.6221 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8797 - loss: 0.2920 - val_accuracy: 0.6627 - val_loss: 0.6318 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.8951 - loss: 0.2532 - val_accuracy: 0.6873 - val_loss: 0.5729 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8875 - loss: 0.2724 - val_accuracy: 0.6618 - val_loss: 0.6753 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9000 - loss: 0.2432 - val_accuracy: 0.7127 - val_loss: 0.5623 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9027 - loss: 0.2355 - val_accuracy: 0.7275 - val_loss: 0.5212 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9069 - loss: 0.2353 - val_accuracy: 0.6922 - val_loss: 0.6123 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9127 - loss: 0.2279 - val_accuracy: 0.7225 - val_loss: 0.5922 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9118 - loss: 0.2237 - val_accuracy: 0.7480 - val_loss: 0.4861 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9235 - loss: 0.1998 - val_accuracy: 0.7892 - val_loss: 0.4426 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9221 - loss: 0.2047 - val_accuracy: 0.8206 - val_loss: 0.3715 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9174 - loss: 0.2136 - val_accuracy: 0.8539 - val_loss: 0.2989 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9176 - loss: 0.2017 - val_accuracy: 0.8706 - val_loss: 0.2810 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9255 - loss: 0.1960 - val_accuracy: 0.9176 - val_loss: 0.2168 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9164 - loss: 0.2088 - val_accuracy: 0.9186 - val_loss: 0.2049 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9245 - loss: 0.2000 - val_accuracy: 0.9324 - val_loss: 0.1673 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9262 - loss: 0.1899 - val_accuracy: 0.9314 - val_loss: 0.1617 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9277 - loss: 0.1881 - val_accuracy: 0.9461 - val_loss: 0.1477 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9267 - loss: 0.1954 - val_accuracy: 0.9529 - val_loss: 0.1306 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9346 - loss: 0.1731 - val_accuracy: 0.9637 - val_loss: 0.1269 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9301 - loss: 0.1848 - val_accuracy: 0.9627 - val_loss: 0.1039 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9333 - loss: 0.1682 - val_accuracy: 0.9657 - val_loss: 0.0995 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9301 - loss: 0.1720 - val_accuracy: 0.9676 - val_loss: 0.0926 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9341 - loss: 0.1638 - val_accuracy: 0.9627 - val_loss: 0.1019 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9321 - loss: 0.1714 - val_accuracy: 0.9598 - val_loss: 0.0989 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9346 - loss: 0.1686 - val_accuracy: 0.9608 - val_loss: 0.1022 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9301 - loss: 0.1774 - val_accuracy: 0.9578 - val_loss: 0.1155 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9259 - loss: 0.1858\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9270 - loss: 0.1804 - val_accuracy: 0.9363 - val_loss: 0.1640 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9289 - loss: 0.1792 - val_accuracy: 0.9637 - val_loss: 0.1109 - learning_rate: 5.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9434 - loss: 0.1598 - val_accuracy: 0.9657 - val_loss: 0.1014 - learning_rate: 5.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9404 - loss: 0.1544 - val_accuracy: 0.9627 - val_loss: 0.1004 - learning_rate: 5.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9377 - loss: 0.1573 - val_accuracy: 0.9569 - val_loss: 0.1102 - learning_rate: 5.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9417 - loss: 0.1549 - val_accuracy: 0.9637 - val_loss: 0.1023 - learning_rate: 5.0000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9412 - loss: 0.1590 - val_accuracy: 0.9647 - val_loss: 0.0996 - learning_rate: 5.0000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9407 - loss: 0.1523 - val_accuracy: 0.9676 - val_loss: 0.0933 - learning_rate: 5.0000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.9424 - loss: 0.1541 - val_accuracy: 0.9627 - val_loss: 0.0974 - learning_rate: 5.0000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.9397 - loss: 0.1522 - val_accuracy: 0.9618 - val_loss: 0.0962 - learning_rate: 5.0000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.9346 - loss: 0.1610 - val_accuracy: 0.9657 - val_loss: 0.0969 - learning_rate: 5.0000e-04\n",
      "Epoch 56: early stopping\n",
      "Restoring model weights from the end of the best epoch: 41.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] Model saved as cole-final.h5\n",
      "\n",
      "[CONFUSION MATRIX]\n",
      "[[521  19]\n",
      " [ 14 466]]\n",
      "[VAL] acc=0.9676, prec=0.9608, rec=0.9708, f1=0.9658\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 4: 80:20 Train / Validation Split (EEGNet)\n",
    "# ====================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, SeparableConv2D, BatchNormalization, Activation,\n",
    "    DepthwiseConv2D, Conv2D, AveragePooling2D,\n",
    "    Dropout, Dense, Flatten\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# --------------------------------------------------------------------\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# EEGNet model\n",
    "# --------------------------------------------------------------------\n",
    "def create_eegnet_model(input_shape, dropout_rate=0.5, num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(16, (1, 64), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = DepthwiseConv2D(\n",
    "        (input_shape[0], 1),\n",
    "        depth_multiplier=2,\n",
    "        padding='valid',\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 4))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = SeparableConv2D(16, (1, 16), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 8))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Compile model\n",
    "# --------------------------------------------------------------------\n",
    "def compile_model():\n",
    "    input_shape = (Xtr.shape[1], Xtr.shape[2], Xtr.shape[3])\n",
    "    model = create_eegnet_model(input_shape)\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-3),\n",
    "        loss=BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[BinaryAccuracy(name=\"accuracy\", threshold=0.5)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def make_callbacks():\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"loss\",\n",
    "            factor=0.5,\n",
    "            patience=4,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Use ALL normalized training data\n",
    "# --------------------------------------------------------------------\n",
    "Xtr = X_train_norm\n",
    "ytr = y_train_norm\n",
    "\n",
    "print(f\"[TRAIN] Xtr: {Xtr.shape}, ytr: {ytr.shape}\")\n",
    "print(\"[INFO] Test / LOSO subject is NOT used here.\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 80:20 Stratified Split\n",
    "# --------------------------------------------------------------------\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(sss.split(Xtr, ytr))\n",
    "\n",
    "X_train, X_val = Xtr[train_idx], Xtr[val_idx]\n",
    "y_train, y_val = ytr[train_idx], ytr[val_idx]\n",
    "\n",
    "print(f\"[SPLIT] Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Train model\n",
    "# --------------------------------------------------------------------\n",
    "model = compile_model()\n",
    "callbacks = make_callbacks()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Save final model\n",
    "# --------------------------------------------------------------------\n",
    "model.save(\"adam-final.h5\")\n",
    "print(\"[SAVE] Model saved as cole-final.h5\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Validation Confusion Matrix\n",
    "# --------------------------------------------------------------------\n",
    "y_prob = model.predict(X_val, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "y_true = y_val.astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "eps = 1e-12\n",
    "\n",
    "acc  = (tp + tn) / max(tp + tn + fp + fn, eps)\n",
    "prec = tp / max(tp + fp, eps)\n",
    "rec  = tp / max(tp + fn, eps)\n",
    "f1   = 2 * prec * rec / max(prec + rec, eps)\n",
    "\n",
    "print(\"\\n[CONFUSION MATRIX]\")\n",
    "print(cm)\n",
    "print(f\"[VAL] acc={acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e2a13-6367-41ca-be66-a43594c2940e",
   "metadata": {},
   "source": [
    "Cell 8 — Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2ac6662-33c1-4e25-b6c5-d47c860fba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Preparing TEST data...\n",
      "[TEST] Xte shape: (300, 14, 128, 1)\n",
      "[TEST] yte shape: (300,), class distribution: (array([1]), array([300]))\n",
      "[TEST] Loading final model: adam-final.h5\n",
      "[TEST] Running inference on TEST subject...\n",
      "[TEST] Mean predicted probability: 0.3700\n",
      "\n",
      "[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[  0   0]\n",
      " [201  99]]\n",
      "Accuracy : 0.3300\n",
      "Precision: 1.0000\n",
      "Recall   : 0.3300\n",
      "F1-score : 0.4962\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Inference on TEST subject (adam) using FINAL EEGNet\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Prepare TEST data\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Preparing TEST data...\")\n",
    "\n",
    "Xte = X_test_norm\n",
    "yte = y_test_norm.astype(int)\n",
    "\n",
    "print(f\"[TEST] Xte shape: {Xte.shape}\")\n",
    "print(f\"[TEST] yte shape: {yte.shape}, class distribution: {np.unique(yte, return_counts=True)}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Load FINAL trained model\n",
    "# ------------------------------------------------------------------\n",
    "model_path = \"adam-final.h5\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        \"Final model 'cole-final.h5' not found. \"\n",
    "        \"Please run Cell 4 (80:20 training) first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[TEST] Loading final model: {model_path}\")\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Run inference on TEST subject\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Running inference on TEST subject...\")\n",
    "\n",
    "y_prob = model.predict(Xte, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(f\"[TEST] Mean predicted probability: {y_prob.mean():.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Metrics on TEST subject (segment-level)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc = accuracy_score(yte, y_pred)\n",
    "prec = precision_score(yte, y_pred, zero_division=0)\n",
    "rec = recall_score(yte, y_pred, zero_division=0)\n",
    "f1 = f1_score(yte, y_pred, zero_division=0)\n",
    "\n",
    "print(\"\\n[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572dbf3e-bbb5-4865-883f-30e374700949",
   "metadata": {},
   "source": [
    "Cell 9 - Performance Matrices for each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21a57240-50ea-4736-a7ac-52ef7878b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] Verifying predictions and labels are available...\n",
      "[CHECK] Found predictions for 300 test segments.\n",
      "\n",
      "[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[  0   0]\n",
      " [201  99]]\n",
      "Accuracy : 0.3300\n",
      "Precision: 1.0000\n",
      "Recall   : 0.3300\n",
      "F1-score : 0.4962\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg8AAAHWCAYAAADw/GrYAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQmtJREFUeJzt3XtclGX+//H3jcqAwoCoiBSCh/KQpmnFupZCmofMzdQtzTY8ZFbSQdPMTqC22dpmWpnW71vqllbrVtZqW5mmdqCTLtthy8QobRU1DRBUTnP//iAmR0DnkoFx4vV8PO6HznVfc93XjIPz4XMdbsu2bVsAAABeCvJ3BwAAQGAheAAAAEYIHgAAgBGCBwAAYITgAQAAGCF4AAAARggeAACAEYIHAABghOABAAAYIXiAV7Zv364BAwYoIiJClmVp9erVPm3/+++/l2VZWrZsmU/bDWRJSUlKSkrydzcAoBKChwCyY8cOTZo0SW3btlVISIicTqd69+6thQsX6siRI7V67ZSUFH3xxRf685//rOeee07nn39+rV6vLo0dO1aWZcnpdFb5Pm7fvl2WZcmyLP31r381bn/37t1KT09XZmamD3pbNxISEtyv+fjj6NGjkqRly5bJsix99tln7uelp6fLsiy1bNlShw8frrLdyy+/vMpr5ubmKiQkRJZl6euvv66yztixYxUWFmb8eiqC0+qOhx56yF03KSmp2nodO3as1HZ2drZSU1N19tlnq3HjxmrcuLE6d+6syZMn6/PPP/eoW5P352SefPJJgm/UmYb+7gC8s3btWv3xj3+Uw+HQddddpy5duqi4uFjvv/++pk+frq+++kpPP/10rVz7yJEjysjI0D333KPU1NRauUZ8fLyOHDmiRo0a1Ur7J9OwYUMdPnxY//znP3XVVVd5nFuxYoVCQkLcX5qmdu/erVmzZikhIUHdu3f3+nlvv/32KV3PV7p376477rijUnlwcPBJn7tv3z4tXry4yudXZ9WqVbIsSzExMVqxYoUeeOABo/56Y/To0brssssqlZ933nkej88880zNnTu3Ur2IiAiPx2vWrNHVV1+thg0basyYMerWrZuCgoL0zTff6JVXXtHixYuVnZ2t+Ph4j+edyvtzMk8++aSaN2+usWPH+qxNoDoEDwEgOztbo0aNUnx8vDZs2KBWrVq5z02ePFlZWVlau3ZtrV1///79kqTIyMhau4ZlWQoJCam19k/G4XCod+/eeuGFFyoFDytXrtSQIUP08ssv10lfDh8+rMaNG3v1JV2bzjjjDF177bWn9Nzu3bvr4Ycf1s0336zQ0FCvnvP888/rsssuU3x8vFauXFkrwUOPHj28ek0REREnrbdjxw73z+X69es9fi4l6S9/+YuefPJJBQVVTvCeyvsDnE4YtggA8+bNU0FBgZ555plK/0FJUvv27XXbbbe5H5eWlmrOnDlq166dHA6HEhISdPfdd6uoqMjjeRUp0vfff18XXnihQkJC1LZtW/3tb39z10lPT3f/1jR9+nRZlqWEhARJ5Snkir8fqyI1e6x169bpoosuUmRkpMLCwtShQwfdfffd7vPVzXnYsGGDLr74YjVp0kSRkZG64oorKqW0K66XlZWlsWPHKjIyUhERERo3blyVqeHqXHPNNfrXv/6l3Nxcd9mnn36q7du365prrqlU/+DBg5o2bZq6du2qsLAwOZ1ODR48WP/5z3/cdTZu3KgLLrhAkjRu3Dh3+rvidSYlJalLly7asmWL+vTpo8aNG7vfl+PnPKSkpCgkJKTS6x84cKCaNm2q3bt3e/1aa9v999+vvXv3avHixV7V37lzp9577z2NGjVKo0aNUnZ2tj788MNa7mXNzJs3T4WFhVq6dGmVP5cNGzbUrbfeqri4uErnTN4fl8ulBQsW6JxzzlFISIhatmypSZMm6eeff3bXSUhI0FdffaVNmza5P2PMl0FtIngIAP/85z/Vtm1b/f73v/eq/vXXX6/7779fPXr00KOPPqq+fftq7ty5GjVqVKW6WVlZGjlypC699FI98sgjatq0qcaOHauvvvpKkjR8+HA9+uijkspTvs8995wWLFhg1P+vvvpKl19+uYqKijR79mw98sgj+sMf/qAPPvjghM975513NHDgQO3bt0/p6emaOnWqPvzwQ/Xu3Vvff/99pfpXXXWVDh06pLlz5+qqq67SsmXLNGvWLK/7OXz4cFmWpVdeecVdtnLlSnXs2FE9evSoVP+7777T6tWrdfnll2v+/PmaPn26vvjiC/Xt29f9Rd6pUyfNnj1bknTDDTfoueee03PPPac+ffq42zlw4IAGDx6s7t27a8GCBUpOTq6yfwsXLlSLFi2UkpKisrIySdJTTz2lt99+W48//rhiY2O9fq3eKCkp0U8//eRxeBuMXXzxxbrkkks0b948r+bjvPDCC2rSpIkuv/xyXXjhhWrXrp1WrFhR05dQyeHDhyu9pp9++kmlpaUe9crKyqqsV1hY6K6zZs0atW/fXomJicb9MHl/Jk2apOnTp7vnN40bN04rVqzQwIEDVVJSIklasGCBzjzzTHXs2NH9GbvnnnuM+wV4zcZpLS8vz5ZkX3HFFV7Vz8zMtCXZ119/vUf5tGnTbEn2hg0b3GXx8fG2JHvz5s3usn379tkOh8O+44473GXZ2dm2JPvhhx/2aDMlJcWOj4+v1Ie0tDT72I/Wo48+akuy9+/fX22/K66xdOlSd1n37t3t6Oho+8CBA+6y//znP3ZQUJB93XXXVbre+PHjPdq88sor7WbNmlV7zWNfR5MmTWzbtu2RI0fa/fr1s23btsvKyuyYmBh71qxZVb4HR48etcvKyiq9DofDYc+ePdtd9umnn1Z6bRX69u1rS7KXLFlS5bm+fft6lL311lu2JPuBBx6wv/vuOzssLMweNmzYSV+jqYrPxvFHWlqau87SpUttSfann37qLqv4t9i/f7+9adMmW5I9f/58j3aHDBlS6Xpdu3a1x4wZ43589913282bN7dLSko86h37b2Wi4t+vuiMjI8Ndt+LfpKpj0qRJtm3/+nNZ1Xv/888/2/v373cfhw8fPuX357333rMl2StWrPC4xptvvlmp/Jxzzqn0eQFqC5mH01x+fr4kKTw83Kv6b7zxhiRp6tSpHuUVE7OOnxvRuXNnXXzxxe7HLVq0UIcOHfTdd9+dcp+PVzFX4rXXXpPL5fLqOXv27FFmZqbGjh2rqKgod/m5556rSy+91P06j3XjjTd6PL744ot14MAB93vojWuuuUYbN25UTk6ONmzYoJycnCqHLKTyeRIV49llZWU6cOCAe0hm69atXl/T4XBo3LhxXtUdMGCAJk2apNmzZ2v48OEKCQnRU0895fW1TCQmJmrdunUex3XXXef18/v06aPk5OST/nb9+eef64svvtDo0aPdZaNHj9ZPP/2kt956q0av4Xg33HBDpde0bt06de7c2aNeQkJClfVuv/12Sb/+XFa18iMpKUktWrRwH4sWLaqyL968P6tWrVJERIQuvfRSjwxIz549FRYWpnfffbcG7wZw6pgweZpzOp2SpEOHDnlV/4cfflBQUJDat2/vUR4TE6PIyEj98MMPHuWtW7eu1EbTpk09xlNr6uqrr9b//d//6frrr9ddd92lfv36afjw4Ro5cmSVk8kqXockdejQodK5Tp066a233lJhYaGaNGniLj/+tTRt2lSS9PPPP7vfx5O57LLLFB4erpdeekmZmZm64IIL1L59+yqHSVwulxYuXKgnn3xS2dnZ7qEESWrWrJlX15PKJyaaTI7861//qtdee02ZmZlauXKloqOjT/qc/fv3e/QvLCzspEsemzdvrv79+3vdr6qkp6erb9++WrJkiaZMmVJlneeff15NmjRR27ZtlZWVJUkKCQlRQkKCVqxYoSFDhtSoD8c666yzvHpNTZo0OWG9imC+oKCg0rmnnnpKhw4d0t69e0866fJk78/27duVl5dX7b/xvn37Ttg+UFsIHk5zTqdTsbGx+vLLL42ed/yExeo0aNCgynLbtk/5Gsd+SUlSaGioNm/erHfffVdr167Vm2++qZdeekmXXHKJ3n777Wr7YKomr6WCw+HQ8OHDtXz5cn333XdKT0+vtu6DDz6o++67T+PHj9ecOXMUFRWloKAg3X777V5nWCQZz7b/97//7f7SOP439upccMEFHoFjWlraCV+br/Tp00dJSUmaN29epcyQVP5v88ILL6iwsLDSb/9S+ZdjQUHBKe3tUJsiIiLUqlWrKn8uK+ZAVBVwHu9k74/L5VJ0dHS18z9atGhh1nHARwgeAsDll1+up59+WhkZGerVq9cJ68bHx8vlcmn79u3q1KmTu3zv3r3Kzc2ttN68Jpo2beqxMqHC8dkNSQoKClK/fv3Ur18/zZ8/Xw8++KDuuecevfvuu1X+hlfRz23btlU6980336h58+YeWQdfuuaaa/Tss88qKCioykmmFf7xj38oOTlZzzzzjEd5bm6umjdv7n7sbSDnjcLCQo0bN06dO3fW73//e82bN09XXnmle0VHdVasWOGRGm/btq3P+nQy6enpSkpKqnJ4ZdOmTfrxxx81e/Zsj8+rVJ4xuuGGG7R69epTXjJam4YMGaL/+7//0yeffKILL7zwlNs50fvTrl07vfPOO+rdu/dJg0xffs6Ak2HOQwC488471aRJE11//fXau3dvpfM7duzQwoULJcm9Ac7xKyLmz58vST5NAbdr1055eXkeu+jt2bNHr776qke9gwcPVnpuxWZJxy8frdCqVSt1795dy5cv9whQvvzyS7399ttVbvTjK8nJyZozZ46eeOIJxcTEVFuvQYMGlbIaq1at0v/+9z+Psoogp6pAy9SMGTO0c+dOLV++XPPnz1dCQoJSUlKqfR8r9O7dW/3793cfdRk89O3bV0lJSfrLX/5SaaOtiiGL6dOna+TIkR7HxIkTddZZZ9XKqgtfuPPOO9W4cWONHz++yp9LbzNeJ3p/rrrqKpWVlWnOnDmVnldaWurxmWrSpIlPPmOAN8g8BIB27dpp5cqVuvrqq9WpUyePHSY//PBDrVq1yr2rXLdu3ZSSkqKnn35aubm56tu3rz755BMtX75cw4YNq3YZ4KkYNWqUZsyYoSuvvFK33nqrDh8+rMWLF+vss8/2mDA4e/Zsbd68WUOGDFF8fLz27dunJ598UmeeeaYuuuiiatt/+OGHNXjwYPXq1UsTJkzQkSNH9PjjjysiIqJWU+5BQUG69957T1rv8ssv1+zZszVu3Dj9/ve/1xdffKEVK1ZU+mJu166dIiMjtWTJEoWHh6tJkyZKTExUmzZtjPq1YcMGPfnkk0pLS3MvHV26dKmSkpJ03333ad68eUbt1aW0tLRKn72ioiK9/PLLuvTSS6vdIOwPf/iDFi5cqH379rnH/UtKSqrcQCoqKko333zzCfuxdetWPf/885XK27Vr55HVy8vLq7KeJHcW5KyzztLKlSs1evRodejQwb3DpG3bys7O1sqVKxUUFKQzzzzzhH2Sqn5/pPLAYtKkSZo7d64yMzM1YMAANWrUSNu3b9eqVau0cOFCjRw5UpLUs2dPLV68WA888IDat2+v6OhoXXLJJSe9NnBK/LnUA2a+/fZbe+LEiXZCQoIdHBxsh4eH271797Yff/xx++jRo+56JSUl9qxZs+w2bdrYjRo1suPi4uyZM2d61LHt6pfNHb9EsLqlmrZt22+//bbdpUsXOzg42O7QoYP9/PPPV1qquX79evuKK66wY2Nj7eDgYDs2NtYePXq0/e2331a6xvHLGd955x27d+/edmhoqO10Ou2hQ4fa//3vfz3qHLv87VgVSwmzs7OrfU9t27vlf9Ut1bzjjjvsVq1a2aGhoXbv3r3tjIyMKpdYvvbaa3bnzp3thg0berzOvn372uecc06V1zy2nfz8fDs+Pt7u0aNHpeWLU6ZMsYOCgjyWG9ZUdZ+NY51sqebxKpZAVrT78ssv25LsZ555ptprbNy40ZZkL1y40Lbt8n8rVbOMsl27dtW2c7KlmikpKZX6Wd1xvKysLPumm26y27dvb4eEhNihoaF2x44d7RtvvNHOzMz0qGvy/hzr6aeftnv27GmHhoba4eHhdteuXe0777zT3r17t7tOTk6OPWTIEDs8PNyWxLJN1CrLtg1mkwEAgHqPOQ8AAMAIwQMAADBC8AAAAIwQPAAAACMEDwAAwAjBAwAAAWDu3Lm64IILFB4erujoaA0bNqzSLrxHjx7V5MmT1axZM4WFhWnEiBGVNjHbuXOnhgwZosaNGys6OlrTp0+vdFv6kwnoTaJcLpd2796t8PBwtmYFgHrGtm0dOnRIsbGx1d5krzYcPXpUxcXFPmsvODi42o3SjrVp0yZNnjxZF1xwgUpLS3X33XdrwIAB+u9//+veyXbKlClau3at+46sqampGj58uD744ANJ5fceGjJkiGJiYvThhx9qz549uu6669SoUSM9+OCD3nfaz/tM1MiuXbtOuJkLBwcHB8dv/9i1a1edfe8cOXLEjolu4NP+x8TE2EeOHDHuy759+2xJ9qZNm2zbtu3c3Fy7UaNG9qpVq9x1vv76a1uSexO5N954ww4KCrJzcnLcdRYvXmw7nU67qKjI62sHdOah4ra4F+kyNVQjP/cGAFCXSlWi9/WG+7ugLhQXFytnX5l+2JIgZ3jNsx35h1yK7/m9fvrpJzmdTne5w+GQw+E44XPz8vIklW/NLklbtmxRSUmJx80GO3bsqNatWysjI0O/+93vlJGRoa5du6ply5buOgMHDtRNN92kr776Suedd55X/Q7o4KFiqKKhGqmhRfAAAPWKXf6HP4atw8IthYXX/LoulbcRFxfnUZ6WlnbCe/i4XC7dfvvt6t27t7p06SJJysnJUXBwsCIjIz3qtmzZUjk5Oe46xwYOFecrznkroIMHAAD8ocx2qcz2TTuStGvXrkqZhxOZPHmyvvzyS73//vs178QpYLUFAAB+5nQ6PY4TBQ+pqalas2aN3n33XY+7tsbExKi4uLjSrdn37t2rmJgYd53jV19UPK6o4w2CBwAADLlk++zwlm3bSk1N1auvvqoNGzaoTZs2Hud79uypRo0aaf369e6ybdu2aefOne5bzvfq1UtffPGF9u3b566zbt06OZ1Ode7c2eu+MGwBAIAhl1xy+agdb02ePFkrV67Ua6+9pvDwcPcchYiICIWGhioiIkITJkzQ1KlTFRUVJafTqVtuuUW9evXS7373O0nSgAED1LlzZ/3pT3/SvHnzlJOTo3vvvVeTJ08+6VDJsQgeAAAIAIsXL5YkJSUleZQvXbpUY8eOlSQ9+uijCgoK0ogRI1RUVKSBAwfqySefdNdt0KCB1qxZo5tuukm9evVSkyZNlJKSotmzZxv1xbJt2wdTPvwjPz9fERERStIVrLYAgHqm1C7RRr2mvLw8j8mGtanie2fXN2f4bKlmXMf/1elr8AUyDwAAGDKdr3CidgIREyYBAIARMg8AABhyyVZZPc48EDwAAGCIYQsAAAADZB4AADBUZtsq88FiRV+04Q8EDwAAGHL9cviinUDEsAUAADBC5gEAAENlPlpt4Ys2/IHgAQAAQ2W2fHRL7pq34Q8MWwAAACNkHgAAMFTfJ0wSPAAAYMglS2WyfNJOIGLYAgAAGCHzAACAIZddfviinUBE8AAAgKEyHw1b+KINf2DYAgAAGCHzAACAofqeeSB4AADAkMu25LJ9sNrCB234A8MWAADACJkHAAAMMWwBAACMlClIZT5I3pf5oC/+wLAFAAAwQuYBAABDto8mTNoBOmGS4AEAAEP1fc4DwxYAAMAImQcAAAyV2UEqs30wYZJ7WwAAUD+4ZMnlg+S9S4EZPTBsAQAAjJB5AADAUH2fMEnwAACAId/NeWDYAgAA1ANkHgAAMFQ+YdIHd9Vk2AIAgPrB5aN7W7DaAgAA1AtkHgAAMFTfJ0wSPAAAYMilIDaJAgAAp7/Nmzdr6NChio2NlWVZWr16tcd5y7KqPB5++GF3nYSEhErnH3roIaN+kHkAAMBQmW2pzAe30zZto7CwUN26ddP48eM1fPjwSuf37Nnj8fhf//qXJkyYoBEjRniUz549WxMnTnQ/Dg8PN+oHwQMAAIbKfLTaosxw2GLw4MEaPHhwtedjYmI8Hr/22mtKTk5W27ZtPcrDw8Mr1TXBsAUAAH6Wn5/vcRQVFdW4zb1792rt2rWaMGFCpXMPPfSQmjVrpvPOO08PP/ywSktLjdom8wAAgCGXHSSXD1ZbuH5ZbREXF+dRnpaWpvT09Bq1vXz5coWHh1ca3rj11lvVo0cPRUVF6cMPP9TMmTO1Z88ezZ8/3+u2CR4AADDk62GLXbt2yel0ussdDkeN23722Wc1ZswYhYSEeJRPnTrV/fdzzz1XwcHBmjRpkubOnev1dQkeAADwM6fT6RE81NR7772nbdu26aWXXjpp3cTERJWWlur7779Xhw4dvGqf4AEAAEMuma+UqK6d2vDMM8+oZ8+e6tat20nrZmZmKigoSNHR0V63T/AAAIAh320SZdZGQUGBsrKy3I+zs7OVmZmpqKgotW7dWlL55MtVq1bpkUceqfT8jIwMffzxx0pOTlZ4eLgyMjI0ZcoUXXvttWratKnX/SB4AAAgQHz22WdKTk52P66Yv5CSkqJly5ZJkl588UXZtq3Ro0dXer7D4dCLL76o9PR0FRUVqU2bNpoyZYrHPAhvWLYdoBtrqzy6ioiIUJKuUEOrkb+7AwCoQ6V2iTbqNeXl5fl0vsCJVHzvPLElUaFhNf/9+0hBqVJ7flynr8EXyDwAAGDIJUsu+WLOQ83b8Ac2iQIAAEbIPAAAYMh3t+QOzN/hCR4AADDku02iAjN4CMxeAwAAvyHzAACAIZdtyeWLTaJ80IY/EDwAAGDI5aNhC19sNOUPgdlrAADgN2QeAAAw5Ltbcgfm7/AEDwAAGCqTpTIfbPDkizb8ITBDHgAA4DdkHgAAMMSwBQAAMFIm3ww5lNW8K34RmCEPAADwGzIPAAAYYtgCAAAYqe83xgrMXgMAAL8h8wAAgCFbllw+mDBpB+g+DwQPAAAYYtgCAADAAJkHAAAMcUtuAABgpMxHt+T2RRv+EJi9BgAAfkPmAQAAQwxbAAAAIy4FyeWD5L0v2vCHwOw1AADwGzIPAAAYKrMtlflgyMEXbfgDwQMAAIbq+5wHhi0AAIARMg8AABiyfXRLbjtAt6cmeAAAwFCZLJX54KZWvmjDHwIz5AEAAH5D5gEAAEMu2zeTHV22DzrjBwQPOGW77Cz9oG9VrKMKU4Q66DxFWFH+7hZQJ/j8128uH8158EUb/hCYvYbf5di79K0+V1t11oXqr3BF6t96T8X2UX93Dah1fP5R350WwcOiRYuUkJCgkJAQJSYm6pNPPvF3l3ASO/WtzlAbxVoJCrOc6qgeaqAG2q3v/d01oNbx+YdLls+OQOT34OGll17S1KlTlZaWpq1bt6pbt24aOHCg9u3b5++uoRou26VDylWUot1llmUpSi2VqwN+7BlQ+/j8Q/p1h0lfHCY2b96soUOHKjY2VpZlafXq1R7nx44dK8uyPI5BgwZ51Dl48KDGjBkjp9OpyMhITZgwQQUFBUb98HvwMH/+fE2cOFHjxo1T586dtWTJEjVu3FjPPvusv7uGapSoSLZsBSvEozxYDhWLtC1+2/j8w58KCwvVrVs3LVq0qNo6gwYN0p49e9zHCy+84HF+zJgx+uqrr7Ru3TqtWbNGmzdv1g033GDUD79OmCwuLtaWLVs0c+ZMd1lQUJD69++vjIyMSvWLiopUVFTkfpyfn18n/QQA4Fj+mjA5ePBgDR48+IR1HA6HYmJiqjz39ddf680339Snn36q888/X5L0+OOP67LLLtNf//pXxcbGetUPv2YefvrpJ5WVlally5Ye5S1btlROTk6l+nPnzlVERIT7iIuLq6uu4hiN5JAlq9JvWcUqqvTbGPBbw+cf0i9zHmwfHLUw52Hjxo2Kjo5Whw4ddNNNN+nAgV+H0zIyMhQZGekOHCSpf//+CgoK0scff+z1Nfw+bGFi5syZysvLcx+7du3yd5fqpSArSOGK1EH9Oi/Ftm0d1D5FqpkfewbUPj7/qA35+fkex7FZdhODBg3S3/72N61fv15/+ctftGnTJg0ePFhlZWWSpJycHEVHR3s8p2HDhoqKiqryl/bq+HXYonnz5mrQoIH27t3rUb53794qUy4Oh0MOh6OuuocTaK2z9V99KqfdVBGK0k5tV5lK1UoJ/u4aUOv4/MP20UoJ+5c2js+kp6WlKT093bi9UaNGuf/etWtXnXvuuWrXrp02btyofv361aivx/Jr8BAcHKyePXtq/fr1GjZsmCTJ5XJp/fr1Sk1N9WfXcBIxVpxK7CJ9p/+qSEcVrgidp4vksEjb4rePzz98fUvuXbt2yel0ust99Yty27Zt1bx5c2VlZalfv36KiYmptJqxtLRUBw8erHaeRFX8vsPk1KlTlZKSovPPP18XXnihFixYoMLCQo0bN87fXcNJxFntFaf2/u4G4Bd8/uFLTqfTI3jwlR9//FEHDhxQq1atJEm9evVSbm6utmzZop49e0qSNmzYIJfLpcTERK/b9XvwcPXVV2v//v26//77lZOTo+7du+vNN9+sNIkSAIDThb9WWxQUFCgrK8v9ODs7W5mZmYqKilJUVJRmzZqlESNGKCYmRjt27NCdd96p9u3ba+DAgZKkTp06adCgQZo4caKWLFmikpISpaamatSoUV6vtJBOg+BBklJTUxmmAAAEDF8PW3jrs88+U3Jysvvx1KlTJUkpKSlavHixPv/8cy1fvly5ubmKjY3VgAEDNGfOHI9hkBUrVig1NVX9+vVTUFCQRowYoccee8yoH6dF8AAAAE4uKSlJtl39rTjfeuutk7YRFRWllStX1qgfBA8AABjy1X0pAvXeFgQPAAAY8tewxekioDaJAgAA/kfmAQAAQ/U980DwAACAofoePDBsAQAAjJB5AADAUH3PPBA8AABgyJZvlllWv2PD6Y1hCwAAYITMAwAAhhi2AAAARup78MCwBQAAMELmAQAAQ/U980DwAACAofoePDBsAQAAjJB5AADAkG1bsn2QNfBFG/5A8AAAgCGXLJ9sEuWLNvyBYQsAAGCEzAMAAIbq+4RJggcAAAzV9zkPDFsAAAAjZB4AADDEsAUAADDCsAUAAIABMg8AABiyfTRsEaiZB4IHAAAM2ZJs2zftBCKGLQAAgBEyDwAAGHLJklWPt6cmeAAAwBCrLQAAAAyQeQAAwJDLtmSxSRQAAPCWbftotUWALrdg2AIAABgh8wAAgKH6PmGS4AEAAEP1PXhg2AIAABgh8wAAgKH6vtqCzAMAAIYqVlv44jCxefNmDR06VLGxsbIsS6tXr3afKykp0YwZM9S1a1c1adJEsbGxuu6667R7926PNhISEmRZlsfx0EMPGfWD4AEAgABRWFiobt26adGiRZXOHT58WFu3btV9992nrVu36pVXXtG2bdv0hz/8oVLd2bNna8+ePe7jlltuMeoHwxYAABgqzxr4YsKkWf3Bgwdr8ODBVZ6LiIjQunXrPMqeeOIJXXjhhdq5c6dat27tLg8PD1dMTIxxfyuQeQAAwFDFagtfHLUpLy9PlmUpMjLSo/yhhx5Ss2bNdN555+nhhx9WaWmpUbtkHgAA8LP8/HyPxw6HQw6Ho0ZtHj16VDNmzNDo0aPldDrd5bfeeqt69OihqKgoffjhh5o5c6b27Nmj+fPne902wQMAAIbsXw5ftCNJcXFxHuVpaWlKT08/5XZLSkp01VVXybZtLV682OPc1KlT3X8/99xzFRwcrEmTJmnu3LleBywEDwAAGPL1JlG7du3yyA7UJOtQETj88MMP2rBhg0e7VUlMTFRpaam+//57dejQwatrEDwAAOBnTqfzpF/y3qgIHLZv3653331XzZo1O+lzMjMzFRQUpOjoaK+vQ/AAAIApX49beKmgoEBZWVnux9nZ2crMzFRUVJRatWqlkSNHauvWrVqzZo3KysqUk5MjSYqKilJwcLAyMjL08ccfKzk5WeHh4crIyNCUKVN07bXXqmnTpl73g+ABAABTvlopYdjGZ599puTkZPfjivkLKSkpSk9P1+uvvy5J6t69u8fz3n33XSUlJcnhcOjFF19Uenq6ioqK1KZNG02ZMsVjHoQ3CB4AAAgQSUlJsk+wOcSJzklSjx499NFHH9W4HwQPAAAYOpWtpatrJxARPAAAYIhbcgMAABgg8wAAgCnbMp7sWG07AYjgAQAAQ/V9zgPDFgAAwAiZBwAATPlpk6jTBcEDAACGWG0BAABggMwDAACnIkCHHHyB4AEAAEMMWwAAABgg8wAAgKl6vtqCzAMAADBC5gEAAGPWL4cv2gk8BA8AAJhi2AIAAMB7ZB4AADBVzzMPBA8AAJiq57fkZtgCAAAYIfMAAIAh2y4/fNFOICJ4AADAVD2f88CwBQAAMELmAQAAU/V8wiTBAwAAhiy7/PBFO4GIYQsAAGCEzAMAAKaYMGnuvffe07XXXqtevXrpf//7nyTpueee0/vvv+/TzgEAcFqqmPPgiyMAGQcPL7/8sgYOHKjQ0FD9+9//VlFRkSQpLy9PDz74oM87CAAATi/GwcMDDzygJUuW6P/9v/+nRo0auct79+6trVu3+rRzAACclmwfHgHIeM7Dtm3b1KdPn0rlERERys3N9UWfAAA4vTHnwUxMTIyysrIqlb///vtq27atTzoFAABOX8bBw8SJE3Xbbbfp448/lmVZ2r17t1asWKFp06bppptuqo0+AgBwemHYwsxdd90ll8ulfv366fDhw+rTp48cDoemTZumW265pTb6CADA6YUdJs1YlqV77rlH06dPV1ZWlgoKCtS5c2eFhYXVRv8AAMBp5pQ3iQoODlbnzp192RcAAAJCfd+e2jh4SE5OlmVVn2bZsGFDjToEAMBpj9UWZrp3765u3bq5j86dO6u4uFhbt25V165da6OPAABA0ubNmzV06FDFxsbKsiytXr3a47xt27r//vvVqlUrhYaGqn///tq+fbtHnYMHD2rMmDFyOp2KjIzUhAkTVFBQYNQP48zDo48+WmV5enq68cUBAID3CgsL1a1bN40fP17Dhw+vdH7evHl67LHHtHz5crVp00b33XefBg4cqP/+978KCQmRJI0ZM0Z79uzRunXrVFJSonHjxumGG27QypUrve6HZdu2T5ImWVlZuvDCC3Xw4EFfNOeV/Px8RUREKElXqKHV6ORPAAD8ZpTaJdqo15SXlyen01kn16z43on/ywMK+uXLuCZcR4/qhxn3ntJrsCxLr776qoYNGyapPOsQGxurO+64Q9OmTZNUfuuIli1batmyZRo1apS+/vprde7cWZ9++qnOP/98SdKbb76pyy67TD/++KNiY2O9urbP7qqZkZHhjmrq2qvffiFnOHcXR/3T/oUb/d0FwG9cR49Kd7/m7274RH5+vsdjh8Mhh8Nh1EZ2drZycnLUv39/d1lERIQSExOVkZGhUaNGKSMjQ5GRke7AQZL69++voKAgffzxx7ryyiu9upZx8HB8msS2be3Zs0efffaZ7rvvPtPmAAAIPD7e5yEuLs6jOC0tTenp6UZN5eTkSJJatmzpUd6yZUv3uZycHEVHR3ucb9iwoaKiotx1vGEcPERERHg8DgoKUocOHTR79mwNGDDAtDkAAAKPj1db7Nq1y2PYwjTrUNeMgoeysjKNGzdOXbt2VdOmTWurTwAA1CtOp7PG8zZiYmIkSXv37lWrVq3c5Xv37lX37t3ddfbt2+fxvNLSUh08eND9fG8YTRRo0KCBBgwYwN0zAQD122l4b4s2bdooJiZG69evd5fl5+fr448/Vq9evSRJvXr1Um5urrZs2eKus2HDBrlcLiUmJnp9LeNhiy5duui7775TmzZtTJ8KAMBvgr92mCwoKPC4s3V2drYyMzMVFRWl1q1b6/bbb9cDDzygs846y71UMzY21r0io1OnTho0aJAmTpyoJUuWqKSkRKmpqRo1apTXKy2kUwgeHnjgAU2bNk1z5sxRz5491aRJE4/zdbVcBgCA+uazzz5TcnKy+/HUqVMlSSkpKVq2bJnuvPNOFRYW6oYbblBubq4uuugivfnmmx6rIVesWKHU1FT169dPQUFBGjFihB577DGjfni9z8Ps2bN1xx13KDw8/NcnH7NNtW3bsixLZWVlRh2oiYr1tj9/25almqiXWKqJ+sx19Kh+uPvU9kg4VRXfOwkP/Nln+zx8f+89dfoafMHrzMOsWbN044036t13363N/gAAcPqr5/e28Dp4qEhQ9O3bt9Y6AwAATn9Gcx5OdDdNAADqC27JbeDss88+aQBRl/e2AADAL3y8w2SgMQoeZs2aVWmHSQAAUL8YBQ+jRo2qtCc2AAD1DhMmvcN8BwAAytX3OQ9eb47g5XYQAADgN87rzIPL5arNfgAAEDgYtgAAAEZ8NGwRqMEDezoDAAAjZB4AADDFsAUAADBSz4MHhi0AAIARMg8AABhinwcAAAADBA8AAMAIwxYAAJiq5xMmCR4AADDEnAcAAAADZB4AADgVAZo18AWCBwAATNXzOQ8MWwAAACNkHgAAMFTfJ0wSPAAAYIphCwAAAO+ReQAAwBDDFgAAwAzDFgAAAN4j8wAAgKl6nnkgeAAAwFB9n/PAsAUAADBC5gEAAFMMWwAAACP1PHhg2AIAABgh8wAAgKH6PmGS4AEAAFMMWwAAgECQkJAgy7IqHZMnT5YkJSUlVTp34403+rwfZB4AADDkr2GLTz/9VGVlZe7HX375pS699FL98Y9/dJdNnDhRs2fPdj9u3Lhxjft5PIIHAABM+WnYokWLFh6PH3roIbVr1059+/Z1lzVu3FgxMTE+6Fz1GLYAAMDP8vPzPY6ioqKTPqe4uFjPP/+8xo8fL8uy3OUrVqxQ8+bN1aVLF82cOVOHDx/2eX/JPAAAYMrHmYe4uDiP4rS0NKWnp5/wqatXr1Zubq7Gjh3rLrvmmmsUHx+v2NhYff7555oxY4a2bdumV155xQed/RXBAwAAhqxfDl+0I0m7du2S0+l0lzscjpM+95lnntHgwYMVGxvrLrvhhhvcf+/atatatWqlfv36aceOHWrXrp0PelyO4AEAAD9zOp0ewcPJ/PDDD3rnnXdOmlFITEyUJGVlZRE8AADgV37e52Hp0qWKjo7WkCFDTlgvMzNTktSqVatTu1A1CB4AADDkzx0mXS6Xli5dqpSUFDVs+OvX+I4dO7Ry5UpddtllatasmT7//HNNmTJFffr00bnnnlvzzh6D4AEAgADyzjvvaOfOnRo/frxHeXBwsN555x0tWLBAhYWFiouL04gRI3Tvvff6vA8EDwAAmPLjsMWAAQNk25WfGBcXp02bNvmgUydH8AAAwKkI0PtS+AKbRAEAACNkHgAAMMQtuQEAgBluyQ0AAOA9Mg8AABhi2AIAAJhh2AIAAMB7ZB4AADDEsAUAADDDsAUAAID3yDwAAGCqnmceCB4AADBU3+c8MGwBAACMkHkAAMAUwxYAAMCEZduy7Jp/8/uiDX9g2AIAABgh8wAAgCmGLQAAgAlWWwAAABgg8wAAgCmGLQAAgAmGLQAAAAyQeQAAwBTDFgAAwATDFgAAAAbIPAAAYIphCwAAYCpQhxx8gWELAABghMwDAACmbLv88EU7AYjgAQAAQ6y2AAAAMEDmAQAAU6y2AAAAJixX+eGLdgIRwxYAAMAImQd4ajJJVsgAqUFbyS6SSrbKPvSwVJZ9TKVgWeEzpdAhkoKl4vdl56dJrgPuGlb4fVJwD6nh2VLpDtkH/lDnLwXwBdfRo/r5X2+p8Msv5DpUoOAzz1CzYVfI0bq1JKns0CEdXLNWR7Z9K9eRIwpp21bNhg9ToxYt/Nxz1Kp6Pmzh18zD5s2bNXToUMXGxsqyLK1evdqf3YEkK/hC2YdXyD74R9k/j5XUSFbUUskK/bWO8x4p5BLZubfKPjhGCoqWFbmoUlv2kX9IR9fWXeeBWvDT31fpyLffqsU1o3XG9GkKPfts7VnytEpz82TbtvY+u0wlBw6o5fixir1jiho2bao9S56Sq6jI311HLapYbeGLIxD5NXgoLCxUt27dtGhR5S8e+If98wTpyCtSaZZU+o3svBmyGpwhNexSXsEKk0JHys6fKxV/JJV+JTvvLlnBPaVG3X9t59Ac6fAKqWyXf14I4AOu4hIVfv6FooYOUWi7dmrUormaDhqoRs2bKf/DD1W6/ycV/fCDmo8cIUfr1gqOjlazkcNll5So8N+Z/u4+fmPS09NlWZbH0bFjR/f5o0ePavLkyWrWrJnCwsI0YsQI7d27t1b64tdhi8GDB2vw4MH+7AJOJiis/E87t/zPRl1kWcGyiz/4tU7Zd7LL/lcePJRk1nEHgVrkKpNcLlkNG3kUW40aqSg7W2Hdu5c/bvjrf6VWUJCshg11NDtb4b9LrMveoi75aZOoc845R++88477ccNjPntTpkzR2rVrtWrVKkVERCg1NVXDhw/XBx98UFVTNcKcB5yAJSv8XtnFn0ml28uLglrItosl+5Bn1bKfZAW1CNThO6BKQSEhciTEK3fdOjVqGa0G4eEq3PpvFX3/gxo1b15e1jRSP699Q83+OFJBwcHK27RZZbl5KsvP93f3UYv8tUlUw4YNFRMTU6k8Ly9PzzzzjFauXKlLLrlEkrR06VJ16tRJH330kX73u9/VvLPH9sOnrdWyoqIiFR0zjpjPD2etspzpUqOzZB8Y7e+uAH7T4prR+unFv2vXrDlSUJCCzzhDTc47T8U//iirQQO1HDtWP730d+28934pKEihZ52l0I4dFbAz4eAXx3+fORwOORyOSvW2b9+u2NhYhYSEqFevXpo7d65at26tLVu2qKSkRP3793fX7dixo1q3bq2MjIz6HTzMnTtXs2bN8nc36gUr/H7JkSz74DWSK+fXE6795cMWVrhn9qFBc9mu/XXfUaCWNWreXK1Sb5arqEiuoiI1dDq172/PqWGzKEmSI+5MnTFtqlxHjsguK1ODsDDtXrBQwXFxfu45apWPV1vEHfd5SUtLU3p6ukdZYmKili1bpg4dOmjPnj2aNWuWLr74Yn355ZfKyclRcHCwIiMjPZ7TsmVL5eTkyNcCKniYOXOmpk6d6n6cn59f6Q1HzVnh90shl8o+eK1U9qPnyZIvy4ctgn8vFb1VXtagjawGZ8hmvgN+w4IcDgU5HCo7fFhHvtmmpkMv9zwfWr4iqWT/fhXt+lGRgwf5o5uoI74etti1a5ecTqe7vKqsw7FzBM8991wlJiYqPj5ef//73xUaGlqpfm0KqOChujQOfMdypkshQ2X/fJNkF0pBzctPuA5JKpLsAunIP2Q5Z8rOy5VcBbKc98su3uo5WbJBa8lqIgW1kCyH1LBTeXlplqSSOn1NQE0c/mabZNtqFN1CpT8d0MF/rlGj6GiFX3iBJKkw8z8KCmuihk2bqnjPHh189TU17tJFjTt08HPPEUicTqdH8OCNyMhInX322crKytKll16q4uJi5ebmemQf9u7dW+UciZrya/BQUFCgrKws9+Ps7GxlZmYqKipKrX/ZgAV1y2o8pvzPZis8yl15M8qXcEqy8/8sK9wlK/IJeWwSdWw7EQ/KCv51prnV/PXydvYnSWX/q70XAPiY6+gR/bz2XyrNzVWDxo3V+NyuirpssKwGDSRJpfn5ynv9dZUdKlADZ7jCzz9fkZf2P0mrCHinwS25CwoKtGPHDv3pT39Sz5491ahRI61fv14jRoyQJG3btk07d+5Ur169at7P4/g1ePjss8+UnJzsflwxJJGSkqJly5b5qVf1myvnLC9qFcs+NEs6VP38E/vgtUwXw29CWPfu7iWZVYnoc7Ei+lxcdx3CacEfqy2mTZumoUOHKj4+Xrt371ZaWpoaNGig0aNHKyIiQhMmTNDUqVMVFRUlp9OpW265Rb169fL5ZEnJz8FDUlKSbF9EbgAA/Mb9+OOPGj16tA4cOKAWLVrooosu0kcffaQWv2yF/uijjyooKEgjRoxQUVGRBg4cqCeffLJW+hJQcx4AADgt+OHeFi+++OIJz4eEhGjRokV1smszwQMAAIb8tUnU6YJbcgMAACNkHgAAMOWyyw9ftBOACB4AADDlhzkPpxOGLQAAgBEyDwAAGLLkowmTNW/CLwgeAAAwdRrsMOlPDFsAAAAjZB4AADBU3/d5IHgAAMAUqy0AAAC8R+YBAABDlm3L8sFkR1+04Q8EDwAAmHL9cviinQDEsAUAADBC5gEAAEMMWwAAADOstgAAAPAemQcAAEzV8+2pCR4AADBU33eYZNgCAAAYIfMAAIAphi0AAIAJy1V++KKdQMSwBQAAMELmAQAAUwxbAAAAI2wSBQAA4D0yDwAAGOLeFgAAwEw9n/PAsAUAADBC5gEAAFO2JF/s0RCYiQeCBwAATNX3OQ8MWwAAACNkHgAAMGXLRxMma96EPxA8AABgitUWAAAA3iPzAACAKZcky0ftBCAyDwAAGKpYbeGLw1tz587VBRdcoPDwcEVHR2vYsGHatm2bR52kpCRZluVx3Hjjjb5++QQPAAAEgk2bNmny5Mn66KOPtG7dOpWUlGjAgAEqLCz0qDdx4kTt2bPHfcybN8/nfWHYAgAAU36YMPnmm296PF62bJmio6O1ZcsW9enTx13euHFjxcTE1LxvJ0DmAQAAUxXBgy+OU5SXlydJioqK8ihfsWKFmjdvri5dumjmzJk6fPhwjV5qVcg8AADgZ/n5+R6PHQ6HHA5HtfVdLpduv/129e7dW126dHGXX3PNNYqPj1dsbKw+//xzzZgxQ9u2bdMrr7zi0/4SPAAAYMrHwxZxcXEexWlpaUpPT6/2aZMnT9aXX36p999/36P8hhtucP+9a9euatWqlfr166cdO3aoXbt2Ne/vLwgeAAAw5eOlmrt27ZLT6XQXnyjrkJqaqjVr1mjz5s0688wzT9h8YmKiJCkrK4vgAQCA3xKn0+kRPFTFtm3dcsstevXVV7Vx40a1adPmpO1mZmZKklq1auWLbroRPAAAYMgfd9WcPHmyVq5cqddee03h4eHKycmRJEVERCg0NFQ7duzQypUrddlll6lZs2b6/PPPNWXKFPXp00fnnntujft6LIIHAABM+WGp5uLFiyWVbwR1rKVLl2rs2LEKDg7WO++8owULFqiwsFBxcXEaMWKE7r333pr38zgEDwAABAD7JIFGXFycNm3aVCd9IXgAAMCUy5YsH2QeXIF5V02CBwAATHFLbgAAAO+ReQAAwJiPMg8KzMwDwQMAAKYYtgAAAPAemQcAAEy5bPlkyIHVFgAA1BO2q/zwRTsBiGELAABghMwDAACm6vmESYIHAABM1fM5DwxbAAAAI2QeAAAwxbAFAAAwYstHwUPNm/AHhi0AAIARMg8AAJhi2AIAABhxuST5YIMnF5tEAQCAeoDMAwAAphi2AAAARup58MCwBQAAMELmAQAAU/V8e2qCBwAADNm2S7YPbqftizb8gWELAABghMwDAACmbNs3Qw4BOmGS4AEAAFO2j+Y8BGjwwLAFAAAwQuYBAABTLpdk+WCyY4BOmCR4AADAFMMWAAAA3iPzAACAIdvlku2DYYtA3eeB4AEAAFMMWwAAAHiPzAMAAKZctmTV38wDwQMAAKZsW5IvlmoGZvDAsAUAADBC5gEAAEO2y5btg2ELm8wDAAD1hO3y3WFo0aJFSkhIUEhIiBITE/XJJ5/Uwgs8MYIHAAACxEsvvaSpU6cqLS1NW7duVbdu3TRw4EDt27evTvtB8AAAgCHbZfvsMDF//nxNnDhR48aNU+fOnbVkyRI1btxYzz77bC290qoRPAAAYMoPwxbFxcXasmWL+vfv7y4LCgpS//79lZGRURuvsloBPWGyYqJJfkFgbu8J1JTr6FF/dwHwm4rPvz8mHZaqxCcbTJaqRJKUn5/vUe5wOORwODzKfvrpJ5WVlally5Ye5S1bttQ333xT884YCOjg4dChQ5Kk+B7f+7cjgN/c6+8OAH536NAhRURE1Mm1goODFRMTo/dz3vBZm2FhYYqLi/MoS0tLU3p6us+u4WsBHTzExsZq165dCg8Pl2VZ/u5OvZOfn6+4uDjt2rVLTqfT390B6hSff/+zbVuHDh1SbGxsnV0zJCRE2dnZKi4u9lmbtm1X+g47PusgSc2bN1eDBg20d+9ej/K9e/cqJibGZ/3xRkAHD0FBQTrzzDP93Y16z+l08p8n6i0+//5VVxmHY4WEhCgkJKTOrxscHKyePXtq/fr1GjZsmCTJ5XJp/fr1Sk1NrdO+BHTwAABAfTJ16lSlpKTo/PPP14UXXqgFCxaosLBQ48aNq9N+EDwAABAgrr76au3fv1/333+/cnJy1L17d7355puVJlHWNoIHnDKHw6G0tLQqx+aA3zo+//CX1NTUOh+mOJ5lB+rG2gAAwC/YJAoAABgheAAAAEYIHgAAgBGCB5yy0+G2sIA/bN68WUOHDlVsbKwsy9Lq1av93SWgThE84JScLreFBfyhsLBQ3bp106JFi/zdFcAvWG2BU5KYmKgLLrhATzzxhKTyXc7i4uJ0yy236K677vJz74C6Y1mWXn31VfeOf0B9QOYBxk6n28ICAOoewQOMnei2sDk5OX7qFQCgrhA8AAAAIwQPMHY63RYWAFD3CB5g7NjbwlaouC1sr169/NgzAEBd4MZYOCWny21hAX8oKChQVlaW+3F2drYyMzMVFRWl1q1b+7FnQN1gqSZO2RNPPKGHH37YfVvYxx57TImJif7uFlDrNm7cqOTk5ErlKSkpWrZsWd13CKhjBA8AAMAIcx4AAIARggcAAGCE4AEAABgheAAAAEYIHgAAgBGCBwAAYITgAQAAGCF4AAAARggegAAxduxYDRs2zP04KSlJt99+e533Y+PGjbIsS7m5uXV+bQCnB4IHoIbGjh0ry7JkWZaCg4PVvn17zZ49W6WlpbV63VdeeUVz5szxqi5f+AB8iRtjAT4waNAgLV26VEVFRXrjjTc0efJkNWrUSDNnzvSoV1xcrODgYJ9cMyoqyiftAIApMg+ADzgcDsXExCg+Pl433XST+vfvr9dff9091PDnP/9ZsbGx6tChgyRp165duuqqqxQZGamoqChdccUV+v77793tlZWVaerUqYqMjFSzZs1055136vjb0Bw/bFFUVKQZM2YoLi5ODodD7du31zPPPKPvv//efROnpk2byrIsjR07VlL5rdTnzp2rNm3aKDQ0VN26ddM//vEPj+u88cYbOvvssxUaGqrk5GSPfgKonwgegFoQGhqq4uJiSdL69eu1bds2rVu3TmvWrFFJSYkGDhyo8PBwvffee/rggw8UFhamQYMGuZ/zyCOPaNmyZXr22Wf1/vvv6+DBg3r11VdPeM3rrrtOL7zwgh577DF9/fXXeuqppxQWFqa4uDi9/PLLkqRt27Zpz549WrhwoSRp7ty5+tvf/qYlS5boq6++0pQpU3Tttddq06ZNksqDnOHDh2vo0KHKzMzU9ddfr7vuuqu23jYAgcIGUCMpKSn2FVdcYdu2bbtcLnvdunW2w+Gwp02bZqekpNgtW7a0i4qK3PWfe+45u0OHDrbL5XKXFRUV2aGhofZbb71l27Ztt2rVyp43b577fElJiX3mmWe6r2Pbtt23b1/7tttus23btrdt22ZLstetW1dlH999911bkv3zzz+7y44ePWo3btzY/vDDDz3qTpgwwR49erRt27Y9c+ZMu3Pnzh7nZ8yYUaktAPULcx4AH1izZo3CwsJUUlIil8ula665Runp6Zo8ebK6du3qMc/hP//5j7KyshQeHu7RxtGjR7Vjxw7l5eVpz549SkxMdJ9r2LChzj///EpDFxUyMzPVoEED9e3b1+s+Z2Vl6fDhw7r00ks9youLi3XeeedJkr7++muPfkhSr169vL4GgN8mggfAB5KTk7V48WIFBwcrNjZWDRv++qPVpEkTj7oFBQXq2bOnVqxYUamdFi1anNL1Q0NDjZ9TUFAgSVq7dq3OOOMMj3MOh+OU+gGgfiB4AHygSZMmat++vVd1e/TooZdeeknR0dFyOp1V1mnVqpU+/vhj9enTR5JUWlqqLVu2qEePHlXW79q1q1wulzZt2qT+/ftXOl+R+SgrK3OXde7cWQ6HQzt37qw2Y9GpUye9/vrrHmUfffTRyV8kgN80JkwCdWzMmDFq3ry5rrjiCr333nvKzs7Wxo0bdeutt+rHH3+UJN1222166KGHtHr1an3zzTe6+eabT7hHQ0JCglJSUjR+/HitXr3a3ebf//53SVJ8fLwsy9KaNWu0f/9+FRQUKDw8XNOmTdOUKVO0fPly7dixQ1u3btXjjz+u5cuXS5JuvPFGbd++XdOnT9e2bdu0cuVKLVu2rLbfIgCnOYIHoI41btxYmzdvVuvWrTV8+HB16tRJEyZM0NGjR92ZiDvuuEN/+tOflJKSol69eik8PFxXXnnlCdtdvHixRo4cqZtvvlkdO3bUxIkTVVhYKEk644wzNGvWLN11111q2bKlUlNTJUlz5szRfffdp7lz56pTp04aNGiQ1q5dqzZt2kiSWrdurZdfflmrV69Wt27dtGTJEj344IO1+O4ACASWXd0MLAAAgCqQeQAAAEYIHgAAgBGCBwAAYITgAQAAGCF4AAAARggeAACAEYIHAABghOABAAAYIXgAAABGCB4AAIARggcAAGCE4AEAABj5/14oce0B3m1vAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Final model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================\n",
    "# Cell 6: Confusion Matrix & Metrics on TEST (FINAL EEGNet)\n",
    "# =======================================================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# ------------------------------------------------------------------\n",
    "def compute_metrics_from_cm(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    eps = 1e-12\n",
    "    acc  = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec  = tp / (tp + fn + eps)\n",
    "    f1   = 2 * prec * rec / (prec + rec + eps)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], ['0', '1'])\n",
    "    plt.yticks([0, 1], ['0', '1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(\n",
    "                j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Validate upstream artifacts\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[CHECK] Verifying predictions and labels are available...\")\n",
    "\n",
    "if 'y_pred' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Predictions 'y_pred' not found. \"\n",
    "        \"Please run Cell 5 (final model inference) first.\"\n",
    "    )\n",
    "\n",
    "if 'yte' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Test labels 'yte' not found. \"\n",
    "        \"Please run Cell 5 first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[CHECK] Found predictions for {len(y_pred)} test segments.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Confusion Matrix & Metrics (FINAL model)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc, prec, rec, f1 = compute_metrics_from_cm(cm)\n",
    "\n",
    "print(\"\\n[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(cm, title=\"Confusion Matrix - FINAL EEGNet\")\n",
    "\n",
    "print(\"[DONE] Final model evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0f120-39fd-497e-9098-05e6c82f4807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
