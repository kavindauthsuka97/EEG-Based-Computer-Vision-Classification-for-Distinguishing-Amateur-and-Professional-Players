{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af5b471-cea9-4194-bc23-da5cdc2f9c67",
   "metadata": {},
   "source": [
    "Cell 1 — Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64aaa897-ab61-4eb4-a558-4c078acf751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Base path set to:\n",
      "C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\n",
      "[INFO] Found 18 folders:\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Selected test subject (LOSO): Mohammad\n",
      "[INFO] Training subjects (17):\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Sampling rate (SFREQ) = 128.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 1: Setup & discovery\n",
    "# =========================\n",
    "\n",
    "import os  # file ops\n",
    "import glob  # file matching\n",
    "import numpy as np  # arrays\n",
    "import random  # seeds\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data path\n",
    "BASE_PATH = r\"C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\"\n",
    "print(f\"[INFO] Base path set to:\\n{BASE_PATH}\")\n",
    "\n",
    "# List subject folders\n",
    "all_folders = [p for p in sorted(os.listdir(BASE_PATH)) if os.path.isdir(os.path.join(BASE_PATH, p))]\n",
    "print(f\"[INFO] Found {len(all_folders)} folders:\\n{all_folders}\")\n",
    "\n",
    "# Expected count (change if needed)\n",
    "assert len(all_folders) == 18, f\"Expected 18 folders, found {len(all_folders)}.\"\n",
    "\n",
    "# LOSO test subject\n",
    "TEST_SUBJECT = \"Mohammad\"\n",
    "print(f\"[INFO] Selected test subject (LOSO): {TEST_SUBJECT}\")\n",
    "\n",
    "# Train subjects = remaining\n",
    "train_subjects = [s for s in all_folders if s.lower() != TEST_SUBJECT.lower()]\n",
    "print(f\"[INFO] Training subjects ({len(train_subjects)}):\\n{train_subjects}\")\n",
    "\n",
    "# Sampling rate\n",
    "SFREQ = 128.0\n",
    "print(f\"[INFO] Sampling rate (SFREQ) = {SFREQ} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e649681-2861-4ce2-bbd1-0d9dd3359f42",
   "metadata": {},
   "source": [
    "Cell 2 — Load EEG + labels per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943c5c6e-a7aa-4618-899c-daf341c25358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 2] Loading data from folders...\n",
      "[STEP 2]       Amin | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Amin | EEG=          Amin.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Amin1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]      Amin1 | EEG=         Amin1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Cole | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Cole | EEG=          Cole.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Daniel | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]     Daniel | EEG=        Daniel.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]    Ismayil | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]    Ismayil | EEG=       Ismayil.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Jack | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Jack | EEG=          Jack.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]      James | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      James | EEG=         James.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Josh | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Josh | EEG=          Josh.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Marjan | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Marjan | EEG=        Marjan.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]        Max | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Max | EEG=           Max.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Mina | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mina | EEG=          Mina.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 1 | EEG=        Mina 1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 3 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 3 | EEG=        Mina 3.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]   Mohammad | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]   Mohammad | EEG=      Mohammad.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Mona | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mona | EEG=          Mona.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Roddy | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      Roddy | EEG=         Roddy.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]        Sam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Sam | EEG=           Sam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       adam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       adam | EEG=          adam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2] Loading complete ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 2: Load data (EMPTY LABEL => CLASS 0)\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path  # helper\n",
    "\n",
    "def load_subject(subj_dir: str, subj_name: str, expected_channels: int = 14):\n",
    "    \"\"\"\n",
    "    EEG file: {subj_name}.npy\n",
    "    Label file: {subj_name}_target.npy\n",
    "    Label rule:\n",
    "      - empty array -> class 0\n",
    "      - [0] -> class 0\n",
    "      - [1] -> class 1\n",
    "    \"\"\"\n",
    "\n",
    "    # EEG file path\n",
    "    eeg_file = os.path.join(subj_dir, f\"{subj_name}.npy\")\n",
    "    assert os.path.exists(eeg_file), f\"[ERROR] Missing EEG file: {eeg_file}\"\n",
    "\n",
    "    # Label file path\n",
    "    target_file = os.path.join(subj_dir, f\"{subj_name}_target.npy\")\n",
    "    assert os.path.exists(target_file), f\"[ERROR] Missing label file: {target_file}\"\n",
    "\n",
    "    # Load EEG\n",
    "    X_raw = np.load(eeg_file, allow_pickle=True)\n",
    "    X_raw = np.asarray(X_raw)\n",
    "\n",
    "    # Standardize EEG to (n_trials, 14, T)\n",
    "    if X_raw.ndim == 2:\n",
    "        X_trials = X_raw[None, :, :]\n",
    "    elif X_raw.ndim == 3:\n",
    "        dims = list(X_raw.shape)\n",
    "        chan_axes = [i for i, d in enumerate(dims) if d == expected_channels]\n",
    "        chan_axis = chan_axes[0] if len(chan_axes) == 1 else 1\n",
    "        time_axis = int(np.argmax(dims))\n",
    "        trial_axis = [i for i in range(3) if i not in (chan_axis, time_axis)][0]\n",
    "        X_trials = np.moveaxis(X_raw, (trial_axis, chan_axis, time_axis), (0, 1, 2))\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] Unsupported EEG shape {X_raw.shape} in {eeg_file}\")\n",
    "\n",
    "    assert X_trials.shape[1] == expected_channels, f\"[ERROR] Expected {expected_channels} channels, got {X_trials.shape[1]} in {eeg_file}\"\n",
    "\n",
    "    # Load label\n",
    "    y_raw = np.load(target_file, allow_pickle=True)\n",
    "    y_arr = np.asarray(y_raw).ravel()\n",
    "\n",
    "    # Apply your rule\n",
    "    if y_arr.size == 0:\n",
    "        label = 0\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\")\n",
    "    else:\n",
    "        label = int(y_arr[0])\n",
    "        if label not in (0, 1):\n",
    "            raise ValueError(f\"[ERROR] Invalid label {label} in {target_file} (must be 0 or 1)\")\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape={y_arr.shape}, value={y_arr} -> class={label}\")\n",
    "\n",
    "    # Repeat label for all trials\n",
    "    y_trials = np.full((X_trials.shape[0],), label, dtype=np.int32)\n",
    "\n",
    "    return X_trials.astype(np.float32), y_trials.astype(np.int32), eeg_file, target_file\n",
    "\n",
    "\n",
    "# Load all subjects\n",
    "subject_data = {}\n",
    "print(\"[STEP 2] Loading data from folders...\")\n",
    "\n",
    "for subj in all_folders:\n",
    "    subj_dir = os.path.join(BASE_PATH, subj)\n",
    "    X_trials, y_trials, eeg_file, target_file = load_subject(subj_dir, subj)\n",
    "\n",
    "    subject_data[subj] = (X_trials, y_trials)\n",
    "\n",
    "    print(f\"[STEP 2] {subj:>10} | EEG={Path(eeg_file).name:>18} | X={X_trials.shape} | y={y_trials.shape} | class={y_trials[0]}\")\n",
    "\n",
    "print(\"[STEP 2] Loading complete ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd93ca8-a94c-485c-8a61-e6e7441f6299",
   "metadata": {},
   "source": [
    "Cell 3 — Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733e592c-98e1-4cbd-afd2-6fdc8cf407f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 3] Preprocessing functions ready ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3: Preprocessing functions\n",
    "# =========================\n",
    "\n",
    "import mne  # EEG processing\n",
    "import pywt  # wavelets\n",
    "from sklearn.decomposition import FastICA  # ICA\n",
    "from typing import Optional, Tuple, Union, Sequence, Dict, List  # typing\n",
    "\n",
    "def wavelet_enhanced_ica(\n",
    "    data: np.ndarray,\n",
    "    n_components: int = 10,\n",
    "    wavelet: str = \"db4\",\n",
    "    level: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> np.ndarray:\n",
    "    n_ch, n_t = data.shape\n",
    "    n_components = min(n_components, n_ch)\n",
    "\n",
    "    coeffs = pywt.wavedec(data, wavelet=wavelet, level=level, axis=1)\n",
    "    A = coeffs[0]\n",
    "\n",
    "    ica = FastICA(n_components=n_components, random_state=random_state)\n",
    "    S = ica.fit_transform(A.T).T\n",
    "    A_denoised = ica.inverse_transform(S.T).T\n",
    "\n",
    "    coeffs[0] = A_denoised\n",
    "    cleaned = pywt.waverec(coeffs, wavelet=wavelet, axis=1)\n",
    "\n",
    "    if cleaned.shape[1] != n_t:\n",
    "        if cleaned.shape[1] > n_t:\n",
    "            cleaned = cleaned[:, :n_t]\n",
    "        else:\n",
    "            cleaned = np.pad(cleaned, ((0, 0), (0, n_t - cleaned.shape[1])), mode=\"constant\")\n",
    "    return cleaned\n",
    "\n",
    "def _names_from_index_mapping(n_channels: int, index_to_name: Optional[Dict[int, str]]) -> List[str]:\n",
    "    if index_to_name is None:\n",
    "        return [f\"EEG{i+1}\" for i in range(n_channels)]\n",
    "    keys = list(index_to_name.keys())\n",
    "    is_zero_based = (0 in keys) and (1 not in keys)\n",
    "    names = []\n",
    "    for i in range(n_channels):\n",
    "        key = i if is_zero_based else (i + 1)\n",
    "        names.append(index_to_name.get(key, f\"EEG{i+1}\"))\n",
    "    return names\n",
    "\n",
    "def preprocess_eeg(\n",
    "    eeg: np.ndarray,\n",
    "    sfreq: float,\n",
    "    *,\n",
    "    index_to_name: Optional[Dict[int, str]] = None,\n",
    "    use_standard_1010: bool = True,\n",
    "    resample_to: Optional[float] = None,\n",
    "    notch_freqs: Union[None, float, Sequence[float]] = 50.0,\n",
    "    highpass: Optional[float] = 0.05,\n",
    "    bad_point_z: float = 6.0,\n",
    "    bad_channel_z: float = 5.0,\n",
    "    interpolate_bad_channels: bool = True,\n",
    "    car: bool = True,\n",
    "    use_wica: bool = True,\n",
    "    wica_components: int = 10,\n",
    "    wica_wavelet: str = \"db4\",\n",
    "    wica_level: int = 3,\n",
    "    wica_random_state: int = 42,\n",
    "    return_raw: bool = False,\n",
    "):\n",
    "\n",
    "    eeg = np.asarray(eeg, dtype=np.float32)\n",
    "    assert eeg.ndim == 2, \"eeg must be 2D: (n_channels, n_times)\"\n",
    "\n",
    "    n_channels, _ = eeg.shape\n",
    "    ch_names = _names_from_index_mapping(n_channels, index_to_name)\n",
    "    ch_types = ['eog' if str(n).upper().startswith(\"EOG\") else 'eeg' for n in ch_names]\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    raw = mne.io.RawArray(eeg, info, verbose=False)\n",
    "\n",
    "    montage_applied = False\n",
    "    if use_standard_1010:\n",
    "        try:\n",
    "            mont = mne.channels.make_standard_montage(\"standard_1010\")\n",
    "            raw.set_montage(mont, match_case=False, on_missing=\"ignore\")\n",
    "            montage_applied = True\n",
    "        except Exception:\n",
    "            montage_applied = False\n",
    "\n",
    "    if resample_to is not None and float(resample_to) != float(sfreq):\n",
    "        raw.resample(sfreq=resample_to, npad=\"auto\")\n",
    "    sfreq_out = float(raw.info[\"sfreq\"])\n",
    "\n",
    "    if notch_freqs is not None:\n",
    "        raw.notch_filter(freqs=notch_freqs, verbose=False)\n",
    "    if highpass is not None:\n",
    "        raw.filter(l_freq=highpass, h_freq=None, verbose=False)\n",
    "\n",
    "    X = raw.get_data()\n",
    "    mu = np.mean(X, axis=1, keepdims=True)\n",
    "    sd = np.std(X, axis=1, keepdims=True) + 1e-12\n",
    "    hi = mu + bad_point_z * sd\n",
    "    lo = mu - bad_point_z * sd\n",
    "    bad_idx = (X > hi) | (X < lo)\n",
    "\n",
    "    if np.any(bad_idx):\n",
    "        X_fixed = X.copy()\n",
    "        t = np.arange(X.shape[1], dtype=float)\n",
    "        for ch in range(n_channels):\n",
    "            mask = bad_idx[ch]\n",
    "            if mask.any():\n",
    "                good = ~mask\n",
    "                if good.sum() >= 2:\n",
    "                    X_fixed[ch, mask] = np.interp(t[mask], t[good], X_fixed[ch, good])\n",
    "        raw._data = X_fixed\n",
    "\n",
    "    if interpolate_bad_channels and montage_applied:\n",
    "        X = raw.get_data(picks=\"eeg\")\n",
    "        if X.size > 0:\n",
    "            ch_std = X.std(axis=1)\n",
    "            med = np.median(ch_std)\n",
    "            mad = np.median(np.abs(ch_std - med)) + 1e-12\n",
    "            z = 0.6745 * (ch_std - med) / mad\n",
    "            eeg_names = mne.pick_info(raw.info, mne.pick_types(raw.info, eeg=True)).ch_names\n",
    "            bads = [eeg_names[i] for i in np.where(np.abs(z) > bad_channel_z)[0]]\n",
    "            raw.info[\"bads\"] = bads\n",
    "            if bads:\n",
    "                raw.interpolate_bads(reset_bads=True, verbose=False)\n",
    "\n",
    "    if car:\n",
    "        raw.set_eeg_reference(\"average\", projection=True)\n",
    "        raw.apply_proj()\n",
    "\n",
    "    if use_wica:\n",
    "        cleaned = wavelet_enhanced_ica(\n",
    "            raw.get_data(),\n",
    "            n_components=wica_components,\n",
    "            wavelet=wica_wavelet,\n",
    "            level=wica_level,\n",
    "            random_state=wica_random_state\n",
    "        )\n",
    "        raw = mne.io.RawArray(cleaned, raw.info, verbose=False)\n",
    "\n",
    "    cleaned = raw.get_data()\n",
    "    return cleaned, sfreq_out\n",
    "\n",
    "print(\"[STEP 3] Preprocessing functions ready ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a9d20-889d-45c4-bbc2-de62216908cd",
   "metadata": {},
   "source": [
    "Cell 4 — Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bfcbde0-1cc2-409c-8627-93da46606661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 4] Starting preprocessing...\n",
      "[INFO] USE_WICA = True\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 01/17: Amin | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 02/17: Amin1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 03/17: Cole | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 04/17: Daniel | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 05/17: Ismayil | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 06/17: Jack | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 07/17: James | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 08/17: Josh | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 09/17: Marjan | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 10/17: Max | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 11/17: Mina | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 12/17: Mina 1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 13/17: Mina 3 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 14/17: Mona | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 15/17: Roddy | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 16/17: Sam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 17/17: adam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Test subject: Mohammad | trials=1 | fs_out=128.0\n",
      "[STEP 4] Preprocessing done ✅\n",
      "  X_train_trials_all: (17, 14, 38400)\n",
      "  y_train_trials_all: (17,) (array([0, 1], dtype=int32), array([8, 9]))\n",
      "  X_test_trials_all : (1, 14, 38400)\n",
      "  y_test_trials_all : (1,) (array([0], dtype=int32), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 4: Apply preprocessing (LOSO)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 4] Starting preprocessing...\")\n",
    "\n",
    "USE_WICA = True\n",
    "print(f\"[INFO] USE_WICA = {USE_WICA}\")\n",
    "\n",
    "X_train_trials_all, y_train_trials_all = [], []\n",
    "X_test_trials_all, y_test_trials_all = [], []\n",
    "\n",
    "def preprocess_trials(X_trials, y_trials):\n",
    "    cleaned_list = []\n",
    "    label_list = []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        trial = X_trials[i]\n",
    "        label = int(y_trials[i])\n",
    "\n",
    "        cleaned, fs_out = preprocess_eeg(\n",
    "            eeg=trial,\n",
    "            sfreq=SFREQ,\n",
    "            use_wica=USE_WICA\n",
    "        )\n",
    "\n",
    "        cleaned_list.append(cleaned.astype(np.float32, copy=False))\n",
    "        label_list.append(label)\n",
    "\n",
    "    return cleaned_list, np.array(label_list, dtype=np.int32), fs_out\n",
    "\n",
    "# Train subjects\n",
    "for idx, subj in enumerate(train_subjects, start=1):\n",
    "    X_trials, y_trials = subject_data[subj]\n",
    "    cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "    X_train_trials_all.extend(cleaned_trials)\n",
    "    y_train_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "    print(f\"[STEP 4] Train {idx:02d}/{len(train_subjects)}: {subj} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Test subject\n",
    "X_trials, y_trials = subject_data[TEST_SUBJECT]\n",
    "cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "X_test_trials_all.extend(cleaned_trials)\n",
    "y_test_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "print(f\"[STEP 4] Test subject: {TEST_SUBJECT} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Convert to arrays\n",
    "X_train_trials_all = np.array(X_train_trials_all, dtype=np.float32)\n",
    "y_train_trials_all = np.array(y_train_trials_all, dtype=np.int32)\n",
    "\n",
    "X_test_trials_all = np.array(X_test_trials_all, dtype=np.float32)\n",
    "y_test_trials_all = np.array(y_test_trials_all, dtype=np.int32)\n",
    "\n",
    "print(\"[STEP 4] Preprocessing done ✅\")\n",
    "print(\"  X_train_trials_all:\", X_train_trials_all.shape)\n",
    "print(\"  y_train_trials_all:\", y_train_trials_all.shape, np.unique(y_train_trials_all, return_counts=True))\n",
    "print(\"  X_test_trials_all :\", X_test_trials_all.shape)\n",
    "print(\"  y_test_trials_all :\", y_test_trials_all.shape, np.unique(y_test_trials_all, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc3da1-2967-4ee3-84cd-bfcf0f98c0b1",
   "metadata": {},
   "source": [
    "Cell 5 — Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c21ad54-7001-4039-9360-29dfa2905127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 5] Starting segmentation...\n",
      "[STEP 5] Segmented 5/17 trials...\n",
      "[STEP 5] Segmented 10/17 trials...\n",
      "[STEP 5] Segmented 15/17 trials...\n",
      "[STEP 5] Segmented 17/17 trials...\n",
      "[STEP 5] Segmented 1/1 trials...\n",
      "[STEP 5] Segmentation done ✅\n",
      "  X_train: (5100, 14, 128, 1)\n",
      "  y_train: (5100,) (array([0, 1], dtype=int32), array([2400, 2700]))\n",
      "  X_test : (300, 14, 128, 1)\n",
      "  y_test : (300,) (array([0], dtype=int32), array([300]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 5: Segmentation\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 5] Starting segmentation...\")\n",
    "\n",
    "def segment_trial(trial_2d: np.ndarray, label: int, segment_len: int = 128):\n",
    "    C, T = trial_2d.shape\n",
    "    n_segs = T // segment_len\n",
    "    use_T = n_segs * segment_len\n",
    "\n",
    "    if n_segs == 0:\n",
    "        return np.empty((0, C, segment_len), dtype=np.float32), np.empty((0,), dtype=np.int32)\n",
    "\n",
    "    trial_trim = trial_2d[:, :use_T]\n",
    "    Xsegs = trial_trim.reshape(C, n_segs, segment_len).transpose(1, 0, 2)\n",
    "    ysegs = np.full((n_segs,), int(label), dtype=np.int32)\n",
    "    return Xsegs.astype(np.float32, copy=False), ysegs\n",
    "\n",
    "def segment_dataset(X_trials: np.ndarray, y_trials: np.ndarray, segment_len: int = 128):\n",
    "    X_all, y_all = [], []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        Xsegs, ysegs = segment_trial(X_trials[i], int(y_trials[i]), segment_len=segment_len)\n",
    "        X_all.append(Xsegs)\n",
    "        y_all.append(ysegs)\n",
    "\n",
    "        if (i + 1) % 5 == 0 or (i + 1) == X_trials.shape[0]:\n",
    "            print(f\"[STEP 5] Segmented {i+1}/{X_trials.shape[0]} trials...\")\n",
    "\n",
    "    X_out = np.concatenate(X_all, axis=0)\n",
    "    y_out = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    X_out = np.expand_dims(X_out, axis=-1)  # (N,14,128,1)\n",
    "    return X_out.astype(np.float32), y_out.astype(np.int32)\n",
    "\n",
    "X_train, y_train = segment_dataset(X_train_trials_all, y_train_trials_all, segment_len=128)\n",
    "X_test,  y_test  = segment_dataset(X_test_trials_all,  y_test_trials_all,  segment_len=128)\n",
    "\n",
    "print(\"[STEP 5] Segmentation done ✅\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  y_train:\", y_train.shape, np.unique(y_train, return_counts=True))\n",
    "print(\"  X_test :\", X_test.shape)\n",
    "print(\"  y_test :\", y_test.shape, np.unique(y_test, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b4ffd5-bf2f-490e-8854-0a2fc747f1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e69ba7-3952-47b1-bd80-f05237b2e018",
   "metadata": {},
   "source": [
    "Cell 6 — Split train/val + Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97fda4ae-e1c8-4fd1-bf0c-c82ae2a85764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 6] Normalizing using ALL training data (no train/val split)...\n",
      "[STEP 6] Normalization done ✅\n",
      "  X_train_norm: (5100, 14, 128, 1)\n",
      "  X_test_norm : (300, 14, 128, 1)\n",
      "  y_train_norm: (5100,)\n",
      "  y_test_norm : (300,)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 6: Normalize (ALL training data)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 6] Normalizing using ALL training data (no train/val split)...\")\n",
    "\n",
    "def normalize_eeg(train_data: np.ndarray, test_data: np.ndarray):\n",
    "    if train_data.ndim == 5:\n",
    "        train_flat = train_data.reshape(-1, *train_data.shape[2:])\n",
    "        test_flat  = test_data.reshape(-1, *test_data.shape[2:])\n",
    "    else:\n",
    "        train_flat, test_flat = train_data, test_data\n",
    "\n",
    "    # Compute stats on ALL training data\n",
    "    train_mean = np.mean(train_flat, axis=(0, 2, 3), keepdims=True)\n",
    "    train_std  = np.std(train_flat,  axis=(0, 2, 3), keepdims=True) + 1e-8\n",
    "\n",
    "    train_norm = (train_flat - train_mean) / train_std\n",
    "    test_norm  = (test_flat  - train_mean) / train_std\n",
    "\n",
    "    if train_data.ndim == 5:\n",
    "        train_norm = train_norm.reshape(train_data.shape)\n",
    "        test_norm  = test_norm.reshape(test_data.shape)\n",
    "\n",
    "    return (\n",
    "        train_norm.astype(np.float32),\n",
    "        test_norm.astype(np.float32),\n",
    "        train_mean,\n",
    "        train_std\n",
    "    )\n",
    "\n",
    "# Normalize ALL training data\n",
    "X_train_norm, X_test_norm, mean, std = normalize_eeg(X_train, X_test)\n",
    "\n",
    "y_train_norm = y_train.astype(np.float32)\n",
    "y_test_norm  = y_test.astype(np.float32)\n",
    "\n",
    "print(\"[STEP 6] Normalization done ✅\")\n",
    "print(\"  X_train_norm:\", X_train_norm.shape)\n",
    "print(\"  X_test_norm :\", X_test_norm.shape)\n",
    "print(\"  y_train_norm:\", y_train_norm.shape)\n",
    "print(\"  y_test_norm :\", y_test_norm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f527c-34db-4c23-bdbd-c8ce03b519f0",
   "metadata": {},
   "source": [
    "Cell 7 — EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ea4279-53c2-49c1-b834-7b1e1c20775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\DL\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Xtr: (5100, 14, 128, 1), ytr: (5100,)\n",
      "[INFO] Test / LOSO subject is NOT used here.\n",
      "[SPLIT] Train: (4080, 14, 128, 1), Val: (1020, 14, 128, 1)\n",
      "Epoch 1/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 119ms/step - accuracy: 0.5380 - loss: 0.7214 - val_accuracy: 0.4716 - val_loss: 0.6955 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.5583 - loss: 0.6817 - val_accuracy: 0.4853 - val_loss: 0.6942 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.5784 - loss: 0.6616 - val_accuracy: 0.5029 - val_loss: 0.6912 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.6257 - loss: 0.6349 - val_accuracy: 0.5843 - val_loss: 0.6847 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.6537 - loss: 0.6110 - val_accuracy: 0.6157 - val_loss: 0.6784 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.6642 - loss: 0.5880 - val_accuracy: 0.6559 - val_loss: 0.6650 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.7056 - loss: 0.5554 - val_accuracy: 0.7196 - val_loss: 0.6527 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.7328 - loss: 0.5252 - val_accuracy: 0.6765 - val_loss: 0.6387 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7417 - loss: 0.5116 - val_accuracy: 0.6627 - val_loss: 0.6278 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.7640 - loss: 0.4844 - val_accuracy: 0.6314 - val_loss: 0.6182 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.7870 - loss: 0.4459 - val_accuracy: 0.5990 - val_loss: 0.6392 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.7919 - loss: 0.4397 - val_accuracy: 0.6353 - val_loss: 0.5955 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.7958 - loss: 0.4346 - val_accuracy: 0.6441 - val_loss: 0.5841 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.8059 - loss: 0.4133 - val_accuracy: 0.6412 - val_loss: 0.5945 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8162 - loss: 0.4227 - val_accuracy: 0.6500 - val_loss: 0.5829 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.8025 - loss: 0.4259 - val_accuracy: 0.7029 - val_loss: 0.5247 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.8230 - loss: 0.4078 - val_accuracy: 0.6863 - val_loss: 0.5469 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8338 - loss: 0.3785 - val_accuracy: 0.6824 - val_loss: 0.5656 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8429 - loss: 0.3566 - val_accuracy: 0.6912 - val_loss: 0.5456 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8451 - loss: 0.3542 - val_accuracy: 0.7255 - val_loss: 0.5139 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.8390 - loss: 0.3664 - val_accuracy: 0.7539 - val_loss: 0.4641 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8534 - loss: 0.3503 - val_accuracy: 0.7480 - val_loss: 0.5037 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8578 - loss: 0.3440 - val_accuracy: 0.7627 - val_loss: 0.4671 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8640 - loss: 0.3244 - val_accuracy: 0.7814 - val_loss: 0.4276 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8667 - loss: 0.3190 - val_accuracy: 0.8176 - val_loss: 0.3713 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8676 - loss: 0.3239 - val_accuracy: 0.8118 - val_loss: 0.3842 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8784 - loss: 0.2951 - val_accuracy: 0.8343 - val_loss: 0.3407 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8826 - loss: 0.2925 - val_accuracy: 0.8676 - val_loss: 0.2902 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8831 - loss: 0.3091 - val_accuracy: 0.8676 - val_loss: 0.2883 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8799 - loss: 0.2923 - val_accuracy: 0.8637 - val_loss: 0.2794 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.8782 - loss: 0.3011 - val_accuracy: 0.9000 - val_loss: 0.2445 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8922 - loss: 0.2606 - val_accuracy: 0.8912 - val_loss: 0.2390 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9032 - loss: 0.2487 - val_accuracy: 0.9284 - val_loss: 0.1883 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9076 - loss: 0.2286 - val_accuracy: 0.9451 - val_loss: 0.1640 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9032 - loss: 0.2376 - val_accuracy: 0.9520 - val_loss: 0.1610 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9071 - loss: 0.2404 - val_accuracy: 0.9431 - val_loss: 0.1652 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9029 - loss: 0.2408 - val_accuracy: 0.9510 - val_loss: 0.1518 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9118 - loss: 0.2316\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9074 - loss: 0.2386 - val_accuracy: 0.9529 - val_loss: 0.1566 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9118 - loss: 0.2304 - val_accuracy: 0.9500 - val_loss: 0.1580 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9194 - loss: 0.2053 - val_accuracy: 0.9549 - val_loss: 0.1275 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9255 - loss: 0.2024 - val_accuracy: 0.9637 - val_loss: 0.1187 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9255 - loss: 0.1887 - val_accuracy: 0.9676 - val_loss: 0.1145 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9201 - loss: 0.2087 - val_accuracy: 0.9696 - val_loss: 0.1062 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9252 - loss: 0.1914 - val_accuracy: 0.9706 - val_loss: 0.1129 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9201 - loss: 0.1995 - val_accuracy: 0.9667 - val_loss: 0.1160 - learning_rate: 5.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9257 - loss: 0.1870 - val_accuracy: 0.9706 - val_loss: 0.1113 - learning_rate: 5.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9235 - loss: 0.1872 - val_accuracy: 0.9637 - val_loss: 0.1197 - learning_rate: 5.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9289 - loss: 0.1856 - val_accuracy: 0.9647 - val_loss: 0.1088 - learning_rate: 5.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9306 - loss: 0.1769 - val_accuracy: 0.9676 - val_loss: 0.1098 - learning_rate: 5.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9294 - loss: 0.1789 - val_accuracy: 0.9735 - val_loss: 0.1015 - learning_rate: 5.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9328 - loss: 0.1788 - val_accuracy: 0.9686 - val_loss: 0.1030 - learning_rate: 5.0000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9292 - loss: 0.1823 - val_accuracy: 0.9706 - val_loss: 0.1002 - learning_rate: 5.0000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9368 - loss: 0.1688 - val_accuracy: 0.9706 - val_loss: 0.0927 - learning_rate: 5.0000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9289 - loss: 0.1772 - val_accuracy: 0.9676 - val_loss: 0.1003 - learning_rate: 5.0000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9328 - loss: 0.1767 - val_accuracy: 0.9725 - val_loss: 0.0926 - learning_rate: 5.0000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9314 - loss: 0.1765 - val_accuracy: 0.9755 - val_loss: 0.0903 - learning_rate: 5.0000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9368 - loss: 0.1620 - val_accuracy: 0.9725 - val_loss: 0.0925 - learning_rate: 5.0000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9336 - loss: 0.1639 - val_accuracy: 0.9745 - val_loss: 0.1016 - learning_rate: 5.0000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9338 - loss: 0.1712 - val_accuracy: 0.9745 - val_loss: 0.0937 - learning_rate: 5.0000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9395 - loss: 0.1595 - val_accuracy: 0.9765 - val_loss: 0.0882 - learning_rate: 5.0000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9404 - loss: 0.1627 - val_accuracy: 0.9755 - val_loss: 0.0919 - learning_rate: 5.0000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9282 - loss: 0.1740 - val_accuracy: 0.9608 - val_loss: 0.1249 - learning_rate: 5.0000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9387 - loss: 0.1638 - val_accuracy: 0.9706 - val_loss: 0.0926 - learning_rate: 5.0000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9397 - loss: 0.1640\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9397 - loss: 0.1607 - val_accuracy: 0.9765 - val_loss: 0.0853 - learning_rate: 5.0000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9368 - loss: 0.1533 - val_accuracy: 0.9725 - val_loss: 0.0876 - learning_rate: 2.5000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9404 - loss: 0.1513 - val_accuracy: 0.9765 - val_loss: 0.0852 - learning_rate: 2.5000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9402 - loss: 0.1503 - val_accuracy: 0.9706 - val_loss: 0.0840 - learning_rate: 2.5000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9451 - loss: 0.1435 - val_accuracy: 0.9755 - val_loss: 0.0892 - learning_rate: 2.5000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9461 - loss: 0.1377 - val_accuracy: 0.9745 - val_loss: 0.0869 - learning_rate: 2.5000e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9429 - loss: 0.1539 - val_accuracy: 0.9716 - val_loss: 0.0924 - learning_rate: 2.5000e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9402 - loss: 0.1491 - val_accuracy: 0.9735 - val_loss: 0.0887 - learning_rate: 2.5000e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9424 - loss: 0.1542 - val_accuracy: 0.9676 - val_loss: 0.0981 - learning_rate: 2.5000e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9365 - loss: 0.1487\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 134ms/step - accuracy: 0.9387 - loss: 0.1494 - val_accuracy: 0.9735 - val_loss: 0.0847 - learning_rate: 2.5000e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9490 - loss: 0.1462 - val_accuracy: 0.9755 - val_loss: 0.0794 - learning_rate: 1.2500e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9529 - loss: 0.1327 - val_accuracy: 0.9775 - val_loss: 0.0815 - learning_rate: 1.2500e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9517 - loss: 0.1283 - val_accuracy: 0.9784 - val_loss: 0.0779 - learning_rate: 1.2500e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9468 - loss: 0.1384 - val_accuracy: 0.9745 - val_loss: 0.0807 - learning_rate: 1.2500e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9444 - loss: 0.1424 - val_accuracy: 0.9784 - val_loss: 0.0820 - learning_rate: 1.2500e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9458 - loss: 0.1515 - val_accuracy: 0.9735 - val_loss: 0.0782 - learning_rate: 1.2500e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9427 - loss: 0.1425\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9419 - loss: 0.1483 - val_accuracy: 0.9794 - val_loss: 0.0766 - learning_rate: 1.2500e-04\n",
      "Epoch 81/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9478 - loss: 0.1398 - val_accuracy: 0.9775 - val_loss: 0.0796 - learning_rate: 6.2500e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9483 - loss: 0.1288 - val_accuracy: 0.9784 - val_loss: 0.0770 - learning_rate: 6.2500e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9544 - loss: 0.1241 - val_accuracy: 0.9765 - val_loss: 0.0762 - learning_rate: 6.2500e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9458 - loss: 0.1506 - val_accuracy: 0.9765 - val_loss: 0.0767 - learning_rate: 6.2500e-05\n",
      "Epoch 85/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9453 - loss: 0.1404 - val_accuracy: 0.9755 - val_loss: 0.0775 - learning_rate: 6.2500e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9515 - loss: 0.1252 - val_accuracy: 0.9755 - val_loss: 0.0748 - learning_rate: 6.2500e-05\n",
      "Epoch 87/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9469 - loss: 0.1459\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9478 - loss: 0.1379 - val_accuracy: 0.9765 - val_loss: 0.0796 - learning_rate: 6.2500e-05\n",
      "Epoch 88/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9463 - loss: 0.1437 - val_accuracy: 0.9755 - val_loss: 0.0755 - learning_rate: 3.1250e-05\n",
      "Epoch 89/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9505 - loss: 0.1371 - val_accuracy: 0.9765 - val_loss: 0.0763 - learning_rate: 3.1250e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9478 - loss: 0.1307 - val_accuracy: 0.9765 - val_loss: 0.0740 - learning_rate: 3.1250e-05\n",
      "Epoch 91/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9488 - loss: 0.1321\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9490 - loss: 0.1323 - val_accuracy: 0.9814 - val_loss: 0.0760 - learning_rate: 3.1250e-05\n",
      "Epoch 92/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9456 - loss: 0.1306 - val_accuracy: 0.9765 - val_loss: 0.0767 - learning_rate: 1.5625e-05\n",
      "Epoch 93/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9493 - loss: 0.1353 - val_accuracy: 0.9775 - val_loss: 0.0743 - learning_rate: 1.5625e-05\n",
      "Epoch 94/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9495 - loss: 0.1344 - val_accuracy: 0.9765 - val_loss: 0.0752 - learning_rate: 1.5625e-05\n",
      "Epoch 95/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9558 - loss: 0.1265\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9539 - loss: 0.1304 - val_accuracy: 0.9784 - val_loss: 0.0764 - learning_rate: 1.5625e-05\n",
      "Epoch 96/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9502 - loss: 0.1361 - val_accuracy: 0.9784 - val_loss: 0.0760 - learning_rate: 7.8125e-06\n",
      "Epoch 97/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 127ms/step - accuracy: 0.9520 - loss: 0.1364 - val_accuracy: 0.9784 - val_loss: 0.0762 - learning_rate: 7.8125e-06\n",
      "Epoch 98/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9483 - loss: 0.1314 - val_accuracy: 0.9784 - val_loss: 0.0764 - learning_rate: 7.8125e-06\n",
      "Epoch 99/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9461 - loss: 0.1319\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9429 - loss: 0.1396 - val_accuracy: 0.9784 - val_loss: 0.0760 - learning_rate: 7.8125e-06\n",
      "Epoch 100/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9461 - loss: 0.1429 - val_accuracy: 0.9784 - val_loss: 0.0764 - learning_rate: 3.9063e-06\n",
      "Epoch 101/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9485 - loss: 0.1334 - val_accuracy: 0.9784 - val_loss: 0.0765 - learning_rate: 3.9063e-06\n",
      "Epoch 102/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9498 - loss: 0.1327 - val_accuracy: 0.9784 - val_loss: 0.0758 - learning_rate: 3.9063e-06\n",
      "Epoch 103/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9492 - loss: 0.1312\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9512 - loss: 0.1296 - val_accuracy: 0.9784 - val_loss: 0.0756 - learning_rate: 3.9063e-06\n",
      "Epoch 104/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9483 - loss: 0.1354 - val_accuracy: 0.9784 - val_loss: 0.0757 - learning_rate: 1.9531e-06\n",
      "Epoch 105/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9502 - loss: 0.1272 - val_accuracy: 0.9784 - val_loss: 0.0757 - learning_rate: 1.9531e-06\n",
      "Epoch 105: early stopping\n",
      "Restoring model weights from the end of the best epoch: 90.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] Model saved as cole-final.h5\n",
      "\n",
      "[CONFUSION MATRIX]\n",
      "[[470  10]\n",
      " [ 14 526]]\n",
      "[VAL] acc=0.9765, prec=0.9813, rec=0.9741, f1=0.9777\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 4: 80:20 Train / Validation Split (EEGNet)\n",
    "# ====================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, SeparableConv2D, BatchNormalization, Activation,\n",
    "    DepthwiseConv2D, Conv2D, AveragePooling2D,\n",
    "    Dropout, Dense, Flatten\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# --------------------------------------------------------------------\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# EEGNet model\n",
    "# --------------------------------------------------------------------\n",
    "def create_eegnet_model(input_shape, dropout_rate=0.5, num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(16, (1, 64), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = DepthwiseConv2D(\n",
    "        (input_shape[0], 1),\n",
    "        depth_multiplier=2,\n",
    "        padding='valid',\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 4))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = SeparableConv2D(16, (1, 16), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 8))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Compile model\n",
    "# --------------------------------------------------------------------\n",
    "def compile_model():\n",
    "    input_shape = (Xtr.shape[1], Xtr.shape[2], Xtr.shape[3])\n",
    "    model = create_eegnet_model(input_shape)\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-3),\n",
    "        loss=BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[BinaryAccuracy(name=\"accuracy\", threshold=0.5)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def make_callbacks():\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"loss\",\n",
    "            factor=0.5,\n",
    "            patience=4,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Use ALL normalized training data\n",
    "# --------------------------------------------------------------------\n",
    "Xtr = X_train_norm\n",
    "ytr = y_train_norm\n",
    "\n",
    "print(f\"[TRAIN] Xtr: {Xtr.shape}, ytr: {ytr.shape}\")\n",
    "print(\"[INFO] Test / LOSO subject is NOT used here.\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 80:20 Stratified Split\n",
    "# --------------------------------------------------------------------\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(sss.split(Xtr, ytr))\n",
    "\n",
    "X_train, X_val = Xtr[train_idx], Xtr[val_idx]\n",
    "y_train, y_val = ytr[train_idx], ytr[val_idx]\n",
    "\n",
    "print(f\"[SPLIT] Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Train model\n",
    "# --------------------------------------------------------------------\n",
    "model = compile_model()\n",
    "callbacks = make_callbacks()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Save final model\n",
    "# --------------------------------------------------------------------\n",
    "model.save(\"mohommad-final.h5\")\n",
    "print(\"[SAVE] Model saved as cole-final.h5\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Validation Confusion Matrix\n",
    "# --------------------------------------------------------------------\n",
    "y_prob = model.predict(X_val, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "y_true = y_val.astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "eps = 1e-12\n",
    "\n",
    "acc  = (tp + tn) / max(tp + tn + fp + fn, eps)\n",
    "prec = tp / max(tp + fp, eps)\n",
    "rec  = tp / max(tp + fn, eps)\n",
    "f1   = 2 * prec * rec / max(prec + rec, eps)\n",
    "\n",
    "print(\"\\n[CONFUSION MATRIX]\")\n",
    "print(cm)\n",
    "print(f\"[VAL] acc={acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e2a13-6367-41ca-be66-a43594c2940e",
   "metadata": {},
   "source": [
    "Cell 8 — Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ac6662-33c1-4e25-b6c5-d47c860fba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Preparing TEST data...\n",
      "[TEST] Xte shape: (300, 14, 128, 1)\n",
      "[TEST] yte shape: (300,), class distribution: (array([0]), array([300]))\n",
      "[TEST] Loading final model: mohommad-final.h5\n",
      "[TEST] Running inference on TEST subject...\n",
      "[TEST] Mean predicted probability: 0.6310\n",
      "\n",
      "[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[ 92 208]\n",
      " [  0   0]]\n",
      "Accuracy : 0.3067\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-score : 0.0000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Inference on TEST subject (adam) using FINAL EEGNet\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Prepare TEST data\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Preparing TEST data...\")\n",
    "\n",
    "Xte = X_test_norm\n",
    "yte = y_test_norm.astype(int)\n",
    "\n",
    "print(f\"[TEST] Xte shape: {Xte.shape}\")\n",
    "print(f\"[TEST] yte shape: {yte.shape}, class distribution: {np.unique(yte, return_counts=True)}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Load FINAL trained model\n",
    "# ------------------------------------------------------------------\n",
    "model_path = \"mohommad-final.h5\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        \"Final model 'cole-final.h5' not found. \"\n",
    "        \"Please run Cell 4 (80:20 training) first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[TEST] Loading final model: {model_path}\")\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Run inference on TEST subject\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Running inference on TEST subject...\")\n",
    "\n",
    "y_prob = model.predict(Xte, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(f\"[TEST] Mean predicted probability: {y_prob.mean():.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Metrics on TEST subject (segment-level)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc = accuracy_score(yte, y_pred)\n",
    "prec = precision_score(yte, y_pred, zero_division=0)\n",
    "rec = recall_score(yte, y_pred, zero_division=0)\n",
    "f1 = f1_score(yte, y_pred, zero_division=0)\n",
    "\n",
    "print(\"\\n[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572dbf3e-bbb5-4865-883f-30e374700949",
   "metadata": {},
   "source": [
    "Cell 9 - Performance Matrices for each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a57240-50ea-4736-a7ac-52ef7878b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] Verifying predictions and labels are available...\n",
      "[CHECK] Found predictions for 300 test segments.\n",
      "\n",
      "[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[ 92 208]\n",
      " [  0   0]]\n",
      "Accuracy : 0.3067\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-score : 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Final model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================\n",
    "# Cell 6: Confusion Matrix & Metrics on TEST (FINAL EEGNet)\n",
    "# =======================================================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# ------------------------------------------------------------------\n",
    "def compute_metrics_from_cm(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    eps = 1e-12\n",
    "    acc  = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec  = tp / (tp + fn + eps)\n",
    "    f1   = 2 * prec * rec / (prec + rec + eps)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], ['0', '1'])\n",
    "    plt.yticks([0, 1], ['0', '1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(\n",
    "                j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Validate upstream artifacts\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[CHECK] Verifying predictions and labels are available...\")\n",
    "\n",
    "if 'y_pred' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Predictions 'y_pred' not found. \"\n",
    "        \"Please run Cell 5 (final model inference) first.\"\n",
    "    )\n",
    "\n",
    "if 'yte' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Test labels 'yte' not found. \"\n",
    "        \"Please run Cell 5 first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[CHECK] Found predictions for {len(y_pred)} test segments.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Confusion Matrix & Metrics (FINAL model)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc, prec, rec, f1 = compute_metrics_from_cm(cm)\n",
    "\n",
    "print(\"\\n[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(cm, title=\"Confusion Matrix - FINAL EEGNet\")\n",
    "\n",
    "print(\"[DONE] Final model evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0f120-39fd-497e-9098-05e6c82f4807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
