{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af5b471-cea9-4194-bc23-da5cdc2f9c67",
   "metadata": {},
   "source": [
    "Cell 1 — Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64aaa897-ab61-4eb4-a558-4c078acf751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Base path set to:\n",
      "C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\n",
      "[INFO] Found 18 folders:\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Selected test subject (LOSO): Marjan\n",
      "[INFO] Training subjects (17):\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Sampling rate (SFREQ) = 128.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 1: Setup & discovery\n",
    "# =========================\n",
    "\n",
    "import os  # file ops\n",
    "import glob  # file matching\n",
    "import numpy as np  # arrays\n",
    "import random  # seeds\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data path\n",
    "BASE_PATH = r\"C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\"\n",
    "print(f\"[INFO] Base path set to:\\n{BASE_PATH}\")\n",
    "\n",
    "# List subject folders\n",
    "all_folders = [p for p in sorted(os.listdir(BASE_PATH)) if os.path.isdir(os.path.join(BASE_PATH, p))]\n",
    "print(f\"[INFO] Found {len(all_folders)} folders:\\n{all_folders}\")\n",
    "\n",
    "# Expected count (change if needed)\n",
    "assert len(all_folders) == 18, f\"Expected 18 folders, found {len(all_folders)}.\"\n",
    "\n",
    "# LOSO test subject\n",
    "TEST_SUBJECT = \"Marjan\"\n",
    "print(f\"[INFO] Selected test subject (LOSO): {TEST_SUBJECT}\")\n",
    "\n",
    "# Train subjects = remaining\n",
    "train_subjects = [s for s in all_folders if s.lower() != TEST_SUBJECT.lower()]\n",
    "print(f\"[INFO] Training subjects ({len(train_subjects)}):\\n{train_subjects}\")\n",
    "\n",
    "# Sampling rate\n",
    "SFREQ = 128.0\n",
    "print(f\"[INFO] Sampling rate (SFREQ) = {SFREQ} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e649681-2861-4ce2-bbd1-0d9dd3359f42",
   "metadata": {},
   "source": [
    "Cell 2 — Load EEG + labels per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943c5c6e-a7aa-4618-899c-daf341c25358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 2] Loading data from folders...\n",
      "[STEP 2]       Amin | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Amin | EEG=          Amin.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Amin1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]      Amin1 | EEG=         Amin1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Cole | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Cole | EEG=          Cole.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Daniel | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]     Daniel | EEG=        Daniel.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]    Ismayil | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]    Ismayil | EEG=       Ismayil.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Jack | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Jack | EEG=          Jack.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]      James | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      James | EEG=         James.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Josh | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Josh | EEG=          Josh.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Marjan | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Marjan | EEG=        Marjan.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]        Max | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Max | EEG=           Max.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Mina | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mina | EEG=          Mina.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 1 | EEG=        Mina 1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 3 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 3 | EEG=        Mina 3.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]   Mohammad | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]   Mohammad | EEG=      Mohammad.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Mona | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mona | EEG=          Mona.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Roddy | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      Roddy | EEG=         Roddy.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]        Sam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Sam | EEG=           Sam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       adam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       adam | EEG=          adam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2] Loading complete ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 2: Load data (EMPTY LABEL => CLASS 0)\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path  # helper\n",
    "\n",
    "def load_subject(subj_dir: str, subj_name: str, expected_channels: int = 14):\n",
    "    \"\"\"\n",
    "    EEG file: {subj_name}.npy\n",
    "    Label file: {subj_name}_target.npy\n",
    "    Label rule:\n",
    "      - empty array -> class 0\n",
    "      - [0] -> class 0\n",
    "      - [1] -> class 1\n",
    "    \"\"\"\n",
    "\n",
    "    # EEG file path\n",
    "    eeg_file = os.path.join(subj_dir, f\"{subj_name}.npy\")\n",
    "    assert os.path.exists(eeg_file), f\"[ERROR] Missing EEG file: {eeg_file}\"\n",
    "\n",
    "    # Label file path\n",
    "    target_file = os.path.join(subj_dir, f\"{subj_name}_target.npy\")\n",
    "    assert os.path.exists(target_file), f\"[ERROR] Missing label file: {target_file}\"\n",
    "\n",
    "    # Load EEG\n",
    "    X_raw = np.load(eeg_file, allow_pickle=True)\n",
    "    X_raw = np.asarray(X_raw)\n",
    "\n",
    "    # Standardize EEG to (n_trials, 14, T)\n",
    "    if X_raw.ndim == 2:\n",
    "        X_trials = X_raw[None, :, :]\n",
    "    elif X_raw.ndim == 3:\n",
    "        dims = list(X_raw.shape)\n",
    "        chan_axes = [i for i, d in enumerate(dims) if d == expected_channels]\n",
    "        chan_axis = chan_axes[0] if len(chan_axes) == 1 else 1\n",
    "        time_axis = int(np.argmax(dims))\n",
    "        trial_axis = [i for i in range(3) if i not in (chan_axis, time_axis)][0]\n",
    "        X_trials = np.moveaxis(X_raw, (trial_axis, chan_axis, time_axis), (0, 1, 2))\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] Unsupported EEG shape {X_raw.shape} in {eeg_file}\")\n",
    "\n",
    "    assert X_trials.shape[1] == expected_channels, f\"[ERROR] Expected {expected_channels} channels, got {X_trials.shape[1]} in {eeg_file}\"\n",
    "\n",
    "    # Load label\n",
    "    y_raw = np.load(target_file, allow_pickle=True)\n",
    "    y_arr = np.asarray(y_raw).ravel()\n",
    "\n",
    "    # Apply your rule\n",
    "    if y_arr.size == 0:\n",
    "        label = 0\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\")\n",
    "    else:\n",
    "        label = int(y_arr[0])\n",
    "        if label not in (0, 1):\n",
    "            raise ValueError(f\"[ERROR] Invalid label {label} in {target_file} (must be 0 or 1)\")\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape={y_arr.shape}, value={y_arr} -> class={label}\")\n",
    "\n",
    "    # Repeat label for all trials\n",
    "    y_trials = np.full((X_trials.shape[0],), label, dtype=np.int32)\n",
    "\n",
    "    return X_trials.astype(np.float32), y_trials.astype(np.int32), eeg_file, target_file\n",
    "\n",
    "\n",
    "# Load all subjects\n",
    "subject_data = {}\n",
    "print(\"[STEP 2] Loading data from folders...\")\n",
    "\n",
    "for subj in all_folders:\n",
    "    subj_dir = os.path.join(BASE_PATH, subj)\n",
    "    X_trials, y_trials, eeg_file, target_file = load_subject(subj_dir, subj)\n",
    "\n",
    "    subject_data[subj] = (X_trials, y_trials)\n",
    "\n",
    "    print(f\"[STEP 2] {subj:>10} | EEG={Path(eeg_file).name:>18} | X={X_trials.shape} | y={y_trials.shape} | class={y_trials[0]}\")\n",
    "\n",
    "print(\"[STEP 2] Loading complete ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd93ca8-a94c-485c-8a61-e6e7441f6299",
   "metadata": {},
   "source": [
    "Cell 3 — Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733e592c-98e1-4cbd-afd2-6fdc8cf407f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 3] Preprocessing functions ready ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3: Preprocessing functions\n",
    "# =========================\n",
    "\n",
    "import mne  # EEG processing\n",
    "import pywt  # wavelets\n",
    "from sklearn.decomposition import FastICA  # ICA\n",
    "from typing import Optional, Tuple, Union, Sequence, Dict, List  # typing\n",
    "\n",
    "def wavelet_enhanced_ica(\n",
    "    data: np.ndarray,\n",
    "    n_components: int = 10,\n",
    "    wavelet: str = \"db4\",\n",
    "    level: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> np.ndarray:\n",
    "    n_ch, n_t = data.shape\n",
    "    n_components = min(n_components, n_ch)\n",
    "\n",
    "    coeffs = pywt.wavedec(data, wavelet=wavelet, level=level, axis=1)\n",
    "    A = coeffs[0]\n",
    "\n",
    "    ica = FastICA(n_components=n_components, random_state=random_state)\n",
    "    S = ica.fit_transform(A.T).T\n",
    "    A_denoised = ica.inverse_transform(S.T).T\n",
    "\n",
    "    coeffs[0] = A_denoised\n",
    "    cleaned = pywt.waverec(coeffs, wavelet=wavelet, axis=1)\n",
    "\n",
    "    if cleaned.shape[1] != n_t:\n",
    "        if cleaned.shape[1] > n_t:\n",
    "            cleaned = cleaned[:, :n_t]\n",
    "        else:\n",
    "            cleaned = np.pad(cleaned, ((0, 0), (0, n_t - cleaned.shape[1])), mode=\"constant\")\n",
    "    return cleaned\n",
    "\n",
    "def _names_from_index_mapping(n_channels: int, index_to_name: Optional[Dict[int, str]]) -> List[str]:\n",
    "    if index_to_name is None:\n",
    "        return [f\"EEG{i+1}\" for i in range(n_channels)]\n",
    "    keys = list(index_to_name.keys())\n",
    "    is_zero_based = (0 in keys) and (1 not in keys)\n",
    "    names = []\n",
    "    for i in range(n_channels):\n",
    "        key = i if is_zero_based else (i + 1)\n",
    "        names.append(index_to_name.get(key, f\"EEG{i+1}\"))\n",
    "    return names\n",
    "\n",
    "def preprocess_eeg(\n",
    "    eeg: np.ndarray,\n",
    "    sfreq: float,\n",
    "    *,\n",
    "    index_to_name: Optional[Dict[int, str]] = None,\n",
    "    use_standard_1010: bool = True,\n",
    "    resample_to: Optional[float] = None,\n",
    "    notch_freqs: Union[None, float, Sequence[float]] = 50.0,\n",
    "    highpass: Optional[float] = 0.05,\n",
    "    bad_point_z: float = 6.0,\n",
    "    bad_channel_z: float = 5.0,\n",
    "    interpolate_bad_channels: bool = True,\n",
    "    car: bool = True,\n",
    "    use_wica: bool = True,\n",
    "    wica_components: int = 10,\n",
    "    wica_wavelet: str = \"db4\",\n",
    "    wica_level: int = 3,\n",
    "    wica_random_state: int = 42,\n",
    "    return_raw: bool = False,\n",
    "):\n",
    "\n",
    "    eeg = np.asarray(eeg, dtype=np.float32)\n",
    "    assert eeg.ndim == 2, \"eeg must be 2D: (n_channels, n_times)\"\n",
    "\n",
    "    n_channels, _ = eeg.shape\n",
    "    ch_names = _names_from_index_mapping(n_channels, index_to_name)\n",
    "    ch_types = ['eog' if str(n).upper().startswith(\"EOG\") else 'eeg' for n in ch_names]\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    raw = mne.io.RawArray(eeg, info, verbose=False)\n",
    "\n",
    "    montage_applied = False\n",
    "    if use_standard_1010:\n",
    "        try:\n",
    "            mont = mne.channels.make_standard_montage(\"standard_1010\")\n",
    "            raw.set_montage(mont, match_case=False, on_missing=\"ignore\")\n",
    "            montage_applied = True\n",
    "        except Exception:\n",
    "            montage_applied = False\n",
    "\n",
    "    if resample_to is not None and float(resample_to) != float(sfreq):\n",
    "        raw.resample(sfreq=resample_to, npad=\"auto\")\n",
    "    sfreq_out = float(raw.info[\"sfreq\"])\n",
    "\n",
    "    if notch_freqs is not None:\n",
    "        raw.notch_filter(freqs=notch_freqs, verbose=False)\n",
    "    if highpass is not None:\n",
    "        raw.filter(l_freq=highpass, h_freq=None, verbose=False)\n",
    "\n",
    "    X = raw.get_data()\n",
    "    mu = np.mean(X, axis=1, keepdims=True)\n",
    "    sd = np.std(X, axis=1, keepdims=True) + 1e-12\n",
    "    hi = mu + bad_point_z * sd\n",
    "    lo = mu - bad_point_z * sd\n",
    "    bad_idx = (X > hi) | (X < lo)\n",
    "\n",
    "    if np.any(bad_idx):\n",
    "        X_fixed = X.copy()\n",
    "        t = np.arange(X.shape[1], dtype=float)\n",
    "        for ch in range(n_channels):\n",
    "            mask = bad_idx[ch]\n",
    "            if mask.any():\n",
    "                good = ~mask\n",
    "                if good.sum() >= 2:\n",
    "                    X_fixed[ch, mask] = np.interp(t[mask], t[good], X_fixed[ch, good])\n",
    "        raw._data = X_fixed\n",
    "\n",
    "    if interpolate_bad_channels and montage_applied:\n",
    "        X = raw.get_data(picks=\"eeg\")\n",
    "        if X.size > 0:\n",
    "            ch_std = X.std(axis=1)\n",
    "            med = np.median(ch_std)\n",
    "            mad = np.median(np.abs(ch_std - med)) + 1e-12\n",
    "            z = 0.6745 * (ch_std - med) / mad\n",
    "            eeg_names = mne.pick_info(raw.info, mne.pick_types(raw.info, eeg=True)).ch_names\n",
    "            bads = [eeg_names[i] for i in np.where(np.abs(z) > bad_channel_z)[0]]\n",
    "            raw.info[\"bads\"] = bads\n",
    "            if bads:\n",
    "                raw.interpolate_bads(reset_bads=True, verbose=False)\n",
    "\n",
    "    if car:\n",
    "        raw.set_eeg_reference(\"average\", projection=True)\n",
    "        raw.apply_proj()\n",
    "\n",
    "    if use_wica:\n",
    "        cleaned = wavelet_enhanced_ica(\n",
    "            raw.get_data(),\n",
    "            n_components=wica_components,\n",
    "            wavelet=wica_wavelet,\n",
    "            level=wica_level,\n",
    "            random_state=wica_random_state\n",
    "        )\n",
    "        raw = mne.io.RawArray(cleaned, raw.info, verbose=False)\n",
    "\n",
    "    cleaned = raw.get_data()\n",
    "    return cleaned, sfreq_out\n",
    "\n",
    "print(\"[STEP 3] Preprocessing functions ready ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a9d20-889d-45c4-bbc2-de62216908cd",
   "metadata": {},
   "source": [
    "Cell 4 — Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bfcbde0-1cc2-409c-8627-93da46606661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 4] Starting preprocessing...\n",
      "[INFO] USE_WICA = True\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 01/17: Amin | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 02/17: Amin1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 03/17: Cole | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 04/17: Daniel | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 05/17: Ismayil | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 06/17: Jack | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 07/17: James | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 08/17: Josh | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 09/17: Max | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 10/17: Mina | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 11/17: Mina 1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 12/17: Mina 3 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 13/17: Mohammad | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 14/17: Mona | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 15/17: Roddy | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 16/17: Sam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 17/17: adam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Test subject: Marjan | trials=1 | fs_out=128.0\n",
      "[STEP 4] Preprocessing done ✅\n",
      "  X_train_trials_all: (17, 14, 38400)\n",
      "  y_train_trials_all: (17,) (array([0, 1], dtype=int32), array([8, 9]))\n",
      "  X_test_trials_all : (1, 14, 38400)\n",
      "  y_test_trials_all : (1,) (array([0], dtype=int32), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 4: Apply preprocessing (LOSO)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 4] Starting preprocessing...\")\n",
    "\n",
    "USE_WICA = True\n",
    "print(f\"[INFO] USE_WICA = {USE_WICA}\")\n",
    "\n",
    "X_train_trials_all, y_train_trials_all = [], []\n",
    "X_test_trials_all, y_test_trials_all = [], []\n",
    "\n",
    "def preprocess_trials(X_trials, y_trials):\n",
    "    cleaned_list = []\n",
    "    label_list = []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        trial = X_trials[i]\n",
    "        label = int(y_trials[i])\n",
    "\n",
    "        cleaned, fs_out = preprocess_eeg(\n",
    "            eeg=trial,\n",
    "            sfreq=SFREQ,\n",
    "            use_wica=USE_WICA\n",
    "        )\n",
    "\n",
    "        cleaned_list.append(cleaned.astype(np.float32, copy=False))\n",
    "        label_list.append(label)\n",
    "\n",
    "    return cleaned_list, np.array(label_list, dtype=np.int32), fs_out\n",
    "\n",
    "# Train subjects\n",
    "for idx, subj in enumerate(train_subjects, start=1):\n",
    "    X_trials, y_trials = subject_data[subj]\n",
    "    cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "    X_train_trials_all.extend(cleaned_trials)\n",
    "    y_train_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "    print(f\"[STEP 4] Train {idx:02d}/{len(train_subjects)}: {subj} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Test subject\n",
    "X_trials, y_trials = subject_data[TEST_SUBJECT]\n",
    "cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "X_test_trials_all.extend(cleaned_trials)\n",
    "y_test_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "print(f\"[STEP 4] Test subject: {TEST_SUBJECT} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Convert to arrays\n",
    "X_train_trials_all = np.array(X_train_trials_all, dtype=np.float32)\n",
    "y_train_trials_all = np.array(y_train_trials_all, dtype=np.int32)\n",
    "\n",
    "X_test_trials_all = np.array(X_test_trials_all, dtype=np.float32)\n",
    "y_test_trials_all = np.array(y_test_trials_all, dtype=np.int32)\n",
    "\n",
    "print(\"[STEP 4] Preprocessing done ✅\")\n",
    "print(\"  X_train_trials_all:\", X_train_trials_all.shape)\n",
    "print(\"  y_train_trials_all:\", y_train_trials_all.shape, np.unique(y_train_trials_all, return_counts=True))\n",
    "print(\"  X_test_trials_all :\", X_test_trials_all.shape)\n",
    "print(\"  y_test_trials_all :\", y_test_trials_all.shape, np.unique(y_test_trials_all, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc3da1-2967-4ee3-84cd-bfcf0f98c0b1",
   "metadata": {},
   "source": [
    "Cell 5 — Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c21ad54-7001-4039-9360-29dfa2905127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 5] Starting segmentation...\n",
      "[STEP 5] Segmented 5/17 trials...\n",
      "[STEP 5] Segmented 10/17 trials...\n",
      "[STEP 5] Segmented 15/17 trials...\n",
      "[STEP 5] Segmented 17/17 trials...\n",
      "[STEP 5] Segmented 1/1 trials...\n",
      "[STEP 5] Segmentation done ✅\n",
      "  X_train: (5100, 14, 128, 1)\n",
      "  y_train: (5100,) (array([0, 1], dtype=int32), array([2400, 2700]))\n",
      "  X_test : (300, 14, 128, 1)\n",
      "  y_test : (300,) (array([0], dtype=int32), array([300]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 5: Segmentation\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 5] Starting segmentation...\")\n",
    "\n",
    "def segment_trial(trial_2d: np.ndarray, label: int, segment_len: int = 128):\n",
    "    C, T = trial_2d.shape\n",
    "    n_segs = T // segment_len\n",
    "    use_T = n_segs * segment_len\n",
    "\n",
    "    if n_segs == 0:\n",
    "        return np.empty((0, C, segment_len), dtype=np.float32), np.empty((0,), dtype=np.int32)\n",
    "\n",
    "    trial_trim = trial_2d[:, :use_T]\n",
    "    Xsegs = trial_trim.reshape(C, n_segs, segment_len).transpose(1, 0, 2)\n",
    "    ysegs = np.full((n_segs,), int(label), dtype=np.int32)\n",
    "    return Xsegs.astype(np.float32, copy=False), ysegs\n",
    "\n",
    "def segment_dataset(X_trials: np.ndarray, y_trials: np.ndarray, segment_len: int = 128):\n",
    "    X_all, y_all = [], []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        Xsegs, ysegs = segment_trial(X_trials[i], int(y_trials[i]), segment_len=segment_len)\n",
    "        X_all.append(Xsegs)\n",
    "        y_all.append(ysegs)\n",
    "\n",
    "        if (i + 1) % 5 == 0 or (i + 1) == X_trials.shape[0]:\n",
    "            print(f\"[STEP 5] Segmented {i+1}/{X_trials.shape[0]} trials...\")\n",
    "\n",
    "    X_out = np.concatenate(X_all, axis=0)\n",
    "    y_out = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    X_out = np.expand_dims(X_out, axis=-1)  # (N,14,128,1)\n",
    "    return X_out.astype(np.float32), y_out.astype(np.int32)\n",
    "\n",
    "X_train, y_train = segment_dataset(X_train_trials_all, y_train_trials_all, segment_len=128)\n",
    "X_test,  y_test  = segment_dataset(X_test_trials_all,  y_test_trials_all,  segment_len=128)\n",
    "\n",
    "print(\"[STEP 5] Segmentation done ✅\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  y_train:\", y_train.shape, np.unique(y_train, return_counts=True))\n",
    "print(\"  X_test :\", X_test.shape)\n",
    "print(\"  y_test :\", y_test.shape, np.unique(y_test, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b4ffd5-bf2f-490e-8854-0a2fc747f1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e69ba7-3952-47b1-bd80-f05237b2e018",
   "metadata": {},
   "source": [
    "Cell 6 — Split train/val + Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97fda4ae-e1c8-4fd1-bf0c-c82ae2a85764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 6] Normalizing using ALL training data (no train/val split)...\n",
      "[STEP 6] Normalization done ✅\n",
      "  X_train_norm: (5100, 14, 128, 1)\n",
      "  X_test_norm : (300, 14, 128, 1)\n",
      "  y_train_norm: (5100,)\n",
      "  y_test_norm : (300,)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 6: Normalize (ALL training data)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 6] Normalizing using ALL training data (no train/val split)...\")\n",
    "\n",
    "def normalize_eeg(train_data: np.ndarray, test_data: np.ndarray):\n",
    "    if train_data.ndim == 5:\n",
    "        train_flat = train_data.reshape(-1, *train_data.shape[2:])\n",
    "        test_flat  = test_data.reshape(-1, *test_data.shape[2:])\n",
    "    else:\n",
    "        train_flat, test_flat = train_data, test_data\n",
    "\n",
    "    # Compute stats on ALL training data\n",
    "    train_mean = np.mean(train_flat, axis=(0, 2, 3), keepdims=True)\n",
    "    train_std  = np.std(train_flat,  axis=(0, 2, 3), keepdims=True) + 1e-8\n",
    "\n",
    "    train_norm = (train_flat - train_mean) / train_std\n",
    "    test_norm  = (test_flat  - train_mean) / train_std\n",
    "\n",
    "    if train_data.ndim == 5:\n",
    "        train_norm = train_norm.reshape(train_data.shape)\n",
    "        test_norm  = test_norm.reshape(test_data.shape)\n",
    "\n",
    "    return (\n",
    "        train_norm.astype(np.float32),\n",
    "        test_norm.astype(np.float32),\n",
    "        train_mean,\n",
    "        train_std\n",
    "    )\n",
    "\n",
    "# Normalize ALL training data\n",
    "X_train_norm, X_test_norm, mean, std = normalize_eeg(X_train, X_test)\n",
    "\n",
    "y_train_norm = y_train.astype(np.float32)\n",
    "y_test_norm  = y_test.astype(np.float32)\n",
    "\n",
    "print(\"[STEP 6] Normalization done ✅\")\n",
    "print(\"  X_train_norm:\", X_train_norm.shape)\n",
    "print(\"  X_test_norm :\", X_test_norm.shape)\n",
    "print(\"  y_train_norm:\", y_train_norm.shape)\n",
    "print(\"  y_test_norm :\", y_test_norm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f527c-34db-4c23-bdbd-c8ce03b519f0",
   "metadata": {},
   "source": [
    "Cell 7 — EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ea4279-53c2-49c1-b834-7b1e1c20775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\DL\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Xtr: (5100, 14, 128, 1), ytr: (5100,)\n",
      "[INFO] Test / LOSO subject is NOT used here.\n",
      "[SPLIT] Train: (4080, 14, 128, 1), Val: (1020, 14, 128, 1)\n",
      "Epoch 1/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 119ms/step - accuracy: 0.5257 - loss: 0.7213 - val_accuracy: 0.4725 - val_loss: 0.6954 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.5488 - loss: 0.6840 - val_accuracy: 0.4765 - val_loss: 0.6945 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.5760 - loss: 0.6648 - val_accuracy: 0.4912 - val_loss: 0.6917 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.6047 - loss: 0.6469 - val_accuracy: 0.5206 - val_loss: 0.6880 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.6343 - loss: 0.6250 - val_accuracy: 0.5833 - val_loss: 0.6798 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.6561 - loss: 0.6032 - val_accuracy: 0.6294 - val_loss: 0.6729 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.6860 - loss: 0.5819 - val_accuracy: 0.7078 - val_loss: 0.6588 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.7054 - loss: 0.5611 - val_accuracy: 0.7059 - val_loss: 0.6395 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.7208 - loss: 0.5435 - val_accuracy: 0.6725 - val_loss: 0.6227 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.7377 - loss: 0.5169 - val_accuracy: 0.6598 - val_loss: 0.6117 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7549 - loss: 0.4997 - val_accuracy: 0.6108 - val_loss: 0.6192 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.7843 - loss: 0.4661 - val_accuracy: 0.5990 - val_loss: 0.6282 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.7917 - loss: 0.4403 - val_accuracy: 0.6069 - val_loss: 0.6339 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.8047 - loss: 0.4309 - val_accuracy: 0.6216 - val_loss: 0.6243 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.8245 - loss: 0.4047 - val_accuracy: 0.6520 - val_loss: 0.5957 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.8135 - loss: 0.4113 - val_accuracy: 0.6353 - val_loss: 0.6414 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8252 - loss: 0.3941 - val_accuracy: 0.6755 - val_loss: 0.5672 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.8382 - loss: 0.3764 - val_accuracy: 0.6627 - val_loss: 0.5940 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 133ms/step - accuracy: 0.8407 - loss: 0.3589 - val_accuracy: 0.6833 - val_loss: 0.5549 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 127ms/step - accuracy: 0.8549 - loss: 0.3465 - val_accuracy: 0.6716 - val_loss: 0.5744 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.8593 - loss: 0.3261 - val_accuracy: 0.6912 - val_loss: 0.5723 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.8672 - loss: 0.3340 - val_accuracy: 0.7294 - val_loss: 0.4743 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.8701 - loss: 0.3139 - val_accuracy: 0.7422 - val_loss: 0.4797 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.8733 - loss: 0.3068 - val_accuracy: 0.7431 - val_loss: 0.4790 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8784 - loss: 0.2854 - val_accuracy: 0.7647 - val_loss: 0.4419 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.8797 - loss: 0.2884 - val_accuracy: 0.7745 - val_loss: 0.4130 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8809 - loss: 0.2927 - val_accuracy: 0.7922 - val_loss: 0.3913 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8941 - loss: 0.2672 - val_accuracy: 0.8020 - val_loss: 0.3827 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8917 - loss: 0.2700 - val_accuracy: 0.8706 - val_loss: 0.2838 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.9017 - loss: 0.2411 - val_accuracy: 0.8275 - val_loss: 0.3417 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9017 - loss: 0.2465 - val_accuracy: 0.8647 - val_loss: 0.2837 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.9100 - loss: 0.2232 - val_accuracy: 0.9010 - val_loss: 0.2354 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9130 - loss: 0.2292 - val_accuracy: 0.9098 - val_loss: 0.2096 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.9169 - loss: 0.2172 - val_accuracy: 0.9245 - val_loss: 0.1915 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9179 - loss: 0.2177 - val_accuracy: 0.9363 - val_loss: 0.1701 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9135 - loss: 0.2203 - val_accuracy: 0.9392 - val_loss: 0.1691 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9169 - loss: 0.2071 - val_accuracy: 0.9402 - val_loss: 0.1626 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9206 - loss: 0.2069 - val_accuracy: 0.9539 - val_loss: 0.1407 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9216 - loss: 0.2034 - val_accuracy: 0.9500 - val_loss: 0.1370 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9203 - loss: 0.1999 - val_accuracy: 0.9529 - val_loss: 0.1309 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.9235 - loss: 0.2021 - val_accuracy: 0.9627 - val_loss: 0.1248 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.9343 - loss: 0.1867 - val_accuracy: 0.9392 - val_loss: 0.1601 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9255 - loss: 0.1954 - val_accuracy: 0.9539 - val_loss: 0.1260 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.9299 - loss: 0.1848 - val_accuracy: 0.9480 - val_loss: 0.1380 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.9282 - loss: 0.1840 - val_accuracy: 0.9461 - val_loss: 0.1536 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9287 - loss: 0.1903 - val_accuracy: 0.9480 - val_loss: 0.1434 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9297 - loss: 0.1854 - val_accuracy: 0.9539 - val_loss: 0.1312 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9331 - loss: 0.1718 - val_accuracy: 0.9578 - val_loss: 0.1229 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.9390 - loss: 0.1580 - val_accuracy: 0.9696 - val_loss: 0.1026 - learning_rate: 0.0010\n",
      "Epoch 50/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.9409 - loss: 0.1601 - val_accuracy: 0.9637 - val_loss: 0.1126 - learning_rate: 0.0010\n",
      "Epoch 51/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.9377 - loss: 0.1698 - val_accuracy: 0.9588 - val_loss: 0.1163 - learning_rate: 0.0010\n",
      "Epoch 52/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.9373 - loss: 0.1706 - val_accuracy: 0.9618 - val_loss: 0.1153 - learning_rate: 0.0010\n",
      "Epoch 53/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.9414 - loss: 0.1509 - val_accuracy: 0.9676 - val_loss: 0.1063 - learning_rate: 0.0010\n",
      "Epoch 54/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9404 - loss: 0.1581 - val_accuracy: 0.9696 - val_loss: 0.0980 - learning_rate: 0.0010\n",
      "Epoch 55/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9419 - loss: 0.1560 - val_accuracy: 0.9637 - val_loss: 0.1100 - learning_rate: 0.0010\n",
      "Epoch 56/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9390 - loss: 0.1574 - val_accuracy: 0.9657 - val_loss: 0.1076 - learning_rate: 0.0010\n",
      "Epoch 57/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9418 - loss: 0.1454\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9387 - loss: 0.1527 - val_accuracy: 0.9686 - val_loss: 0.1037 - learning_rate: 0.0010\n",
      "Epoch 58/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9475 - loss: 0.1422 - val_accuracy: 0.9667 - val_loss: 0.0959 - learning_rate: 5.0000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.9429 - loss: 0.1447 - val_accuracy: 0.9637 - val_loss: 0.0988 - learning_rate: 5.0000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9471 - loss: 0.1446 - val_accuracy: 0.9676 - val_loss: 0.0958 - learning_rate: 5.0000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9520 - loss: 0.1311 - val_accuracy: 0.9627 - val_loss: 0.0973 - learning_rate: 5.0000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9426 - loss: 0.1476 - val_accuracy: 0.9696 - val_loss: 0.0980 - learning_rate: 5.0000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9451 - loss: 0.1440 - val_accuracy: 0.9667 - val_loss: 0.0929 - learning_rate: 5.0000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9424 - loss: 0.1545 - val_accuracy: 0.9706 - val_loss: 0.0973 - learning_rate: 5.0000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9449 - loss: 0.1476\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9439 - loss: 0.1484 - val_accuracy: 0.9696 - val_loss: 0.0945 - learning_rate: 5.0000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9505 - loss: 0.1360 - val_accuracy: 0.9716 - val_loss: 0.0907 - learning_rate: 2.5000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9520 - loss: 0.1311 - val_accuracy: 0.9676 - val_loss: 0.0951 - learning_rate: 2.5000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9500 - loss: 0.1333 - val_accuracy: 0.9706 - val_loss: 0.0920 - learning_rate: 2.5000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9532 - loss: 0.1261 - val_accuracy: 0.9716 - val_loss: 0.0953 - learning_rate: 2.5000e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9537 - loss: 0.1283 - val_accuracy: 0.9686 - val_loss: 0.0937 - learning_rate: 2.5000e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9525 - loss: 0.1252 - val_accuracy: 0.9667 - val_loss: 0.0915 - learning_rate: 2.5000e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 131ms/step - accuracy: 0.9517 - loss: 0.1279 - val_accuracy: 0.9696 - val_loss: 0.0918 - learning_rate: 2.5000e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 129ms/step - accuracy: 0.9502 - loss: 0.1296 - val_accuracy: 0.9667 - val_loss: 0.0935 - learning_rate: 2.5000e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 129ms/step - accuracy: 0.9517 - loss: 0.1287 - val_accuracy: 0.9706 - val_loss: 0.0869 - learning_rate: 2.5000e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 134ms/step - accuracy: 0.9554 - loss: 0.1216 - val_accuracy: 0.9725 - val_loss: 0.0858 - learning_rate: 2.5000e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.9520 - loss: 0.1246 - val_accuracy: 0.9716 - val_loss: 0.0846 - learning_rate: 2.5000e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 129ms/step - accuracy: 0.9505 - loss: 0.1228 - val_accuracy: 0.9716 - val_loss: 0.0865 - learning_rate: 2.5000e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9475 - loss: 0.1417 - val_accuracy: 0.9706 - val_loss: 0.0861 - learning_rate: 2.5000e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9433 - loss: 0.1250\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9446 - loss: 0.1365 - val_accuracy: 0.9706 - val_loss: 0.0885 - learning_rate: 2.5000e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9485 - loss: 0.1298 - val_accuracy: 0.9716 - val_loss: 0.0886 - learning_rate: 1.2500e-04\n",
      "Epoch 81/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9529 - loss: 0.1272 - val_accuracy: 0.9725 - val_loss: 0.0881 - learning_rate: 1.2500e-04\n",
      "Epoch 82/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9485 - loss: 0.1276 - val_accuracy: 0.9706 - val_loss: 0.0873 - learning_rate: 1.2500e-04\n",
      "Epoch 83/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9559 - loss: 0.1191 - val_accuracy: 0.9745 - val_loss: 0.0868 - learning_rate: 1.2500e-04\n",
      "Epoch 84/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9515 - loss: 0.1229 - val_accuracy: 0.9745 - val_loss: 0.0827 - learning_rate: 1.2500e-04\n",
      "Epoch 85/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9559 - loss: 0.1214 - val_accuracy: 0.9716 - val_loss: 0.0839 - learning_rate: 1.2500e-04\n",
      "Epoch 86/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9522 - loss: 0.1260 - val_accuracy: 0.9735 - val_loss: 0.0830 - learning_rate: 1.2500e-04\n",
      "Epoch 87/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9572 - loss: 0.1177\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.9551 - loss: 0.1202 - val_accuracy: 0.9745 - val_loss: 0.0881 - learning_rate: 1.2500e-04\n",
      "Epoch 88/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9500 - loss: 0.1338 - val_accuracy: 0.9745 - val_loss: 0.0821 - learning_rate: 6.2500e-05\n",
      "Epoch 89/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.9515 - loss: 0.1226 - val_accuracy: 0.9765 - val_loss: 0.0864 - learning_rate: 6.2500e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.9522 - loss: 0.1210 - val_accuracy: 0.9745 - val_loss: 0.0823 - learning_rate: 6.2500e-05\n",
      "Epoch 91/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9601 - loss: 0.1092\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9542 - loss: 0.1213 - val_accuracy: 0.9765 - val_loss: 0.0851 - learning_rate: 6.2500e-05\n",
      "Epoch 92/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9529 - loss: 0.1228 - val_accuracy: 0.9755 - val_loss: 0.0813 - learning_rate: 3.1250e-05\n",
      "Epoch 93/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9542 - loss: 0.1171 - val_accuracy: 0.9755 - val_loss: 0.0824 - learning_rate: 3.1250e-05\n",
      "Epoch 94/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9574 - loss: 0.1153 - val_accuracy: 0.9765 - val_loss: 0.0838 - learning_rate: 3.1250e-05\n",
      "Epoch 95/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9600 - loss: 0.1079 - val_accuracy: 0.9765 - val_loss: 0.0841 - learning_rate: 3.1250e-05\n",
      "Epoch 96/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9578 - loss: 0.1110 - val_accuracy: 0.9775 - val_loss: 0.0821 - learning_rate: 3.1250e-05\n",
      "Epoch 97/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9544 - loss: 0.1184 - val_accuracy: 0.9755 - val_loss: 0.0815 - learning_rate: 3.1250e-05\n",
      "Epoch 98/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9564 - loss: 0.1119 - val_accuracy: 0.9775 - val_loss: 0.0816 - learning_rate: 3.1250e-05\n",
      "Epoch 99/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9569 - loss: 0.1083\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9534 - loss: 0.1186 - val_accuracy: 0.9765 - val_loss: 0.0827 - learning_rate: 3.1250e-05\n",
      "Epoch 100/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9556 - loss: 0.1142 - val_accuracy: 0.9755 - val_loss: 0.0834 - learning_rate: 1.5625e-05\n",
      "Epoch 101/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9566 - loss: 0.1200 - val_accuracy: 0.9765 - val_loss: 0.0825 - learning_rate: 1.5625e-05\n",
      "Epoch 102/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9566 - loss: 0.1125 - val_accuracy: 0.9775 - val_loss: 0.0815 - learning_rate: 1.5625e-05\n",
      "Epoch 103/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9558 - loss: 0.1123\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9537 - loss: 0.1173 - val_accuracy: 0.9765 - val_loss: 0.0817 - learning_rate: 1.5625e-05\n",
      "Epoch 104/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9517 - loss: 0.1255 - val_accuracy: 0.9765 - val_loss: 0.0819 - learning_rate: 7.8125e-06\n",
      "Epoch 105/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9576 - loss: 0.1129 - val_accuracy: 0.9765 - val_loss: 0.0822 - learning_rate: 7.8125e-06\n",
      "Epoch 106/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9576 - loss: 0.1099 - val_accuracy: 0.9765 - val_loss: 0.0819 - learning_rate: 7.8125e-06\n",
      "Epoch 107/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9546 - loss: 0.1152\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9525 - loss: 0.1166 - val_accuracy: 0.9765 - val_loss: 0.0821 - learning_rate: 7.8125e-06\n",
      "Epoch 107: early stopping\n",
      "Restoring model weights from the end of the best epoch: 92.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] Model saved as cole-final.h5\n",
      "\n",
      "[CONFUSION MATRIX]\n",
      "[[472   8]\n",
      " [ 17 523]]\n",
      "[VAL] acc=0.9755, prec=0.9849, rec=0.9685, f1=0.9767\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 4: 80:20 Train / Validation Split (EEGNet)\n",
    "# ====================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, SeparableConv2D, BatchNormalization, Activation,\n",
    "    DepthwiseConv2D, Conv2D, AveragePooling2D,\n",
    "    Dropout, Dense, Flatten\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# --------------------------------------------------------------------\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# EEGNet model\n",
    "# --------------------------------------------------------------------\n",
    "def create_eegnet_model(input_shape, dropout_rate=0.5, num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(16, (1, 64), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = DepthwiseConv2D(\n",
    "        (input_shape[0], 1),\n",
    "        depth_multiplier=2,\n",
    "        padding='valid',\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 4))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = SeparableConv2D(16, (1, 16), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 8))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Compile model\n",
    "# --------------------------------------------------------------------\n",
    "def compile_model():\n",
    "    input_shape = (Xtr.shape[1], Xtr.shape[2], Xtr.shape[3])\n",
    "    model = create_eegnet_model(input_shape)\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-3),\n",
    "        loss=BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[BinaryAccuracy(name=\"accuracy\", threshold=0.5)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def make_callbacks():\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"loss\",\n",
    "            factor=0.5,\n",
    "            patience=4,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Use ALL normalized training data\n",
    "# --------------------------------------------------------------------\n",
    "Xtr = X_train_norm\n",
    "ytr = y_train_norm\n",
    "\n",
    "print(f\"[TRAIN] Xtr: {Xtr.shape}, ytr: {ytr.shape}\")\n",
    "print(\"[INFO] Test / LOSO subject is NOT used here.\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 80:20 Stratified Split\n",
    "# --------------------------------------------------------------------\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(sss.split(Xtr, ytr))\n",
    "\n",
    "X_train, X_val = Xtr[train_idx], Xtr[val_idx]\n",
    "y_train, y_val = ytr[train_idx], ytr[val_idx]\n",
    "\n",
    "print(f\"[SPLIT] Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Train model\n",
    "# --------------------------------------------------------------------\n",
    "model = compile_model()\n",
    "callbacks = make_callbacks()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Save final model\n",
    "# --------------------------------------------------------------------\n",
    "model.save(\"marjan-final.h5\")\n",
    "print(\"[SAVE] Model saved as cole-final.h5\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Validation Confusion Matrix\n",
    "# --------------------------------------------------------------------\n",
    "y_prob = model.predict(X_val, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "y_true = y_val.astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "eps = 1e-12\n",
    "\n",
    "acc  = (tp + tn) / max(tp + tn + fp + fn, eps)\n",
    "prec = tp / max(tp + fp, eps)\n",
    "rec  = tp / max(tp + fn, eps)\n",
    "f1   = 2 * prec * rec / max(prec + rec, eps)\n",
    "\n",
    "print(\"\\n[CONFUSION MATRIX]\")\n",
    "print(cm)\n",
    "print(f\"[VAL] acc={acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e2a13-6367-41ca-be66-a43594c2940e",
   "metadata": {},
   "source": [
    "Cell 8 — Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ac6662-33c1-4e25-b6c5-d47c860fba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Preparing TEST data...\n",
      "[TEST] Xte shape: (300, 14, 128, 1)\n",
      "[TEST] yte shape: (300,), class distribution: (array([0]), array([300]))\n",
      "[TEST] Loading final model: marjan-final.h5\n",
      "[TEST] Running inference on TEST subject...\n",
      "[TEST] Mean predicted probability: 0.2146\n",
      "\n",
      "[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[234  66]\n",
      " [  0   0]]\n",
      "Accuracy : 0.7800\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-score : 0.0000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Inference on TEST subject (adam) using FINAL EEGNet\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Prepare TEST data\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Preparing TEST data...\")\n",
    "\n",
    "Xte = X_test_norm\n",
    "yte = y_test_norm.astype(int)\n",
    "\n",
    "print(f\"[TEST] Xte shape: {Xte.shape}\")\n",
    "print(f\"[TEST] yte shape: {yte.shape}, class distribution: {np.unique(yte, return_counts=True)}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Load FINAL trained model\n",
    "# ------------------------------------------------------------------\n",
    "model_path = \"marjan-final.h5\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        \"Final model 'cole-final.h5' not found. \"\n",
    "        \"Please run Cell 4 (80:20 training) first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[TEST] Loading final model: {model_path}\")\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Run inference on TEST subject\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Running inference on TEST subject...\")\n",
    "\n",
    "y_prob = model.predict(Xte, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(f\"[TEST] Mean predicted probability: {y_prob.mean():.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Metrics on TEST subject (segment-level)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc = accuracy_score(yte, y_pred)\n",
    "prec = precision_score(yte, y_pred, zero_division=0)\n",
    "rec = recall_score(yte, y_pred, zero_division=0)\n",
    "f1 = f1_score(yte, y_pred, zero_division=0)\n",
    "\n",
    "print(\"\\n[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572dbf3e-bbb5-4865-883f-30e374700949",
   "metadata": {},
   "source": [
    "Cell 9 - Performance Matrices for each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a57240-50ea-4736-a7ac-52ef7878b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] Verifying predictions and labels are available...\n",
      "[CHECK] Found predictions for 300 test segments.\n",
      "\n",
      "[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[234  66]\n",
      " [  0   0]]\n",
      "Accuracy : 0.7800\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-score : 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Final model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================\n",
    "# Cell 6: Confusion Matrix & Metrics on TEST (FINAL EEGNet)\n",
    "# =======================================================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# ------------------------------------------------------------------\n",
    "def compute_metrics_from_cm(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    eps = 1e-12\n",
    "    acc  = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec  = tp / (tp + fn + eps)\n",
    "    f1   = 2 * prec * rec / (prec + rec + eps)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], ['0', '1'])\n",
    "    plt.yticks([0, 1], ['0', '1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(\n",
    "                j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Validate upstream artifacts\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[CHECK] Verifying predictions and labels are available...\")\n",
    "\n",
    "if 'y_pred' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Predictions 'y_pred' not found. \"\n",
    "        \"Please run Cell 5 (final model inference) first.\"\n",
    "    )\n",
    "\n",
    "if 'yte' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Test labels 'yte' not found. \"\n",
    "        \"Please run Cell 5 first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[CHECK] Found predictions for {len(y_pred)} test segments.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Confusion Matrix & Metrics (FINAL model)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc, prec, rec, f1 = compute_metrics_from_cm(cm)\n",
    "\n",
    "print(\"\\n[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(cm, title=\"Confusion Matrix - FINAL EEGNet\")\n",
    "\n",
    "print(\"[DONE] Final model evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0f120-39fd-497e-9098-05e6c82f4807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
