{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af5b471-cea9-4194-bc23-da5cdc2f9c67",
   "metadata": {},
   "source": [
    "Cell 1 — Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64aaa897-ab61-4eb4-a558-4c078acf751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Base path set to:\n",
      "C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\n",
      "[INFO] Found 18 folders:\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Selected test subject (LOSO): Roddy\n",
      "[INFO] Training subjects (17):\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Sam', 'adam']\n",
      "[INFO] Sampling rate (SFREQ) = 128.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 1: Setup & discovery\n",
    "# =========================\n",
    "\n",
    "import os  # file ops\n",
    "import glob  # file matching\n",
    "import numpy as np  # arrays\n",
    "import random  # seeds\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data path\n",
    "BASE_PATH = r\"C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\"\n",
    "print(f\"[INFO] Base path set to:\\n{BASE_PATH}\")\n",
    "\n",
    "# List subject folders\n",
    "all_folders = [p for p in sorted(os.listdir(BASE_PATH)) if os.path.isdir(os.path.join(BASE_PATH, p))]\n",
    "print(f\"[INFO] Found {len(all_folders)} folders:\\n{all_folders}\")\n",
    "\n",
    "# Expected count (change if needed)\n",
    "assert len(all_folders) == 18, f\"Expected 18 folders, found {len(all_folders)}.\"\n",
    "\n",
    "# LOSO test subject\n",
    "TEST_SUBJECT = \"Roddy\"\n",
    "print(f\"[INFO] Selected test subject (LOSO): {TEST_SUBJECT}\")\n",
    "\n",
    "# Train subjects = remaining\n",
    "train_subjects = [s for s in all_folders if s.lower() != TEST_SUBJECT.lower()]\n",
    "print(f\"[INFO] Training subjects ({len(train_subjects)}):\\n{train_subjects}\")\n",
    "\n",
    "# Sampling rate\n",
    "SFREQ = 128.0\n",
    "print(f\"[INFO] Sampling rate (SFREQ) = {SFREQ} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e649681-2861-4ce2-bbd1-0d9dd3359f42",
   "metadata": {},
   "source": [
    "Cell 2 — Load EEG + labels per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943c5c6e-a7aa-4618-899c-daf341c25358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 2] Loading data from folders...\n",
      "[STEP 2]       Amin | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Amin | EEG=          Amin.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Amin1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]      Amin1 | EEG=         Amin1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Cole | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Cole | EEG=          Cole.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Daniel | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]     Daniel | EEG=        Daniel.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]    Ismayil | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]    Ismayil | EEG=       Ismayil.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Jack | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Jack | EEG=          Jack.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]      James | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      James | EEG=         James.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Josh | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Josh | EEG=          Josh.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Marjan | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Marjan | EEG=        Marjan.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]        Max | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Max | EEG=           Max.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Mina | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mina | EEG=          Mina.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 1 | EEG=        Mina 1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 3 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 3 | EEG=        Mina 3.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]   Mohammad | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]   Mohammad | EEG=      Mohammad.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Mona | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mona | EEG=          Mona.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Roddy | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      Roddy | EEG=         Roddy.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]        Sam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Sam | EEG=           Sam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       adam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       adam | EEG=          adam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2] Loading complete ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 2: Load data (EMPTY LABEL => CLASS 0)\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path  # helper\n",
    "\n",
    "def load_subject(subj_dir: str, subj_name: str, expected_channels: int = 14):\n",
    "    \"\"\"\n",
    "    EEG file: {subj_name}.npy\n",
    "    Label file: {subj_name}_target.npy\n",
    "    Label rule:\n",
    "      - empty array -> class 0\n",
    "      - [0] -> class 0\n",
    "      - [1] -> class 1\n",
    "    \"\"\"\n",
    "\n",
    "    # EEG file path\n",
    "    eeg_file = os.path.join(subj_dir, f\"{subj_name}.npy\")\n",
    "    assert os.path.exists(eeg_file), f\"[ERROR] Missing EEG file: {eeg_file}\"\n",
    "\n",
    "    # Label file path\n",
    "    target_file = os.path.join(subj_dir, f\"{subj_name}_target.npy\")\n",
    "    assert os.path.exists(target_file), f\"[ERROR] Missing label file: {target_file}\"\n",
    "\n",
    "    # Load EEG\n",
    "    X_raw = np.load(eeg_file, allow_pickle=True)\n",
    "    X_raw = np.asarray(X_raw)\n",
    "\n",
    "    # Standardize EEG to (n_trials, 14, T)\n",
    "    if X_raw.ndim == 2:\n",
    "        X_trials = X_raw[None, :, :]\n",
    "    elif X_raw.ndim == 3:\n",
    "        dims = list(X_raw.shape)\n",
    "        chan_axes = [i for i, d in enumerate(dims) if d == expected_channels]\n",
    "        chan_axis = chan_axes[0] if len(chan_axes) == 1 else 1\n",
    "        time_axis = int(np.argmax(dims))\n",
    "        trial_axis = [i for i in range(3) if i not in (chan_axis, time_axis)][0]\n",
    "        X_trials = np.moveaxis(X_raw, (trial_axis, chan_axis, time_axis), (0, 1, 2))\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] Unsupported EEG shape {X_raw.shape} in {eeg_file}\")\n",
    "\n",
    "    assert X_trials.shape[1] == expected_channels, f\"[ERROR] Expected {expected_channels} channels, got {X_trials.shape[1]} in {eeg_file}\"\n",
    "\n",
    "    # Load label\n",
    "    y_raw = np.load(target_file, allow_pickle=True)\n",
    "    y_arr = np.asarray(y_raw).ravel()\n",
    "\n",
    "    # Apply your rule\n",
    "    if y_arr.size == 0:\n",
    "        label = 0\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\")\n",
    "    else:\n",
    "        label = int(y_arr[0])\n",
    "        if label not in (0, 1):\n",
    "            raise ValueError(f\"[ERROR] Invalid label {label} in {target_file} (must be 0 or 1)\")\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape={y_arr.shape}, value={y_arr} -> class={label}\")\n",
    "\n",
    "    # Repeat label for all trials\n",
    "    y_trials = np.full((X_trials.shape[0],), label, dtype=np.int32)\n",
    "\n",
    "    return X_trials.astype(np.float32), y_trials.astype(np.int32), eeg_file, target_file\n",
    "\n",
    "\n",
    "# Load all subjects\n",
    "subject_data = {}\n",
    "print(\"[STEP 2] Loading data from folders...\")\n",
    "\n",
    "for subj in all_folders:\n",
    "    subj_dir = os.path.join(BASE_PATH, subj)\n",
    "    X_trials, y_trials, eeg_file, target_file = load_subject(subj_dir, subj)\n",
    "\n",
    "    subject_data[subj] = (X_trials, y_trials)\n",
    "\n",
    "    print(f\"[STEP 2] {subj:>10} | EEG={Path(eeg_file).name:>18} | X={X_trials.shape} | y={y_trials.shape} | class={y_trials[0]}\")\n",
    "\n",
    "print(\"[STEP 2] Loading complete ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd93ca8-a94c-485c-8a61-e6e7441f6299",
   "metadata": {},
   "source": [
    "Cell 3 — Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733e592c-98e1-4cbd-afd2-6fdc8cf407f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 3] Preprocessing functions ready ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3: Preprocessing functions\n",
    "# =========================\n",
    "\n",
    "import mne  # EEG processing\n",
    "import pywt  # wavelets\n",
    "from sklearn.decomposition import FastICA  # ICA\n",
    "from typing import Optional, Tuple, Union, Sequence, Dict, List  # typing\n",
    "\n",
    "def wavelet_enhanced_ica(\n",
    "    data: np.ndarray,\n",
    "    n_components: int = 10,\n",
    "    wavelet: str = \"db4\",\n",
    "    level: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> np.ndarray:\n",
    "    n_ch, n_t = data.shape\n",
    "    n_components = min(n_components, n_ch)\n",
    "\n",
    "    coeffs = pywt.wavedec(data, wavelet=wavelet, level=level, axis=1)\n",
    "    A = coeffs[0]\n",
    "\n",
    "    ica = FastICA(n_components=n_components, random_state=random_state)\n",
    "    S = ica.fit_transform(A.T).T\n",
    "    A_denoised = ica.inverse_transform(S.T).T\n",
    "\n",
    "    coeffs[0] = A_denoised\n",
    "    cleaned = pywt.waverec(coeffs, wavelet=wavelet, axis=1)\n",
    "\n",
    "    if cleaned.shape[1] != n_t:\n",
    "        if cleaned.shape[1] > n_t:\n",
    "            cleaned = cleaned[:, :n_t]\n",
    "        else:\n",
    "            cleaned = np.pad(cleaned, ((0, 0), (0, n_t - cleaned.shape[1])), mode=\"constant\")\n",
    "    return cleaned\n",
    "\n",
    "def _names_from_index_mapping(n_channels: int, index_to_name: Optional[Dict[int, str]]) -> List[str]:\n",
    "    if index_to_name is None:\n",
    "        return [f\"EEG{i+1}\" for i in range(n_channels)]\n",
    "    keys = list(index_to_name.keys())\n",
    "    is_zero_based = (0 in keys) and (1 not in keys)\n",
    "    names = []\n",
    "    for i in range(n_channels):\n",
    "        key = i if is_zero_based else (i + 1)\n",
    "        names.append(index_to_name.get(key, f\"EEG{i+1}\"))\n",
    "    return names\n",
    "\n",
    "def preprocess_eeg(\n",
    "    eeg: np.ndarray,\n",
    "    sfreq: float,\n",
    "    *,\n",
    "    index_to_name: Optional[Dict[int, str]] = None,\n",
    "    use_standard_1010: bool = True,\n",
    "    resample_to: Optional[float] = None,\n",
    "    notch_freqs: Union[None, float, Sequence[float]] = 50.0,\n",
    "    highpass: Optional[float] = 0.05,\n",
    "    bad_point_z: float = 6.0,\n",
    "    bad_channel_z: float = 5.0,\n",
    "    interpolate_bad_channels: bool = True,\n",
    "    car: bool = True,\n",
    "    use_wica: bool = True,\n",
    "    wica_components: int = 10,\n",
    "    wica_wavelet: str = \"db4\",\n",
    "    wica_level: int = 3,\n",
    "    wica_random_state: int = 42,\n",
    "    return_raw: bool = False,\n",
    "):\n",
    "\n",
    "    eeg = np.asarray(eeg, dtype=np.float32)\n",
    "    assert eeg.ndim == 2, \"eeg must be 2D: (n_channels, n_times)\"\n",
    "\n",
    "    n_channels, _ = eeg.shape\n",
    "    ch_names = _names_from_index_mapping(n_channels, index_to_name)\n",
    "    ch_types = ['eog' if str(n).upper().startswith(\"EOG\") else 'eeg' for n in ch_names]\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    raw = mne.io.RawArray(eeg, info, verbose=False)\n",
    "\n",
    "    montage_applied = False\n",
    "    if use_standard_1010:\n",
    "        try:\n",
    "            mont = mne.channels.make_standard_montage(\"standard_1010\")\n",
    "            raw.set_montage(mont, match_case=False, on_missing=\"ignore\")\n",
    "            montage_applied = True\n",
    "        except Exception:\n",
    "            montage_applied = False\n",
    "\n",
    "    if resample_to is not None and float(resample_to) != float(sfreq):\n",
    "        raw.resample(sfreq=resample_to, npad=\"auto\")\n",
    "    sfreq_out = float(raw.info[\"sfreq\"])\n",
    "\n",
    "    if notch_freqs is not None:\n",
    "        raw.notch_filter(freqs=notch_freqs, verbose=False)\n",
    "    if highpass is not None:\n",
    "        raw.filter(l_freq=highpass, h_freq=None, verbose=False)\n",
    "\n",
    "    X = raw.get_data()\n",
    "    mu = np.mean(X, axis=1, keepdims=True)\n",
    "    sd = np.std(X, axis=1, keepdims=True) + 1e-12\n",
    "    hi = mu + bad_point_z * sd\n",
    "    lo = mu - bad_point_z * sd\n",
    "    bad_idx = (X > hi) | (X < lo)\n",
    "\n",
    "    if np.any(bad_idx):\n",
    "        X_fixed = X.copy()\n",
    "        t = np.arange(X.shape[1], dtype=float)\n",
    "        for ch in range(n_channels):\n",
    "            mask = bad_idx[ch]\n",
    "            if mask.any():\n",
    "                good = ~mask\n",
    "                if good.sum() >= 2:\n",
    "                    X_fixed[ch, mask] = np.interp(t[mask], t[good], X_fixed[ch, good])\n",
    "        raw._data = X_fixed\n",
    "\n",
    "    if interpolate_bad_channels and montage_applied:\n",
    "        X = raw.get_data(picks=\"eeg\")\n",
    "        if X.size > 0:\n",
    "            ch_std = X.std(axis=1)\n",
    "            med = np.median(ch_std)\n",
    "            mad = np.median(np.abs(ch_std - med)) + 1e-12\n",
    "            z = 0.6745 * (ch_std - med) / mad\n",
    "            eeg_names = mne.pick_info(raw.info, mne.pick_types(raw.info, eeg=True)).ch_names\n",
    "            bads = [eeg_names[i] for i in np.where(np.abs(z) > bad_channel_z)[0]]\n",
    "            raw.info[\"bads\"] = bads\n",
    "            if bads:\n",
    "                raw.interpolate_bads(reset_bads=True, verbose=False)\n",
    "\n",
    "    if car:\n",
    "        raw.set_eeg_reference(\"average\", projection=True)\n",
    "        raw.apply_proj()\n",
    "\n",
    "    if use_wica:\n",
    "        cleaned = wavelet_enhanced_ica(\n",
    "            raw.get_data(),\n",
    "            n_components=wica_components,\n",
    "            wavelet=wica_wavelet,\n",
    "            level=wica_level,\n",
    "            random_state=wica_random_state\n",
    "        )\n",
    "        raw = mne.io.RawArray(cleaned, raw.info, verbose=False)\n",
    "\n",
    "    cleaned = raw.get_data()\n",
    "    return cleaned, sfreq_out\n",
    "\n",
    "print(\"[STEP 3] Preprocessing functions ready ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a9d20-889d-45c4-bbc2-de62216908cd",
   "metadata": {},
   "source": [
    "Cell 4 — Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bfcbde0-1cc2-409c-8627-93da46606661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 4] Starting preprocessing...\n",
      "[INFO] USE_WICA = True\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 01/17: Amin | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 02/17: Amin1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 03/17: Cole | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 04/17: Daniel | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 05/17: Ismayil | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 06/17: Jack | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 07/17: James | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 08/17: Josh | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 09/17: Marjan | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 10/17: Max | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 11/17: Mina | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 12/17: Mina 1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 13/17: Mina 3 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 14/17: Mohammad | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 15/17: Mona | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 16/17: Sam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 17/17: adam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Test subject: Roddy | trials=1 | fs_out=128.0\n",
      "[STEP 4] Preprocessing done ✅\n",
      "  X_train_trials_all: (17, 14, 38400)\n",
      "  y_train_trials_all: (17,) (array([0, 1], dtype=int32), array([9, 8]))\n",
      "  X_test_trials_all : (1, 14, 38400)\n",
      "  y_test_trials_all : (1,) (array([1], dtype=int32), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 4: Apply preprocessing (LOSO)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 4] Starting preprocessing...\")\n",
    "\n",
    "USE_WICA = True\n",
    "print(f\"[INFO] USE_WICA = {USE_WICA}\")\n",
    "\n",
    "X_train_trials_all, y_train_trials_all = [], []\n",
    "X_test_trials_all, y_test_trials_all = [], []\n",
    "\n",
    "def preprocess_trials(X_trials, y_trials):\n",
    "    cleaned_list = []\n",
    "    label_list = []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        trial = X_trials[i]\n",
    "        label = int(y_trials[i])\n",
    "\n",
    "        cleaned, fs_out = preprocess_eeg(\n",
    "            eeg=trial,\n",
    "            sfreq=SFREQ,\n",
    "            use_wica=USE_WICA\n",
    "        )\n",
    "\n",
    "        cleaned_list.append(cleaned.astype(np.float32, copy=False))\n",
    "        label_list.append(label)\n",
    "\n",
    "    return cleaned_list, np.array(label_list, dtype=np.int32), fs_out\n",
    "\n",
    "# Train subjects\n",
    "for idx, subj in enumerate(train_subjects, start=1):\n",
    "    X_trials, y_trials = subject_data[subj]\n",
    "    cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "    X_train_trials_all.extend(cleaned_trials)\n",
    "    y_train_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "    print(f\"[STEP 4] Train {idx:02d}/{len(train_subjects)}: {subj} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Test subject\n",
    "X_trials, y_trials = subject_data[TEST_SUBJECT]\n",
    "cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "X_test_trials_all.extend(cleaned_trials)\n",
    "y_test_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "print(f\"[STEP 4] Test subject: {TEST_SUBJECT} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Convert to arrays\n",
    "X_train_trials_all = np.array(X_train_trials_all, dtype=np.float32)\n",
    "y_train_trials_all = np.array(y_train_trials_all, dtype=np.int32)\n",
    "\n",
    "X_test_trials_all = np.array(X_test_trials_all, dtype=np.float32)\n",
    "y_test_trials_all = np.array(y_test_trials_all, dtype=np.int32)\n",
    "\n",
    "print(\"[STEP 4] Preprocessing done ✅\")\n",
    "print(\"  X_train_trials_all:\", X_train_trials_all.shape)\n",
    "print(\"  y_train_trials_all:\", y_train_trials_all.shape, np.unique(y_train_trials_all, return_counts=True))\n",
    "print(\"  X_test_trials_all :\", X_test_trials_all.shape)\n",
    "print(\"  y_test_trials_all :\", y_test_trials_all.shape, np.unique(y_test_trials_all, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc3da1-2967-4ee3-84cd-bfcf0f98c0b1",
   "metadata": {},
   "source": [
    "Cell 5 — Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c21ad54-7001-4039-9360-29dfa2905127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 5] Starting segmentation...\n",
      "[STEP 5] Segmented 5/17 trials...\n",
      "[STEP 5] Segmented 10/17 trials...\n",
      "[STEP 5] Segmented 15/17 trials...\n",
      "[STEP 5] Segmented 17/17 trials...\n",
      "[STEP 5] Segmented 1/1 trials...\n",
      "[STEP 5] Segmentation done ✅\n",
      "  X_train: (5100, 14, 128, 1)\n",
      "  y_train: (5100,) (array([0, 1], dtype=int32), array([2700, 2400]))\n",
      "  X_test : (300, 14, 128, 1)\n",
      "  y_test : (300,) (array([1], dtype=int32), array([300]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 5: Segmentation\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 5] Starting segmentation...\")\n",
    "\n",
    "def segment_trial(trial_2d: np.ndarray, label: int, segment_len: int = 128):\n",
    "    C, T = trial_2d.shape\n",
    "    n_segs = T // segment_len\n",
    "    use_T = n_segs * segment_len\n",
    "\n",
    "    if n_segs == 0:\n",
    "        return np.empty((0, C, segment_len), dtype=np.float32), np.empty((0,), dtype=np.int32)\n",
    "\n",
    "    trial_trim = trial_2d[:, :use_T]\n",
    "    Xsegs = trial_trim.reshape(C, n_segs, segment_len).transpose(1, 0, 2)\n",
    "    ysegs = np.full((n_segs,), int(label), dtype=np.int32)\n",
    "    return Xsegs.astype(np.float32, copy=False), ysegs\n",
    "\n",
    "def segment_dataset(X_trials: np.ndarray, y_trials: np.ndarray, segment_len: int = 128):\n",
    "    X_all, y_all = [], []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        Xsegs, ysegs = segment_trial(X_trials[i], int(y_trials[i]), segment_len=segment_len)\n",
    "        X_all.append(Xsegs)\n",
    "        y_all.append(ysegs)\n",
    "\n",
    "        if (i + 1) % 5 == 0 or (i + 1) == X_trials.shape[0]:\n",
    "            print(f\"[STEP 5] Segmented {i+1}/{X_trials.shape[0]} trials...\")\n",
    "\n",
    "    X_out = np.concatenate(X_all, axis=0)\n",
    "    y_out = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    X_out = np.expand_dims(X_out, axis=-1)  # (N,14,128,1)\n",
    "    return X_out.astype(np.float32), y_out.astype(np.int32)\n",
    "\n",
    "X_train, y_train = segment_dataset(X_train_trials_all, y_train_trials_all, segment_len=128)\n",
    "X_test,  y_test  = segment_dataset(X_test_trials_all,  y_test_trials_all,  segment_len=128)\n",
    "\n",
    "print(\"[STEP 5] Segmentation done ✅\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  y_train:\", y_train.shape, np.unique(y_train, return_counts=True))\n",
    "print(\"  X_test :\", X_test.shape)\n",
    "print(\"  y_test :\", y_test.shape, np.unique(y_test, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b4ffd5-bf2f-490e-8854-0a2fc747f1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e69ba7-3952-47b1-bd80-f05237b2e018",
   "metadata": {},
   "source": [
    "Cell 6 — Split train/val + Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97fda4ae-e1c8-4fd1-bf0c-c82ae2a85764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 6] Normalizing using ALL training data (no train/val split)...\n",
      "[STEP 6] Normalization done ✅\n",
      "  X_train_norm: (5100, 14, 128, 1)\n",
      "  X_test_norm : (300, 14, 128, 1)\n",
      "  y_train_norm: (5100,)\n",
      "  y_test_norm : (300,)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 6: Normalize (ALL training data)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 6] Normalizing using ALL training data (no train/val split)...\")\n",
    "\n",
    "def normalize_eeg(train_data: np.ndarray, test_data: np.ndarray):\n",
    "    if train_data.ndim == 5:\n",
    "        train_flat = train_data.reshape(-1, *train_data.shape[2:])\n",
    "        test_flat  = test_data.reshape(-1, *test_data.shape[2:])\n",
    "    else:\n",
    "        train_flat, test_flat = train_data, test_data\n",
    "\n",
    "    # Compute stats on ALL training data\n",
    "    train_mean = np.mean(train_flat, axis=(0, 2, 3), keepdims=True)\n",
    "    train_std  = np.std(train_flat,  axis=(0, 2, 3), keepdims=True) + 1e-8\n",
    "\n",
    "    train_norm = (train_flat - train_mean) / train_std\n",
    "    test_norm  = (test_flat  - train_mean) / train_std\n",
    "\n",
    "    if train_data.ndim == 5:\n",
    "        train_norm = train_norm.reshape(train_data.shape)\n",
    "        test_norm  = test_norm.reshape(test_data.shape)\n",
    "\n",
    "    return (\n",
    "        train_norm.astype(np.float32),\n",
    "        test_norm.astype(np.float32),\n",
    "        train_mean,\n",
    "        train_std\n",
    "    )\n",
    "\n",
    "# Normalize ALL training data\n",
    "X_train_norm, X_test_norm, mean, std = normalize_eeg(X_train, X_test)\n",
    "\n",
    "y_train_norm = y_train.astype(np.float32)\n",
    "y_test_norm  = y_test.astype(np.float32)\n",
    "\n",
    "print(\"[STEP 6] Normalization done ✅\")\n",
    "print(\"  X_train_norm:\", X_train_norm.shape)\n",
    "print(\"  X_test_norm :\", X_test_norm.shape)\n",
    "print(\"  y_train_norm:\", y_train_norm.shape)\n",
    "print(\"  y_test_norm :\", y_test_norm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f527c-34db-4c23-bdbd-c8ce03b519f0",
   "metadata": {},
   "source": [
    "Cell 7 — EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ea4279-53c2-49c1-b834-7b1e1c20775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\DL\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Xtr: (5100, 14, 128, 1), ytr: (5100,)\n",
      "[INFO] Test / LOSO subject is NOT used here.\n",
      "[SPLIT] Train: (4080, 14, 128, 1), Val: (1020, 14, 128, 1)\n",
      "Epoch 1/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 135ms/step - accuracy: 0.5037 - loss: 0.7391 - val_accuracy: 0.5294 - val_loss: 0.6914 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 125ms/step - accuracy: 0.5475 - loss: 0.6851 - val_accuracy: 0.5294 - val_loss: 0.6910 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.5782 - loss: 0.6689 - val_accuracy: 0.5324 - val_loss: 0.6903 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.6287 - loss: 0.6380 - val_accuracy: 0.5382 - val_loss: 0.6882 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.6417 - loss: 0.6164 - val_accuracy: 0.5520 - val_loss: 0.6835 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.6591 - loss: 0.5989 - val_accuracy: 0.5539 - val_loss: 0.6732 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.6914 - loss: 0.5721 - val_accuracy: 0.5676 - val_loss: 0.6650 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7027 - loss: 0.5507 - val_accuracy: 0.5814 - val_loss: 0.6563 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.7118 - loss: 0.5328 - val_accuracy: 0.6020 - val_loss: 0.6424 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.7346 - loss: 0.5147 - val_accuracy: 0.6373 - val_loss: 0.6183 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.7461 - loss: 0.4934 - val_accuracy: 0.6196 - val_loss: 0.6212 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.7588 - loss: 0.4771 - val_accuracy: 0.6412 - val_loss: 0.6149 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.7740 - loss: 0.4574 - val_accuracy: 0.7039 - val_loss: 0.5674 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.7679 - loss: 0.4595 - val_accuracy: 0.7186 - val_loss: 0.5507 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.7762 - loss: 0.4492 - val_accuracy: 0.7127 - val_loss: 0.5444 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.7892 - loss: 0.4382 - val_accuracy: 0.7059 - val_loss: 0.5369 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8176 - loss: 0.3878 - val_accuracy: 0.7020 - val_loss: 0.5272 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8292 - loss: 0.3752 - val_accuracy: 0.7069 - val_loss: 0.5073 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.8348 - loss: 0.3479 - val_accuracy: 0.7314 - val_loss: 0.4803 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.8373 - loss: 0.3558 - val_accuracy: 0.7127 - val_loss: 0.4871 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.8537 - loss: 0.3460 - val_accuracy: 0.7549 - val_loss: 0.4613 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8610 - loss: 0.3308 - val_accuracy: 0.7284 - val_loss: 0.5063 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8703 - loss: 0.3024 - val_accuracy: 0.7255 - val_loss: 0.5147 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8924 - loss: 0.2785 - val_accuracy: 0.7549 - val_loss: 0.4679 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8968 - loss: 0.2644 - val_accuracy: 0.7206 - val_loss: 0.5322 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8934 - loss: 0.2680 - val_accuracy: 0.7520 - val_loss: 0.5048 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8973 - loss: 0.2564 - val_accuracy: 0.7833 - val_loss: 0.4023 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9127 - loss: 0.2242 - val_accuracy: 0.8186 - val_loss: 0.3408 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9056 - loss: 0.2317 - val_accuracy: 0.8373 - val_loss: 0.3443 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9061 - loss: 0.2473 - val_accuracy: 0.8922 - val_loss: 0.2720 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9103 - loss: 0.2304 - val_accuracy: 0.8873 - val_loss: 0.2645 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9169 - loss: 0.2196 - val_accuracy: 0.9176 - val_loss: 0.2214 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9098 - loss: 0.2266 - val_accuracy: 0.9088 - val_loss: 0.2402 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9078 - loss: 0.2323 - val_accuracy: 0.9196 - val_loss: 0.2033 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9103 - loss: 0.2302 - val_accuracy: 0.9206 - val_loss: 0.2096 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9165 - loss: 0.2247\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9142 - loss: 0.2237 - val_accuracy: 0.9314 - val_loss: 0.1925 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9169 - loss: 0.2159 - val_accuracy: 0.9480 - val_loss: 0.1606 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9179 - loss: 0.1956 - val_accuracy: 0.9520 - val_loss: 0.1453 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 128ms/step - accuracy: 0.9238 - loss: 0.2035 - val_accuracy: 0.9451 - val_loss: 0.1490 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 128ms/step - accuracy: 0.9275 - loss: 0.1882 - val_accuracy: 0.9549 - val_loss: 0.1316 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.9225 - loss: 0.1967 - val_accuracy: 0.9539 - val_loss: 0.1302 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9311 - loss: 0.1823 - val_accuracy: 0.9539 - val_loss: 0.1308 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9265 - loss: 0.1884 - val_accuracy: 0.9569 - val_loss: 0.1252 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.9284 - loss: 0.1883 - val_accuracy: 0.9598 - val_loss: 0.1119 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.9316 - loss: 0.1723 - val_accuracy: 0.9598 - val_loss: 0.1213 - learning_rate: 5.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 128ms/step - accuracy: 0.9297 - loss: 0.1840 - val_accuracy: 0.9637 - val_loss: 0.1077 - learning_rate: 5.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 127ms/step - accuracy: 0.9306 - loss: 0.1776 - val_accuracy: 0.9657 - val_loss: 0.0985 - learning_rate: 5.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 125ms/step - accuracy: 0.9353 - loss: 0.1772 - val_accuracy: 0.9667 - val_loss: 0.1025 - learning_rate: 5.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9320 - loss: 0.1832\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.9348 - loss: 0.1747 - val_accuracy: 0.9667 - val_loss: 0.1015 - learning_rate: 5.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9282 - loss: 0.1773 - val_accuracy: 0.9657 - val_loss: 0.0994 - learning_rate: 2.5000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 129ms/step - accuracy: 0.9328 - loss: 0.1657 - val_accuracy: 0.9657 - val_loss: 0.1007 - learning_rate: 2.5000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.9365 - loss: 0.1721 - val_accuracy: 0.9578 - val_loss: 0.1111 - learning_rate: 2.5000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 152ms/step - accuracy: 0.9417 - loss: 0.1620 - val_accuracy: 0.9637 - val_loss: 0.1045 - learning_rate: 2.5000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 144ms/step - accuracy: 0.9390 - loss: 0.1698 - val_accuracy: 0.9627 - val_loss: 0.1041 - learning_rate: 2.5000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 140ms/step - accuracy: 0.9373 - loss: 0.1632 - val_accuracy: 0.9627 - val_loss: 0.0968 - learning_rate: 2.5000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 136ms/step - accuracy: 0.9336 - loss: 0.1659 - val_accuracy: 0.9647 - val_loss: 0.0985 - learning_rate: 2.5000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9341 - loss: 0.1620\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 132ms/step - accuracy: 0.9360 - loss: 0.1638 - val_accuracy: 0.9627 - val_loss: 0.0956 - learning_rate: 2.5000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 135ms/step - accuracy: 0.9382 - loss: 0.1646 - val_accuracy: 0.9667 - val_loss: 0.0942 - learning_rate: 1.2500e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.9387 - loss: 0.1619 - val_accuracy: 0.9725 - val_loss: 0.0900 - learning_rate: 1.2500e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 133ms/step - accuracy: 0.9375 - loss: 0.1682 - val_accuracy: 0.9745 - val_loss: 0.0895 - learning_rate: 1.2500e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9392 - loss: 0.1617 - val_accuracy: 0.9647 - val_loss: 0.0921 - learning_rate: 1.2500e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9336 - loss: 0.1810 - val_accuracy: 0.9686 - val_loss: 0.0927 - learning_rate: 1.2500e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9409 - loss: 0.1567 - val_accuracy: 0.9696 - val_loss: 0.0914 - learning_rate: 1.2500e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9400 - loss: 0.1535 - val_accuracy: 0.9676 - val_loss: 0.0917 - learning_rate: 1.2500e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9370 - loss: 0.1577 - val_accuracy: 0.9676 - val_loss: 0.0841 - learning_rate: 1.2500e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9365 - loss: 0.1647 - val_accuracy: 0.9716 - val_loss: 0.0876 - learning_rate: 1.2500e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9370 - loss: 0.1595 - val_accuracy: 0.9667 - val_loss: 0.0909 - learning_rate: 1.2500e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9370 - loss: 0.1644\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.9326 - loss: 0.1674 - val_accuracy: 0.9696 - val_loss: 0.0891 - learning_rate: 1.2500e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.9385 - loss: 0.1635 - val_accuracy: 0.9686 - val_loss: 0.0873 - learning_rate: 6.2500e-05\n",
      "Epoch 70/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.9380 - loss: 0.1600 - val_accuracy: 0.9706 - val_loss: 0.0855 - learning_rate: 6.2500e-05\n",
      "Epoch 71/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 125ms/step - accuracy: 0.9346 - loss: 0.1681 - val_accuracy: 0.9716 - val_loss: 0.0860 - learning_rate: 6.2500e-05\n",
      "Epoch 72/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9347 - loss: 0.1691\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9314 - loss: 0.1664 - val_accuracy: 0.9696 - val_loss: 0.0860 - learning_rate: 6.2500e-05\n",
      "Epoch 73/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9422 - loss: 0.1499 - val_accuracy: 0.9696 - val_loss: 0.0857 - learning_rate: 3.1250e-05\n",
      "Epoch 74/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9368 - loss: 0.1618 - val_accuracy: 0.9686 - val_loss: 0.0858 - learning_rate: 3.1250e-05\n",
      "Epoch 75/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9360 - loss: 0.1646 - val_accuracy: 0.9696 - val_loss: 0.0852 - learning_rate: 3.1250e-05\n",
      "Epoch 76/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9390 - loss: 0.1568 - val_accuracy: 0.9676 - val_loss: 0.0844 - learning_rate: 3.1250e-05\n",
      "Epoch 77/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9424 - loss: 0.1664\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.9402 - loss: 0.1641 - val_accuracy: 0.9696 - val_loss: 0.0851 - learning_rate: 3.1250e-05\n",
      "Epoch 78/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.9471 - loss: 0.1425 - val_accuracy: 0.9686 - val_loss: 0.0850 - learning_rate: 1.5625e-05\n",
      "Epoch 79/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9429 - loss: 0.1536 - val_accuracy: 0.9686 - val_loss: 0.0851 - learning_rate: 1.5625e-05\n",
      "Epoch 80/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9451 - loss: 0.1496 - val_accuracy: 0.9696 - val_loss: 0.0844 - learning_rate: 1.5625e-05\n",
      "Epoch 80: early stopping\n",
      "Restoring model weights from the end of the best epoch: 65.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] Model saved as cole-final.h5\n",
      "\n",
      "[CONFUSION MATRIX]\n",
      "[[531   9]\n",
      " [ 24 456]]\n",
      "[VAL] acc=0.9676, prec=0.9806, rec=0.9500, f1=0.9651\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 4: 80:20 Train / Validation Split (EEGNet)\n",
    "# ====================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, SeparableConv2D, BatchNormalization, Activation,\n",
    "    DepthwiseConv2D, Conv2D, AveragePooling2D,\n",
    "    Dropout, Dense, Flatten\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# --------------------------------------------------------------------\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# EEGNet model\n",
    "# --------------------------------------------------------------------\n",
    "def create_eegnet_model(input_shape, dropout_rate=0.5, num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(16, (1, 64), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = DepthwiseConv2D(\n",
    "        (input_shape[0], 1),\n",
    "        depth_multiplier=2,\n",
    "        padding='valid',\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 4))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = SeparableConv2D(16, (1, 16), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 8))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Compile model\n",
    "# --------------------------------------------------------------------\n",
    "def compile_model():\n",
    "    input_shape = (Xtr.shape[1], Xtr.shape[2], Xtr.shape[3])\n",
    "    model = create_eegnet_model(input_shape)\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-3),\n",
    "        loss=BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[BinaryAccuracy(name=\"accuracy\", threshold=0.5)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def make_callbacks():\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"loss\",\n",
    "            factor=0.5,\n",
    "            patience=4,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Use ALL normalized training data\n",
    "# --------------------------------------------------------------------\n",
    "Xtr = X_train_norm\n",
    "ytr = y_train_norm\n",
    "\n",
    "print(f\"[TRAIN] Xtr: {Xtr.shape}, ytr: {ytr.shape}\")\n",
    "print(\"[INFO] Test / LOSO subject is NOT used here.\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 80:20 Stratified Split\n",
    "# --------------------------------------------------------------------\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(sss.split(Xtr, ytr))\n",
    "\n",
    "X_train, X_val = Xtr[train_idx], Xtr[val_idx]\n",
    "y_train, y_val = ytr[train_idx], ytr[val_idx]\n",
    "\n",
    "print(f\"[SPLIT] Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Train model\n",
    "# --------------------------------------------------------------------\n",
    "model = compile_model()\n",
    "callbacks = make_callbacks()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Save final model\n",
    "# --------------------------------------------------------------------\n",
    "model.save(\"roddy-final.h5\")\n",
    "print(\"[SAVE] Model saved as cole-final.h5\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Validation Confusion Matrix\n",
    "# --------------------------------------------------------------------\n",
    "y_prob = model.predict(X_val, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "y_true = y_val.astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "eps = 1e-12\n",
    "\n",
    "acc  = (tp + tn) / max(tp + tn + fp + fn, eps)\n",
    "prec = tp / max(tp + fp, eps)\n",
    "rec  = tp / max(tp + fn, eps)\n",
    "f1   = 2 * prec * rec / max(prec + rec, eps)\n",
    "\n",
    "print(\"\\n[CONFUSION MATRIX]\")\n",
    "print(cm)\n",
    "print(f\"[VAL] acc={acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e2a13-6367-41ca-be66-a43594c2940e",
   "metadata": {},
   "source": [
    "Cell 8 — Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ac6662-33c1-4e25-b6c5-d47c860fba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Preparing TEST data...\n",
      "[TEST] Xte shape: (300, 14, 128, 1)\n",
      "[TEST] yte shape: (300,), class distribution: (array([1]), array([300]))\n",
      "[TEST] Loading final model: roddy-final.h5\n",
      "[TEST] Running inference on TEST subject...\n",
      "[TEST] Mean predicted probability: 0.6898\n",
      "\n",
      "[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[  0   0]\n",
      " [ 73 227]]\n",
      "Accuracy : 0.7567\n",
      "Precision: 1.0000\n",
      "Recall   : 0.7567\n",
      "F1-score : 0.8615\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Inference on TEST subject (adam) using FINAL EEGNet\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Prepare TEST data\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Preparing TEST data...\")\n",
    "\n",
    "Xte = X_test_norm\n",
    "yte = y_test_norm.astype(int)\n",
    "\n",
    "print(f\"[TEST] Xte shape: {Xte.shape}\")\n",
    "print(f\"[TEST] yte shape: {yte.shape}, class distribution: {np.unique(yte, return_counts=True)}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Load FINAL trained model\n",
    "# ------------------------------------------------------------------\n",
    "model_path = \"roddy-final.h5\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        \"Final model 'cole-final.h5' not found. \"\n",
    "        \"Please run Cell 4 (80:20 training) first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[TEST] Loading final model: {model_path}\")\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Run inference on TEST subject\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Running inference on TEST subject...\")\n",
    "\n",
    "y_prob = model.predict(Xte, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(f\"[TEST] Mean predicted probability: {y_prob.mean():.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Metrics on TEST subject (segment-level)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc = accuracy_score(yte, y_pred)\n",
    "prec = precision_score(yte, y_pred, zero_division=0)\n",
    "rec = recall_score(yte, y_pred, zero_division=0)\n",
    "f1 = f1_score(yte, y_pred, zero_division=0)\n",
    "\n",
    "print(\"\\n[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572dbf3e-bbb5-4865-883f-30e374700949",
   "metadata": {},
   "source": [
    "Cell 9 - Performance Matrices for each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a57240-50ea-4736-a7ac-52ef7878b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] Verifying predictions and labels are available...\n",
      "[CHECK] Found predictions for 300 test segments.\n",
      "\n",
      "[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[  0   0]\n",
      " [ 73 227]]\n",
      "Accuracy : 0.7567\n",
      "Precision: 1.0000\n",
      "Recall   : 0.7567\n",
      "F1-score : 0.8615\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Final model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================\n",
    "# Cell 6: Confusion Matrix & Metrics on TEST (FINAL EEGNet)\n",
    "# =======================================================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# ------------------------------------------------------------------\n",
    "def compute_metrics_from_cm(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    eps = 1e-12\n",
    "    acc  = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec  = tp / (tp + fn + eps)\n",
    "    f1   = 2 * prec * rec / (prec + rec + eps)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], ['0', '1'])\n",
    "    plt.yticks([0, 1], ['0', '1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(\n",
    "                j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Validate upstream artifacts\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[CHECK] Verifying predictions and labels are available...\")\n",
    "\n",
    "if 'y_pred' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Predictions 'y_pred' not found. \"\n",
    "        \"Please run Cell 5 (final model inference) first.\"\n",
    "    )\n",
    "\n",
    "if 'yte' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Test labels 'yte' not found. \"\n",
    "        \"Please run Cell 5 first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[CHECK] Found predictions for {len(y_pred)} test segments.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Confusion Matrix & Metrics (FINAL model)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc, prec, rec, f1 = compute_metrics_from_cm(cm)\n",
    "\n",
    "print(\"\\n[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(cm, title=\"Confusion Matrix - FINAL EEGNet\")\n",
    "\n",
    "print(\"[DONE] Final model evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0f120-39fd-497e-9098-05e6c82f4807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
