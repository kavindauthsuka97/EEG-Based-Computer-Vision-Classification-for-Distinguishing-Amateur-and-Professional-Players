{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af5b471-cea9-4194-bc23-da5cdc2f9c67",
   "metadata": {},
   "source": [
    "Cell 1 — Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64aaa897-ab61-4eb4-a558-4c078acf751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Base path set to:\n",
      "C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\n",
      "[INFO] Found 18 folders:\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'James', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Selected test subject (LOSO): James\n",
      "[INFO] Training subjects (17):\n",
      "['Amin', 'Amin1', 'Cole', 'Daniel', 'Ismayil', 'Jack', 'Josh', 'Marjan', 'Max', 'Mina', 'Mina 1', 'Mina 3', 'Mohammad', 'Mona', 'Roddy', 'Sam', 'adam']\n",
      "[INFO] Sampling rate (SFREQ) = 128.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 1: Setup & discovery\n",
    "# =========================\n",
    "\n",
    "import os  # file ops\n",
    "import glob  # file matching\n",
    "import numpy as np  # arrays\n",
    "import random  # seeds\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data path\n",
    "BASE_PATH = r\"C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\UOW Projects\\Distinguishing Amateur Players and Professional Players\\Data\"\n",
    "print(f\"[INFO] Base path set to:\\n{BASE_PATH}\")\n",
    "\n",
    "# List subject folders\n",
    "all_folders = [p for p in sorted(os.listdir(BASE_PATH)) if os.path.isdir(os.path.join(BASE_PATH, p))]\n",
    "print(f\"[INFO] Found {len(all_folders)} folders:\\n{all_folders}\")\n",
    "\n",
    "# Expected count (change if needed)\n",
    "assert len(all_folders) == 18, f\"Expected 18 folders, found {len(all_folders)}.\"\n",
    "\n",
    "# LOSO test subject\n",
    "TEST_SUBJECT = \"James\"\n",
    "print(f\"[INFO] Selected test subject (LOSO): {TEST_SUBJECT}\")\n",
    "\n",
    "# Train subjects = remaining\n",
    "train_subjects = [s for s in all_folders if s.lower() != TEST_SUBJECT.lower()]\n",
    "print(f\"[INFO] Training subjects ({len(train_subjects)}):\\n{train_subjects}\")\n",
    "\n",
    "# Sampling rate\n",
    "SFREQ = 128.0\n",
    "print(f\"[INFO] Sampling rate (SFREQ) = {SFREQ} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e649681-2861-4ce2-bbd1-0d9dd3359f42",
   "metadata": {},
   "source": [
    "Cell 2 — Load EEG + labels per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943c5c6e-a7aa-4618-899c-daf341c25358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 2] Loading data from folders...\n",
      "[STEP 2]       Amin | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Amin | EEG=          Amin.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Amin1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]      Amin1 | EEG=         Amin1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Cole | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Cole | EEG=          Cole.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Daniel | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]     Daniel | EEG=        Daniel.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]    Ismayil | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]    Ismayil | EEG=       Ismayil.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Jack | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Jack | EEG=          Jack.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]      James | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      James | EEG=         James.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Josh | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       Josh | EEG=          Josh.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]     Marjan | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Marjan | EEG=        Marjan.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]        Max | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Max | EEG=           Max.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       Mina | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mina | EEG=          Mina.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 1 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 1 | EEG=        Mina 1.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]     Mina 3 | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]     Mina 3 | EEG=        Mina 3.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]   Mohammad | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]   Mohammad | EEG=      Mohammad.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]       Mona | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\n",
      "[STEP 2]       Mona | EEG=          Mona.npy | X=(1, 14, 38400) | y=(1,) | class=0\n",
      "[STEP 2]      Roddy | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]      Roddy | EEG=         Roddy.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]        Sam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]        Sam | EEG=           Sam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2]       adam | label raw: shape=(1,), value=[1] -> class=1\n",
      "[STEP 2]       adam | EEG=          adam.npy | X=(1, 14, 38400) | y=(1,) | class=1\n",
      "[STEP 2] Loading complete ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 2: Load data (EMPTY LABEL => CLASS 0)\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path  # helper\n",
    "\n",
    "def load_subject(subj_dir: str, subj_name: str, expected_channels: int = 14):\n",
    "    \"\"\"\n",
    "    EEG file: {subj_name}.npy\n",
    "    Label file: {subj_name}_target.npy\n",
    "    Label rule:\n",
    "      - empty array -> class 0\n",
    "      - [0] -> class 0\n",
    "      - [1] -> class 1\n",
    "    \"\"\"\n",
    "\n",
    "    # EEG file path\n",
    "    eeg_file = os.path.join(subj_dir, f\"{subj_name}.npy\")\n",
    "    assert os.path.exists(eeg_file), f\"[ERROR] Missing EEG file: {eeg_file}\"\n",
    "\n",
    "    # Label file path\n",
    "    target_file = os.path.join(subj_dir, f\"{subj_name}_target.npy\")\n",
    "    assert os.path.exists(target_file), f\"[ERROR] Missing label file: {target_file}\"\n",
    "\n",
    "    # Load EEG\n",
    "    X_raw = np.load(eeg_file, allow_pickle=True)\n",
    "    X_raw = np.asarray(X_raw)\n",
    "\n",
    "    # Standardize EEG to (n_trials, 14, T)\n",
    "    if X_raw.ndim == 2:\n",
    "        X_trials = X_raw[None, :, :]\n",
    "    elif X_raw.ndim == 3:\n",
    "        dims = list(X_raw.shape)\n",
    "        chan_axes = [i for i, d in enumerate(dims) if d == expected_channels]\n",
    "        chan_axis = chan_axes[0] if len(chan_axes) == 1 else 1\n",
    "        time_axis = int(np.argmax(dims))\n",
    "        trial_axis = [i for i in range(3) if i not in (chan_axis, time_axis)][0]\n",
    "        X_trials = np.moveaxis(X_raw, (trial_axis, chan_axis, time_axis), (0, 1, 2))\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] Unsupported EEG shape {X_raw.shape} in {eeg_file}\")\n",
    "\n",
    "    assert X_trials.shape[1] == expected_channels, f\"[ERROR] Expected {expected_channels} channels, got {X_trials.shape[1]} in {eeg_file}\"\n",
    "\n",
    "    # Load label\n",
    "    y_raw = np.load(target_file, allow_pickle=True)\n",
    "    y_arr = np.asarray(y_raw).ravel()\n",
    "\n",
    "    # Apply your rule\n",
    "    if y_arr.size == 0:\n",
    "        label = 0\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape=(0,), value=[] -> class=0 (EMPTY RULE)\")\n",
    "    else:\n",
    "        label = int(y_arr[0])\n",
    "        if label not in (0, 1):\n",
    "            raise ValueError(f\"[ERROR] Invalid label {label} in {target_file} (must be 0 or 1)\")\n",
    "        print(f\"[STEP 2] {subj_name:>10} | label raw: shape={y_arr.shape}, value={y_arr} -> class={label}\")\n",
    "\n",
    "    # Repeat label for all trials\n",
    "    y_trials = np.full((X_trials.shape[0],), label, dtype=np.int32)\n",
    "\n",
    "    return X_trials.astype(np.float32), y_trials.astype(np.int32), eeg_file, target_file\n",
    "\n",
    "\n",
    "# Load all subjects\n",
    "subject_data = {}\n",
    "print(\"[STEP 2] Loading data from folders...\")\n",
    "\n",
    "for subj in all_folders:\n",
    "    subj_dir = os.path.join(BASE_PATH, subj)\n",
    "    X_trials, y_trials, eeg_file, target_file = load_subject(subj_dir, subj)\n",
    "\n",
    "    subject_data[subj] = (X_trials, y_trials)\n",
    "\n",
    "    print(f\"[STEP 2] {subj:>10} | EEG={Path(eeg_file).name:>18} | X={X_trials.shape} | y={y_trials.shape} | class={y_trials[0]}\")\n",
    "\n",
    "print(\"[STEP 2] Loading complete ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd93ca8-a94c-485c-8a61-e6e7441f6299",
   "metadata": {},
   "source": [
    "Cell 3 — Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733e592c-98e1-4cbd-afd2-6fdc8cf407f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 3] Preprocessing functions ready ✅\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3: Preprocessing functions\n",
    "# =========================\n",
    "\n",
    "import mne  # EEG processing\n",
    "import pywt  # wavelets\n",
    "from sklearn.decomposition import FastICA  # ICA\n",
    "from typing import Optional, Tuple, Union, Sequence, Dict, List  # typing\n",
    "\n",
    "def wavelet_enhanced_ica(\n",
    "    data: np.ndarray,\n",
    "    n_components: int = 10,\n",
    "    wavelet: str = \"db4\",\n",
    "    level: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> np.ndarray:\n",
    "    n_ch, n_t = data.shape\n",
    "    n_components = min(n_components, n_ch)\n",
    "\n",
    "    coeffs = pywt.wavedec(data, wavelet=wavelet, level=level, axis=1)\n",
    "    A = coeffs[0]\n",
    "\n",
    "    ica = FastICA(n_components=n_components, random_state=random_state)\n",
    "    S = ica.fit_transform(A.T).T\n",
    "    A_denoised = ica.inverse_transform(S.T).T\n",
    "\n",
    "    coeffs[0] = A_denoised\n",
    "    cleaned = pywt.waverec(coeffs, wavelet=wavelet, axis=1)\n",
    "\n",
    "    if cleaned.shape[1] != n_t:\n",
    "        if cleaned.shape[1] > n_t:\n",
    "            cleaned = cleaned[:, :n_t]\n",
    "        else:\n",
    "            cleaned = np.pad(cleaned, ((0, 0), (0, n_t - cleaned.shape[1])), mode=\"constant\")\n",
    "    return cleaned\n",
    "\n",
    "def _names_from_index_mapping(n_channels: int, index_to_name: Optional[Dict[int, str]]) -> List[str]:\n",
    "    if index_to_name is None:\n",
    "        return [f\"EEG{i+1}\" for i in range(n_channels)]\n",
    "    keys = list(index_to_name.keys())\n",
    "    is_zero_based = (0 in keys) and (1 not in keys)\n",
    "    names = []\n",
    "    for i in range(n_channels):\n",
    "        key = i if is_zero_based else (i + 1)\n",
    "        names.append(index_to_name.get(key, f\"EEG{i+1}\"))\n",
    "    return names\n",
    "\n",
    "def preprocess_eeg(\n",
    "    eeg: np.ndarray,\n",
    "    sfreq: float,\n",
    "    *,\n",
    "    index_to_name: Optional[Dict[int, str]] = None,\n",
    "    use_standard_1010: bool = True,\n",
    "    resample_to: Optional[float] = None,\n",
    "    notch_freqs: Union[None, float, Sequence[float]] = 50.0,\n",
    "    highpass: Optional[float] = 0.05,\n",
    "    bad_point_z: float = 6.0,\n",
    "    bad_channel_z: float = 5.0,\n",
    "    interpolate_bad_channels: bool = True,\n",
    "    car: bool = True,\n",
    "    use_wica: bool = True,\n",
    "    wica_components: int = 10,\n",
    "    wica_wavelet: str = \"db4\",\n",
    "    wica_level: int = 3,\n",
    "    wica_random_state: int = 42,\n",
    "    return_raw: bool = False,\n",
    "):\n",
    "\n",
    "    eeg = np.asarray(eeg, dtype=np.float32)\n",
    "    assert eeg.ndim == 2, \"eeg must be 2D: (n_channels, n_times)\"\n",
    "\n",
    "    n_channels, _ = eeg.shape\n",
    "    ch_names = _names_from_index_mapping(n_channels, index_to_name)\n",
    "    ch_types = ['eog' if str(n).upper().startswith(\"EOG\") else 'eeg' for n in ch_names]\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    raw = mne.io.RawArray(eeg, info, verbose=False)\n",
    "\n",
    "    montage_applied = False\n",
    "    if use_standard_1010:\n",
    "        try:\n",
    "            mont = mne.channels.make_standard_montage(\"standard_1010\")\n",
    "            raw.set_montage(mont, match_case=False, on_missing=\"ignore\")\n",
    "            montage_applied = True\n",
    "        except Exception:\n",
    "            montage_applied = False\n",
    "\n",
    "    if resample_to is not None and float(resample_to) != float(sfreq):\n",
    "        raw.resample(sfreq=resample_to, npad=\"auto\")\n",
    "    sfreq_out = float(raw.info[\"sfreq\"])\n",
    "\n",
    "    if notch_freqs is not None:\n",
    "        raw.notch_filter(freqs=notch_freqs, verbose=False)\n",
    "    if highpass is not None:\n",
    "        raw.filter(l_freq=highpass, h_freq=None, verbose=False)\n",
    "\n",
    "    X = raw.get_data()\n",
    "    mu = np.mean(X, axis=1, keepdims=True)\n",
    "    sd = np.std(X, axis=1, keepdims=True) + 1e-12\n",
    "    hi = mu + bad_point_z * sd\n",
    "    lo = mu - bad_point_z * sd\n",
    "    bad_idx = (X > hi) | (X < lo)\n",
    "\n",
    "    if np.any(bad_idx):\n",
    "        X_fixed = X.copy()\n",
    "        t = np.arange(X.shape[1], dtype=float)\n",
    "        for ch in range(n_channels):\n",
    "            mask = bad_idx[ch]\n",
    "            if mask.any():\n",
    "                good = ~mask\n",
    "                if good.sum() >= 2:\n",
    "                    X_fixed[ch, mask] = np.interp(t[mask], t[good], X_fixed[ch, good])\n",
    "        raw._data = X_fixed\n",
    "\n",
    "    if interpolate_bad_channels and montage_applied:\n",
    "        X = raw.get_data(picks=\"eeg\")\n",
    "        if X.size > 0:\n",
    "            ch_std = X.std(axis=1)\n",
    "            med = np.median(ch_std)\n",
    "            mad = np.median(np.abs(ch_std - med)) + 1e-12\n",
    "            z = 0.6745 * (ch_std - med) / mad\n",
    "            eeg_names = mne.pick_info(raw.info, mne.pick_types(raw.info, eeg=True)).ch_names\n",
    "            bads = [eeg_names[i] for i in np.where(np.abs(z) > bad_channel_z)[0]]\n",
    "            raw.info[\"bads\"] = bads\n",
    "            if bads:\n",
    "                raw.interpolate_bads(reset_bads=True, verbose=False)\n",
    "\n",
    "    if car:\n",
    "        raw.set_eeg_reference(\"average\", projection=True)\n",
    "        raw.apply_proj()\n",
    "\n",
    "    if use_wica:\n",
    "        cleaned = wavelet_enhanced_ica(\n",
    "            raw.get_data(),\n",
    "            n_components=wica_components,\n",
    "            wavelet=wica_wavelet,\n",
    "            level=wica_level,\n",
    "            random_state=wica_random_state\n",
    "        )\n",
    "        raw = mne.io.RawArray(cleaned, raw.info, verbose=False)\n",
    "\n",
    "    cleaned = raw.get_data()\n",
    "    return cleaned, sfreq_out\n",
    "\n",
    "print(\"[STEP 3] Preprocessing functions ready ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a9d20-889d-45c4-bbc2-de62216908cd",
   "metadata": {},
   "source": [
    "Cell 4 — Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bfcbde0-1cc2-409c-8627-93da46606661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 4] Starting preprocessing...\n",
      "[INFO] USE_WICA = True\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 01/17: Amin | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 02/17: Amin1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 03/17: Cole | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 04/17: Daniel | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 05/17: Ismayil | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 06/17: Jack | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 07/17: Josh | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 08/17: Marjan | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 09/17: Max | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 10/17: Mina | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 11/17: Mina 1 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 12/17: Mina 3 | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 13/17: Mohammad | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 14/17: Mona | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 15/17: Roddy | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 16/17: Sam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Train 17/17: adam | trials=1 | fs_out=128.0\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[STEP 4] Test subject: James | trials=1 | fs_out=128.0\n",
      "[STEP 4] Preprocessing done ✅\n",
      "  X_train_trials_all: (17, 14, 38400)\n",
      "  y_train_trials_all: (17,) (array([0, 1], dtype=int32), array([9, 8]))\n",
      "  X_test_trials_all : (1, 14, 38400)\n",
      "  y_test_trials_all : (1,) (array([1], dtype=int32), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 4: Apply preprocessing (LOSO)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 4] Starting preprocessing...\")\n",
    "\n",
    "USE_WICA = True\n",
    "print(f\"[INFO] USE_WICA = {USE_WICA}\")\n",
    "\n",
    "X_train_trials_all, y_train_trials_all = [], []\n",
    "X_test_trials_all, y_test_trials_all = [], []\n",
    "\n",
    "def preprocess_trials(X_trials, y_trials):\n",
    "    cleaned_list = []\n",
    "    label_list = []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        trial = X_trials[i]\n",
    "        label = int(y_trials[i])\n",
    "\n",
    "        cleaned, fs_out = preprocess_eeg(\n",
    "            eeg=trial,\n",
    "            sfreq=SFREQ,\n",
    "            use_wica=USE_WICA\n",
    "        )\n",
    "\n",
    "        cleaned_list.append(cleaned.astype(np.float32, copy=False))\n",
    "        label_list.append(label)\n",
    "\n",
    "    return cleaned_list, np.array(label_list, dtype=np.int32), fs_out\n",
    "\n",
    "# Train subjects\n",
    "for idx, subj in enumerate(train_subjects, start=1):\n",
    "    X_trials, y_trials = subject_data[subj]\n",
    "    cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "    X_train_trials_all.extend(cleaned_trials)\n",
    "    y_train_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "    print(f\"[STEP 4] Train {idx:02d}/{len(train_subjects)}: {subj} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Test subject\n",
    "X_trials, y_trials = subject_data[TEST_SUBJECT]\n",
    "cleaned_trials, cleaned_labels, fs_out = preprocess_trials(X_trials, y_trials)\n",
    "\n",
    "X_test_trials_all.extend(cleaned_trials)\n",
    "y_test_trials_all.extend(cleaned_labels.tolist())\n",
    "\n",
    "print(f\"[STEP 4] Test subject: {TEST_SUBJECT} | trials={len(cleaned_trials)} | fs_out={fs_out}\")\n",
    "\n",
    "# Convert to arrays\n",
    "X_train_trials_all = np.array(X_train_trials_all, dtype=np.float32)\n",
    "y_train_trials_all = np.array(y_train_trials_all, dtype=np.int32)\n",
    "\n",
    "X_test_trials_all = np.array(X_test_trials_all, dtype=np.float32)\n",
    "y_test_trials_all = np.array(y_test_trials_all, dtype=np.int32)\n",
    "\n",
    "print(\"[STEP 4] Preprocessing done ✅\")\n",
    "print(\"  X_train_trials_all:\", X_train_trials_all.shape)\n",
    "print(\"  y_train_trials_all:\", y_train_trials_all.shape, np.unique(y_train_trials_all, return_counts=True))\n",
    "print(\"  X_test_trials_all :\", X_test_trials_all.shape)\n",
    "print(\"  y_test_trials_all :\", y_test_trials_all.shape, np.unique(y_test_trials_all, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc3da1-2967-4ee3-84cd-bfcf0f98c0b1",
   "metadata": {},
   "source": [
    "Cell 5 — Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c21ad54-7001-4039-9360-29dfa2905127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 5] Starting segmentation...\n",
      "[STEP 5] Segmented 5/17 trials...\n",
      "[STEP 5] Segmented 10/17 trials...\n",
      "[STEP 5] Segmented 15/17 trials...\n",
      "[STEP 5] Segmented 17/17 trials...\n",
      "[STEP 5] Segmented 1/1 trials...\n",
      "[STEP 5] Segmentation done ✅\n",
      "  X_train: (5100, 14, 128, 1)\n",
      "  y_train: (5100,) (array([0, 1], dtype=int32), array([2700, 2400]))\n",
      "  X_test : (300, 14, 128, 1)\n",
      "  y_test : (300,) (array([1], dtype=int32), array([300]))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 5: Segmentation\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 5] Starting segmentation...\")\n",
    "\n",
    "def segment_trial(trial_2d: np.ndarray, label: int, segment_len: int = 128):\n",
    "    C, T = trial_2d.shape\n",
    "    n_segs = T // segment_len\n",
    "    use_T = n_segs * segment_len\n",
    "\n",
    "    if n_segs == 0:\n",
    "        return np.empty((0, C, segment_len), dtype=np.float32), np.empty((0,), dtype=np.int32)\n",
    "\n",
    "    trial_trim = trial_2d[:, :use_T]\n",
    "    Xsegs = trial_trim.reshape(C, n_segs, segment_len).transpose(1, 0, 2)\n",
    "    ysegs = np.full((n_segs,), int(label), dtype=np.int32)\n",
    "    return Xsegs.astype(np.float32, copy=False), ysegs\n",
    "\n",
    "def segment_dataset(X_trials: np.ndarray, y_trials: np.ndarray, segment_len: int = 128):\n",
    "    X_all, y_all = [], []\n",
    "    for i in range(X_trials.shape[0]):\n",
    "        Xsegs, ysegs = segment_trial(X_trials[i], int(y_trials[i]), segment_len=segment_len)\n",
    "        X_all.append(Xsegs)\n",
    "        y_all.append(ysegs)\n",
    "\n",
    "        if (i + 1) % 5 == 0 or (i + 1) == X_trials.shape[0]:\n",
    "            print(f\"[STEP 5] Segmented {i+1}/{X_trials.shape[0]} trials...\")\n",
    "\n",
    "    X_out = np.concatenate(X_all, axis=0)\n",
    "    y_out = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    X_out = np.expand_dims(X_out, axis=-1)  # (N,14,128,1)\n",
    "    return X_out.astype(np.float32), y_out.astype(np.int32)\n",
    "\n",
    "X_train, y_train = segment_dataset(X_train_trials_all, y_train_trials_all, segment_len=128)\n",
    "X_test,  y_test  = segment_dataset(X_test_trials_all,  y_test_trials_all,  segment_len=128)\n",
    "\n",
    "print(\"[STEP 5] Segmentation done ✅\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  y_train:\", y_train.shape, np.unique(y_train, return_counts=True))\n",
    "print(\"  X_test :\", X_test.shape)\n",
    "print(\"  y_test :\", y_test.shape, np.unique(y_test, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b4ffd5-bf2f-490e-8854-0a2fc747f1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e69ba7-3952-47b1-bd80-f05237b2e018",
   "metadata": {},
   "source": [
    "Cell 6 — Split train/val + Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97fda4ae-e1c8-4fd1-bf0c-c82ae2a85764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 6] Normalizing using ALL training data (no train/val split)...\n",
      "[STEP 6] Normalization done ✅\n",
      "  X_train_norm: (5100, 14, 128, 1)\n",
      "  X_test_norm : (300, 14, 128, 1)\n",
      "  y_train_norm: (5100,)\n",
      "  y_test_norm : (300,)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 6: Normalize (ALL training data)\n",
    "# =========================\n",
    "\n",
    "print(\"[STEP 6] Normalizing using ALL training data (no train/val split)...\")\n",
    "\n",
    "def normalize_eeg(train_data: np.ndarray, test_data: np.ndarray):\n",
    "    if train_data.ndim == 5:\n",
    "        train_flat = train_data.reshape(-1, *train_data.shape[2:])\n",
    "        test_flat  = test_data.reshape(-1, *test_data.shape[2:])\n",
    "    else:\n",
    "        train_flat, test_flat = train_data, test_data\n",
    "\n",
    "    # Compute stats on ALL training data\n",
    "    train_mean = np.mean(train_flat, axis=(0, 2, 3), keepdims=True)\n",
    "    train_std  = np.std(train_flat,  axis=(0, 2, 3), keepdims=True) + 1e-8\n",
    "\n",
    "    train_norm = (train_flat - train_mean) / train_std\n",
    "    test_norm  = (test_flat  - train_mean) / train_std\n",
    "\n",
    "    if train_data.ndim == 5:\n",
    "        train_norm = train_norm.reshape(train_data.shape)\n",
    "        test_norm  = test_norm.reshape(test_data.shape)\n",
    "\n",
    "    return (\n",
    "        train_norm.astype(np.float32),\n",
    "        test_norm.astype(np.float32),\n",
    "        train_mean,\n",
    "        train_std\n",
    "    )\n",
    "\n",
    "# Normalize ALL training data\n",
    "X_train_norm, X_test_norm, mean, std = normalize_eeg(X_train, X_test)\n",
    "\n",
    "y_train_norm = y_train.astype(np.float32)\n",
    "y_test_norm  = y_test.astype(np.float32)\n",
    "\n",
    "print(\"[STEP 6] Normalization done ✅\")\n",
    "print(\"  X_train_norm:\", X_train_norm.shape)\n",
    "print(\"  X_test_norm :\", X_test_norm.shape)\n",
    "print(\"  y_train_norm:\", y_train_norm.shape)\n",
    "print(\"  y_test_norm :\", y_test_norm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f527c-34db-4c23-bdbd-c8ce03b519f0",
   "metadata": {},
   "source": [
    "Cell 7 — EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ea4279-53c2-49c1-b834-7b1e1c20775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\DL\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Xtr: (5100, 14, 128, 1), ytr: (5100,)\n",
      "[INFO] Test / LOSO subject is NOT used here.\n",
      "[SPLIT] Train: (4080, 14, 128, 1), Val: (1020, 14, 128, 1)\n",
      "Epoch 1/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 121ms/step - accuracy: 0.4993 - loss: 0.7381 - val_accuracy: 0.5294 - val_loss: 0.6913 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.5674 - loss: 0.6786 - val_accuracy: 0.5304 - val_loss: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.6039 - loss: 0.6602 - val_accuracy: 0.5314 - val_loss: 0.6899 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.6466 - loss: 0.6275 - val_accuracy: 0.5373 - val_loss: 0.6877 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.6608 - loss: 0.6022 - val_accuracy: 0.5520 - val_loss: 0.6808 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.6914 - loss: 0.5728 - val_accuracy: 0.5667 - val_loss: 0.6718 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7120 - loss: 0.5467 - val_accuracy: 0.5863 - val_loss: 0.6610 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7387 - loss: 0.5196 - val_accuracy: 0.5941 - val_loss: 0.6545 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.7439 - loss: 0.5067 - val_accuracy: 0.6255 - val_loss: 0.6249 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.7598 - loss: 0.4890 - val_accuracy: 0.6343 - val_loss: 0.6178 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.7775 - loss: 0.4593 - val_accuracy: 0.6637 - val_loss: 0.6039 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.7902 - loss: 0.4406 - val_accuracy: 0.6667 - val_loss: 0.5854 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8243 - loss: 0.4054 - val_accuracy: 0.7010 - val_loss: 0.5609 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8216 - loss: 0.3915 - val_accuracy: 0.7480 - val_loss: 0.5279 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8186 - loss: 0.4052 - val_accuracy: 0.7941 - val_loss: 0.4822 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8262 - loss: 0.3870 - val_accuracy: 0.7657 - val_loss: 0.4840 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8471 - loss: 0.3547 - val_accuracy: 0.8000 - val_loss: 0.4603 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.8439 - loss: 0.3487 - val_accuracy: 0.8020 - val_loss: 0.4469 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.8500 - loss: 0.3458 - val_accuracy: 0.8373 - val_loss: 0.4113 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.8478 - loss: 0.3461 - val_accuracy: 0.7735 - val_loss: 0.4827 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.8551 - loss: 0.3311 - val_accuracy: 0.8363 - val_loss: 0.3857 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8544 - loss: 0.3261 - val_accuracy: 0.8559 - val_loss: 0.3804 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.8598 - loss: 0.3206 - val_accuracy: 0.8412 - val_loss: 0.3790 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8725 - loss: 0.3063 - val_accuracy: 0.8471 - val_loss: 0.3858 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.8667 - loss: 0.3024 - val_accuracy: 0.8873 - val_loss: 0.3067 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8738 - loss: 0.2974 - val_accuracy: 0.8647 - val_loss: 0.3229 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8760 - loss: 0.3005 - val_accuracy: 0.8765 - val_loss: 0.3093 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8728 - loss: 0.2961 - val_accuracy: 0.8775 - val_loss: 0.2961 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8797 - loss: 0.2958 - val_accuracy: 0.9167 - val_loss: 0.2616 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8868 - loss: 0.2841 - val_accuracy: 0.9059 - val_loss: 0.2754 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8904 - loss: 0.2594 - val_accuracy: 0.9088 - val_loss: 0.2509 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8963 - loss: 0.2481 - val_accuracy: 0.9216 - val_loss: 0.2468 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.8993 - loss: 0.2439 - val_accuracy: 0.9225 - val_loss: 0.2198 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9010 - loss: 0.2533 - val_accuracy: 0.9206 - val_loss: 0.2277 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8826 - loss: 0.2620 - val_accuracy: 0.9451 - val_loss: 0.1906 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9015 - loss: 0.2322 - val_accuracy: 0.9441 - val_loss: 0.1849 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9096 - loss: 0.2267 - val_accuracy: 0.9235 - val_loss: 0.2203 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9091 - loss: 0.2236 - val_accuracy: 0.9422 - val_loss: 0.1888 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9091 - loss: 0.2270 - val_accuracy: 0.9431 - val_loss: 0.1762 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9203 - loss: 0.2037 - val_accuracy: 0.9471 - val_loss: 0.1541 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9164 - loss: 0.2144 - val_accuracy: 0.9431 - val_loss: 0.1755 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9181 - loss: 0.2162 - val_accuracy: 0.9461 - val_loss: 0.1683 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9194 - loss: 0.2025 - val_accuracy: 0.9480 - val_loss: 0.1485 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9208 - loss: 0.2085 - val_accuracy: 0.9529 - val_loss: 0.1392 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9245 - loss: 0.1956 - val_accuracy: 0.9216 - val_loss: 0.2132 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9194 - loss: 0.2007 - val_accuracy: 0.9431 - val_loss: 0.1607 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9240 - loss: 0.1856 - val_accuracy: 0.9637 - val_loss: 0.1128 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9262 - loss: 0.1804 - val_accuracy: 0.9471 - val_loss: 0.1409 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9306 - loss: 0.1796 - val_accuracy: 0.9627 - val_loss: 0.1071 - learning_rate: 0.0010\n",
      "Epoch 50/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9343 - loss: 0.1723 - val_accuracy: 0.9676 - val_loss: 0.0973 - learning_rate: 0.0010\n",
      "Epoch 51/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9326 - loss: 0.1667 - val_accuracy: 0.9382 - val_loss: 0.1641 - learning_rate: 0.0010\n",
      "Epoch 52/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9262 - loss: 0.1857 - val_accuracy: 0.9637 - val_loss: 0.1054 - learning_rate: 0.0010\n",
      "Epoch 53/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9458 - loss: 0.1554 - val_accuracy: 0.9716 - val_loss: 0.0962 - learning_rate: 0.0010\n",
      "Epoch 54/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9397 - loss: 0.1543 - val_accuracy: 0.9559 - val_loss: 0.1163 - learning_rate: 0.0010\n",
      "Epoch 55/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9400 - loss: 0.1585 - val_accuracy: 0.9578 - val_loss: 0.1053 - learning_rate: 0.0010\n",
      "Epoch 56/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9390 - loss: 0.1545 - val_accuracy: 0.9686 - val_loss: 0.0930 - learning_rate: 0.0010\n",
      "Epoch 57/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9441 - loss: 0.1464 - val_accuracy: 0.9716 - val_loss: 0.0922 - learning_rate: 0.0010\n",
      "Epoch 58/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9480 - loss: 0.1402 - val_accuracy: 0.9539 - val_loss: 0.1235 - learning_rate: 0.0010\n",
      "Epoch 59/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9385 - loss: 0.1579 - val_accuracy: 0.9706 - val_loss: 0.1019 - learning_rate: 0.0010\n",
      "Epoch 60/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9422 - loss: 0.1598 - val_accuracy: 0.9451 - val_loss: 0.1437 - learning_rate: 0.0010\n",
      "Epoch 61/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9363 - loss: 0.1713 - val_accuracy: 0.9333 - val_loss: 0.1490 - learning_rate: 0.0010\n",
      "Epoch 62/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.9326 - loss: 0.1758 \n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9348 - loss: 0.1707 - val_accuracy: 0.9598 - val_loss: 0.1060 - learning_rate: 0.0010\n",
      "Epoch 63/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9480 - loss: 0.1358 - val_accuracy: 0.9618 - val_loss: 0.0992 - learning_rate: 5.0000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9556 - loss: 0.1157 - val_accuracy: 0.9696 - val_loss: 0.0755 - learning_rate: 5.0000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9525 - loss: 0.1213 - val_accuracy: 0.9725 - val_loss: 0.0725 - learning_rate: 5.0000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9488 - loss: 0.1282 - val_accuracy: 0.9716 - val_loss: 0.0841 - learning_rate: 5.0000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9525 - loss: 0.1263 - val_accuracy: 0.9765 - val_loss: 0.0742 - learning_rate: 5.0000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9541 - loss: 0.1264\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9500 - loss: 0.1309 - val_accuracy: 0.9706 - val_loss: 0.0831 - learning_rate: 5.0000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9559 - loss: 0.1200 - val_accuracy: 0.9755 - val_loss: 0.0771 - learning_rate: 2.5000e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9525 - loss: 0.1262 - val_accuracy: 0.9765 - val_loss: 0.0705 - learning_rate: 2.5000e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9510 - loss: 0.1260 - val_accuracy: 0.9725 - val_loss: 0.0740 - learning_rate: 2.5000e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9479 - loss: 0.1346\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9500 - loss: 0.1323 - val_accuracy: 0.9775 - val_loss: 0.0680 - learning_rate: 2.5000e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9549 - loss: 0.1137 - val_accuracy: 0.9784 - val_loss: 0.0647 - learning_rate: 1.2500e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9583 - loss: 0.1140 - val_accuracy: 0.9794 - val_loss: 0.0641 - learning_rate: 1.2500e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9539 - loss: 0.1220 - val_accuracy: 0.9794 - val_loss: 0.0648 - learning_rate: 1.2500e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9515 - loss: 0.1222 - val_accuracy: 0.9765 - val_loss: 0.0663 - learning_rate: 1.2500e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9542 - loss: 0.1158\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9559 - loss: 0.1162 - val_accuracy: 0.9804 - val_loss: 0.0639 - learning_rate: 1.2500e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9578 - loss: 0.1063 - val_accuracy: 0.9775 - val_loss: 0.0637 - learning_rate: 6.2500e-05\n",
      "Epoch 79/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9598 - loss: 0.1106 - val_accuracy: 0.9794 - val_loss: 0.0622 - learning_rate: 6.2500e-05\n",
      "Epoch 80/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9532 - loss: 0.1174 - val_accuracy: 0.9775 - val_loss: 0.0643 - learning_rate: 6.2500e-05\n",
      "Epoch 81/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9549 - loss: 0.1143 - val_accuracy: 0.9755 - val_loss: 0.0660 - learning_rate: 6.2500e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9619 - loss: 0.1177\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9613 - loss: 0.1132 - val_accuracy: 0.9755 - val_loss: 0.0649 - learning_rate: 6.2500e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9581 - loss: 0.1116 - val_accuracy: 0.9755 - val_loss: 0.0668 - learning_rate: 3.1250e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9642 - loss: 0.1060 - val_accuracy: 0.9745 - val_loss: 0.0652 - learning_rate: 3.1250e-05\n",
      "Epoch 85/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 144ms/step - accuracy: 0.9547 - loss: 0.1178 - val_accuracy: 0.9745 - val_loss: 0.0658 - learning_rate: 3.1250e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9527 - loss: 0.1230 - val_accuracy: 0.9735 - val_loss: 0.0666 - learning_rate: 3.1250e-05\n",
      "Epoch 87/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9569 - loss: 0.1155 - val_accuracy: 0.9735 - val_loss: 0.0657 - learning_rate: 3.1250e-05\n",
      "Epoch 88/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9588 - loss: 0.1145\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9588 - loss: 0.1131 - val_accuracy: 0.9745 - val_loss: 0.0645 - learning_rate: 3.1250e-05\n",
      "Epoch 89/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9596 - loss: 0.1050 - val_accuracy: 0.9735 - val_loss: 0.0651 - learning_rate: 1.5625e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.9610 - loss: 0.0992 - val_accuracy: 0.9765 - val_loss: 0.0651 - learning_rate: 1.5625e-05\n",
      "Epoch 91/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9596 - loss: 0.1089 - val_accuracy: 0.9745 - val_loss: 0.0647 - learning_rate: 1.5625e-05\n",
      "Epoch 92/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9596 - loss: 0.1076 - val_accuracy: 0.9745 - val_loss: 0.0648 - learning_rate: 1.5625e-05\n",
      "Epoch 93/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9588 - loss: 0.1127 - val_accuracy: 0.9765 - val_loss: 0.0658 - learning_rate: 1.5625e-05\n",
      "Epoch 94/200\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9582 - loss: 0.1207\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9581 - loss: 0.1175 - val_accuracy: 0.9755 - val_loss: 0.0656 - learning_rate: 1.5625e-05\n",
      "Epoch 94: early stopping\n",
      "Restoring model weights from the end of the best epoch: 79.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] Model saved as cole-final.h5\n",
      "\n",
      "[CONFUSION MATRIX]\n",
      "[[534   6]\n",
      " [ 15 465]]\n",
      "[VAL] acc=0.9794, prec=0.9873, rec=0.9688, f1=0.9779\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 4: 80:20 Train / Validation Split (EEGNet)\n",
    "# ====================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, SeparableConv2D, BatchNormalization, Activation,\n",
    "    DepthwiseConv2D, Conv2D, AveragePooling2D,\n",
    "    Dropout, Dense, Flatten\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# --------------------------------------------------------------------\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# EEGNet model\n",
    "# --------------------------------------------------------------------\n",
    "def create_eegnet_model(input_shape, dropout_rate=0.5, num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(16, (1, 64), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = DepthwiseConv2D(\n",
    "        (input_shape[0], 1),\n",
    "        depth_multiplier=2,\n",
    "        padding='valid',\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 4))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = SeparableConv2D(16, (1, 16), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 8))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Compile model\n",
    "# --------------------------------------------------------------------\n",
    "def compile_model():\n",
    "    input_shape = (Xtr.shape[1], Xtr.shape[2], Xtr.shape[3])\n",
    "    model = create_eegnet_model(input_shape)\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-3),\n",
    "        loss=BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[BinaryAccuracy(name=\"accuracy\", threshold=0.5)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def make_callbacks():\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"loss\",\n",
    "            factor=0.5,\n",
    "            patience=4,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Use ALL normalized training data\n",
    "# --------------------------------------------------------------------\n",
    "Xtr = X_train_norm\n",
    "ytr = y_train_norm\n",
    "\n",
    "print(f\"[TRAIN] Xtr: {Xtr.shape}, ytr: {ytr.shape}\")\n",
    "print(\"[INFO] Test / LOSO subject is NOT used here.\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 80:20 Stratified Split\n",
    "# --------------------------------------------------------------------\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(sss.split(Xtr, ytr))\n",
    "\n",
    "X_train, X_val = Xtr[train_idx], Xtr[val_idx]\n",
    "y_train, y_val = ytr[train_idx], ytr[val_idx]\n",
    "\n",
    "print(f\"[SPLIT] Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Train model\n",
    "# --------------------------------------------------------------------\n",
    "model = compile_model()\n",
    "callbacks = make_callbacks()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Save final model\n",
    "# --------------------------------------------------------------------\n",
    "model.save(\"james-final.h5\")\n",
    "print(\"[SAVE] Model saved as cole-final.h5\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Validation Confusion Matrix\n",
    "# --------------------------------------------------------------------\n",
    "y_prob = model.predict(X_val, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "y_true = y_val.astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "eps = 1e-12\n",
    "\n",
    "acc  = (tp + tn) / max(tp + tn + fp + fn, eps)\n",
    "prec = tp / max(tp + fp, eps)\n",
    "rec  = tp / max(tp + fn, eps)\n",
    "f1   = 2 * prec * rec / max(prec + rec, eps)\n",
    "\n",
    "print(\"\\n[CONFUSION MATRIX]\")\n",
    "print(cm)\n",
    "print(f\"[VAL] acc={acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e2a13-6367-41ca-be66-a43594c2940e",
   "metadata": {},
   "source": [
    "Cell 8 — Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ac6662-33c1-4e25-b6c5-d47c860fba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Preparing TEST data...\n",
      "[TEST] Xte shape: (300, 14, 128, 1)\n",
      "[TEST] yte shape: (300,), class distribution: (array([1]), array([300]))\n",
      "[TEST] Loading final model: james-final.h5\n",
      "[TEST] Running inference on TEST subject...\n",
      "[TEST] Mean predicted probability: 0.2042\n",
      "\n",
      "[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[  0   0]\n",
      " [255  45]]\n",
      "Accuracy : 0.1500\n",
      "Precision: 1.0000\n",
      "Recall   : 0.1500\n",
      "F1-score : 0.2609\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Inference on TEST subject (adam) using FINAL EEGNet\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Prepare TEST data\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Preparing TEST data...\")\n",
    "\n",
    "Xte = X_test_norm\n",
    "yte = y_test_norm.astype(int)\n",
    "\n",
    "print(f\"[TEST] Xte shape: {Xte.shape}\")\n",
    "print(f\"[TEST] yte shape: {yte.shape}, class distribution: {np.unique(yte, return_counts=True)}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Load FINAL trained model\n",
    "# ------------------------------------------------------------------\n",
    "model_path = \"james-final.h5\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        \"Final model 'cole-final.h5' not found. \"\n",
    "        \"Please run Cell 4 (80:20 training) first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[TEST] Loading final model: {model_path}\")\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Run inference on TEST subject\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[TEST] Running inference on TEST subject...\")\n",
    "\n",
    "y_prob = model.predict(Xte, verbose=0).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(f\"[TEST] Mean predicted probability: {y_prob.mean():.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Metrics on TEST subject (segment-level)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc = accuracy_score(yte, y_pred)\n",
    "prec = precision_score(yte, y_pred, zero_division=0)\n",
    "rec = recall_score(yte, y_pred, zero_division=0)\n",
    "f1 = f1_score(yte, y_pred, zero_division=0)\n",
    "\n",
    "print(\"\\n[TEST] ===== LOSO TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572dbf3e-bbb5-4865-883f-30e374700949",
   "metadata": {},
   "source": [
    "Cell 9 - Performance Matrices for each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a57240-50ea-4736-a7ac-52ef7878b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] Verifying predictions and labels are available...\n",
      "[CHECK] Found predictions for 300 test segments.\n",
      "\n",
      "[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\n",
      "Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[  0   0]\n",
      " [255  45]]\n",
      "Accuracy : 0.1500\n",
      "Precision: 1.0000\n",
      "Recall   : 0.1500\n",
      "F1-score : 0.2609\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Final model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================\n",
    "# Cell 6: Confusion Matrix & Metrics on TEST (FINAL EEGNet)\n",
    "# =======================================================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# ------------------------------------------------------------------\n",
    "def compute_metrics_from_cm(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    eps = 1e-12\n",
    "    acc  = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec  = tp / (tp + fn + eps)\n",
    "    f1   = 2 * prec * rec / (prec + rec + eps)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], ['0', '1'])\n",
    "    plt.yticks([0, 1], ['0', '1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(\n",
    "                j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Validate upstream artifacts\n",
    "# ------------------------------------------------------------------\n",
    "print(\"[CHECK] Verifying predictions and labels are available...\")\n",
    "\n",
    "if 'y_pred' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Predictions 'y_pred' not found. \"\n",
    "        \"Please run Cell 5 (final model inference) first.\"\n",
    "    )\n",
    "\n",
    "if 'yte' not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Test labels 'yte' not found. \"\n",
    "        \"Please run Cell 5 first.\"\n",
    "    )\n",
    "\n",
    "print(f\"[CHECK] Found predictions for {len(y_pred)} test segments.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Confusion Matrix & Metrics (FINAL model)\n",
    "# ------------------------------------------------------------------\n",
    "cm = confusion_matrix(yte, y_pred, labels=[0, 1])\n",
    "acc, prec, rec, f1 = compute_metrics_from_cm(cm)\n",
    "\n",
    "print(\"\\n[FINAL MODEL] ===== TEST RESULTS (SEGMENT-LEVEL) =====\")\n",
    "print(\"Confusion Matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "print(cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(cm, title=\"Confusion Matrix - FINAL EEGNet\")\n",
    "\n",
    "print(\"[DONE] Final model evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0f120-39fd-497e-9098-05e6c82f4807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
