{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2f670-3136-4ec0-aacb-8647adf0aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# ----------------- Graph Convolution -----------------\n",
    "\n",
    "class GraphConvolution(layers.Layer):\n",
    "    \"\"\"\n",
    "    Simple GCN layer:\n",
    "      x:   (B, N, in_features)\n",
    "      adj: (B, N, N)\n",
    "      out: (B, N, out_features)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, use_bias=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape = [x_shape, adj_shape]\n",
    "        self.weight = self.add_weight(\n",
    "            shape=(self.in_features, self.out_features),\n",
    "            initializer=tf.keras.initializers.GlorotUniform(),\n",
    "            trainable=True,\n",
    "            name=\"weight\"\n",
    "        )\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(\n",
    "                shape=(1, 1, self.out_features),\n",
    "                initializer=\"zeros\",\n",
    "                trainable=True,\n",
    "                name=\"bias\"\n",
    "            )\n",
    "        else:\n",
    "            self.bias = None\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, adj = inputs  # x: (B,N,in), adj: (B,N,N)\n",
    "\n",
    "        out = tf.matmul(x, self.weight)  # (B,N,out)\n",
    "        if self.bias is not None:\n",
    "            out = out - self.bias        # matches original code\n",
    "        out = tf.matmul(adj, out)        # (B,N,out)\n",
    "        out = tf.nn.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ----------------- Power Layer -----------------\n",
    "\n",
    "class PowerLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Log-transformed power:\n",
    "      - squares input\n",
    "      - AvgPool2D over time\n",
    "      - log()\n",
    "    Input format: channels_first 4D: (B, C, H, W)\n",
    "    \"\"\"\n",
    "    def __init__(self, length, step, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.length = length\n",
    "        self.step = step\n",
    "        self.pool = layers.AveragePooling2D(\n",
    "            pool_size=(1, self.length),\n",
    "            strides=(1, self.step),\n",
    "            data_format=\"channels_first\"\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        x2 = tf.math.square(x)\n",
    "        pooled = self.pool(x2)\n",
    "        return tf.math.log(pooled + 1e-8)\n",
    "\n",
    "\n",
    "# ----------------- Aggregator -----------------\n",
    "\n",
    "class Aggregator(layers.Layer):\n",
    "    \"\"\"\n",
    "    Aggregates channels into brain areas by averaging.\n",
    "    idx_area: list of number of channels in each brain area, e.g. [4, 6, 8, ...]\n",
    "    \"\"\"\n",
    "    def __init__(self, idx_area, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.chan_in_area = idx_area\n",
    "        self.idx = self._get_idx(self.chan_in_area)\n",
    "        self.area = len(idx_area)\n",
    "\n",
    "    def _get_idx(self, chan_in_area):\n",
    "        idx = [0] + chan_in_area\n",
    "        idx_ = [0]\n",
    "        for i in idx:\n",
    "            idx_.append(idx_[-1] + i)\n",
    "        # same as PyTorch: idx_[1:] are region boundaries (starts)\n",
    "        return idx_[1:]\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, channels, features)\n",
    "        returns: (B, num_areas, features)\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        B = tf.shape(x)[0]\n",
    "        C = tf.shape(x)[1]\n",
    "\n",
    "        for i in range(self.area):\n",
    "            start = self.idx[i]\n",
    "            end = self.idx[i + 1] if i < self.area - 1 else C\n",
    "            slice_x = x[:, start:end, :]     # (B, channels_in_area, F)\n",
    "            area_mean = tf.reduce_mean(slice_x, axis=1)  # (B, F)\n",
    "            data.append(area_mean)\n",
    "\n",
    "        # stack along new area dimension\n",
    "        return tf.stack(data, axis=1)        # (B, num_areas, F)\n",
    "\n",
    "\n",
    "# ----------------- Local Filter Layer -----------------\n",
    "\n",
    "class LocalFilterLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Implements: x = ReLU( x * W - bias )\n",
    "    with W: (channels, features), bias: (1, channels, 1)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_channels, feature_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_channels = num_channels\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.local_filter_weight = self.add_weight(\n",
    "            shape=(self.num_channels, self.feature_dim),\n",
    "            initializer=tf.keras.initializers.GlorotUniform(),\n",
    "            trainable=True,\n",
    "            name=\"local_filter_weight\"\n",
    "        )\n",
    "        self.local_filter_bias = self.add_weight(\n",
    "            shape=(1, self.num_channels, 1),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "            name=\"local_filter_bias\"\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # x: (B, channels, features)\n",
    "        B = tf.shape(x)[0]\n",
    "        w = tf.expand_dims(self.local_filter_weight, axis=0)  # (1,C,F)\n",
    "        w = tf.repeat(w, repeats=B, axis=0)                   # (B,C,F)\n",
    "        x = tf.nn.relu(x * w - self.local_filter_bias)        # broadcast over features axis\n",
    "        return x\n",
    "\n",
    "\n",
    "# ----------------- Helper: temporal output size -----------------\n",
    "\n",
    "def compute_temporal_feature_dim(\n",
    "    input_size,      # (freq, channels, time)\n",
    "    sampling_rate,\n",
    "    num_T,\n",
    "    pool,\n",
    "    pool_step_rate,\n",
    "    window=(0.5, 0.25, 0.125),\n",
    "):\n",
    "    \"\"\"\n",
    "    Analytic reimplementation of get_size_temporal() from PyTorch code.\n",
    "    Returns the feature dimension per channel after temporal blocks.\n",
    "    \"\"\"\n",
    "    F, C, T = input_size\n",
    "    widths = []\n",
    "\n",
    "    for w in window:\n",
    "        kernel_len = int(w * sampling_rate)               # conv kernel along time\n",
    "        # Conv2d (no padding, stride=1)\n",
    "        T_conv = T - kernel_len + 1\n",
    "\n",
    "        # AvgPool2d kernel=pool, stride=pool_step_rate*pool (no padding)\n",
    "        stride = int(pool_step_rate * pool)\n",
    "        T_pool = math.floor((T_conv - pool) / stride + 1)\n",
    "        widths.append(T_pool)\n",
    "\n",
    "    total_w = sum(widths)\n",
    "\n",
    "    # OneXOneConv: Conv2d with kernel=(1,1) (no change), then AvgPool2D((1,2), stride=(1,2))\n",
    "    k2 = 2\n",
    "    s2 = 2\n",
    "    width_after = math.floor((total_w - k2) / s2 + 1)\n",
    "\n",
    "    feature_dim = num_T * width_after\n",
    "    return feature_dim\n",
    "\n",
    "\n",
    "# ----------------- LGGNet Keras Model -----------------\n",
    "\n",
    "class LGGNetKeras(Model):\n",
    "    \"\"\"\n",
    "    Keras implementation of LGGNet.\n",
    "\n",
    "    Expected input shape: (batch, F, C, T)\n",
    "      - F: number of \"frequency\" bands (input_size[0] in original code)\n",
    "      - C: number of EEG channels       (input_size[1])\n",
    "      - T: time points                  (input_size[2])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        input_size,        # (F, C, T)\n",
    "        sampling_rate,\n",
    "        num_T,\n",
    "        out_graph,\n",
    "        dropout_rate,\n",
    "        pool,\n",
    "        pool_step_rate,\n",
    "        idx_graph,         # list of channels per brain area\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.window = [0.5, 0.25, 0.125]\n",
    "        self.pool = pool\n",
    "        self.channel = input_size[1]\n",
    "        self.brain_area = len(idx_graph)\n",
    "        self.idx_graph = idx_graph\n",
    "\n",
    "        # Temporal learners (three Tception branches)\n",
    "        self.Tception1 = self._temporal_learner(\n",
    "            in_chan=input_size[0],\n",
    "            out_chan=num_T,\n",
    "            kernel_len=int(self.window[0] * sampling_rate),\n",
    "            pool=pool,\n",
    "            pool_step_rate=pool_step_rate,\n",
    "            name=\"Tception1\"\n",
    "        )\n",
    "        self.Tception2 = self._temporal_learner(\n",
    "            in_chan=input_size[0],\n",
    "            out_chan=num_T,\n",
    "            kernel_len=int(self.window[1] * sampling_rate),\n",
    "            pool=pool,\n",
    "            pool_step_rate=pool_step_rate,\n",
    "            name=\"Tception2\"\n",
    "        )\n",
    "        self.Tception3 = self._temporal_learner(\n",
    "            in_chan=input_size[0],\n",
    "            out_chan=num_T,\n",
    "            kernel_len=int(self.window[2] * sampling_rate),\n",
    "            pool=pool,\n",
    "            pool_step_rate=pool_step_rate,\n",
    "            name=\"Tception3\"\n",
    "        )\n",
    "\n",
    "        self.BN_t  = layers.BatchNormalization(axis=1, name=\"BN_t\")   # channels_first\n",
    "        self.BN_t_ = layers.BatchNormalization(axis=1, name=\"BN_t_\")\n",
    "\n",
    "        self.OneXOneConv = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.Conv2D(\n",
    "                    filters=num_T,\n",
    "                    kernel_size=(1, 1),\n",
    "                    strides=(1, 1),\n",
    "                    padding=\"valid\",\n",
    "                    data_format=\"channels_first\",\n",
    "                    name=\"conv_1x1\"\n",
    "                ),\n",
    "                layers.LeakyReLU(),\n",
    "                layers.AveragePooling2D(\n",
    "                    pool_size=(1, 2),\n",
    "                    strides=(1, 2),\n",
    "                    data_format=\"channels_first\",\n",
    "                    name=\"avgpool_1x2\"\n",
    "                )\n",
    "            ],\n",
    "            name=\"OneXOneConv\"\n",
    "        )\n",
    "\n",
    "        # Compute feature dimension after temporal blocks, per channel\n",
    "        feature_dim = compute_temporal_feature_dim(\n",
    "            input_size=input_size,\n",
    "            sampling_rate=sampling_rate,\n",
    "            num_T=num_T,\n",
    "            pool=pool,\n",
    "            pool_step_rate=pool_step_rate,\n",
    "            window=self.window\n",
    "        )\n",
    "\n",
    "        # Local filter: W and bias\n",
    "        self.local_filter = LocalFilterLayer(\n",
    "            num_channels=self.channel,\n",
    "            feature_dim=feature_dim,\n",
    "            name=\"LocalFilter\"\n",
    "        )\n",
    "\n",
    "        # Aggregator over brain areas  ✅ FIXED LINE\n",
    "        self.aggregate = Aggregator(idx_area=self.idx_graph, name=\"Aggregator\")\n",
    "\n",
    "        # Global adjacency (trainable)\n",
    "        self.global_adj = self.add_weight(\n",
    "            shape=(self.brain_area, self.brain_area),\n",
    "            initializer=tf.keras.initializers.GlorotUniform(),\n",
    "            trainable=True,\n",
    "            name=\"global_adj\"\n",
    "        )\n",
    "\n",
    "        # BN over brain areas\n",
    "        self.bn  = layers.BatchNormalization(axis=1, name=\"bn_global1\")\n",
    "        self.bn_ = layers.BatchNormalization(axis=1, name=\"bn_global2\")\n",
    "\n",
    "        # GCN layer\n",
    "        self.gcn = GraphConvolution(\n",
    "            in_features=feature_dim,\n",
    "            out_features=out_graph,\n",
    "            name=\"GCN\"\n",
    "        )\n",
    "\n",
    "        # Final classifier\n",
    "        self.fc = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "                layers.Dense(num_classes)\n",
    "            ],\n",
    "            name=\"Classifier\"\n",
    "        )\n",
    "\n",
    "    def _temporal_learner(self, in_chan, out_chan, kernel_len, pool, pool_step_rate, name=None):\n",
    "        step = int(pool_step_rate * pool)\n",
    "        return tf.keras.Sequential(\n",
    "            [\n",
    "                layers.Conv2D(\n",
    "                    filters=out_chan,\n",
    "                    kernel_size=(1, kernel_len),\n",
    "                    strides=(1, 1),\n",
    "                    padding=\"valid\",\n",
    "                    data_format=\"channels_first\",\n",
    "                    use_bias=True,\n",
    "                    name=f\"{name}_conv\" if name else None\n",
    "                ),\n",
    "                PowerLayer(\n",
    "                    length=pool,\n",
    "                    step=step,\n",
    "                    name=f\"{name}_power\" if name else None\n",
    "                )\n",
    "            ],\n",
    "            name=name\n",
    "        )\n",
    "\n",
    "    def _self_similarity(self, x):\n",
    "        # x: (B, node, feature)\n",
    "        return tf.matmul(x, x, transpose_b=True)  # (B, node, node)\n",
    "\n",
    "    def _get_adj(self, x, self_loop=True):\n",
    "        # x: (B, node, feature)\n",
    "        adj = self._self_similarity(x)  # (B, N, N)\n",
    "\n",
    "        # symmetric learned adjacency\n",
    "        sym = self.global_adj + tf.transpose(self.global_adj)  # (N, N)\n",
    "        adj = tf.nn.relu(adj * sym)                            # broadcast over batch\n",
    "\n",
    "        num_nodes = tf.shape(adj)[-1]\n",
    "        if self_loop:\n",
    "            adj = adj + tf.eye(num_nodes)[tf.newaxis, :, :]\n",
    "\n",
    "        rowsum = tf.reduce_sum(adj, axis=-1)          # (B, N)\n",
    "        mask = tf.cast(tf.equal(rowsum, 0.0), rowsum.dtype)\n",
    "        rowsum = rowsum + mask\n",
    "        d_inv_sqrt = tf.pow(rowsum, -0.5)\n",
    "        d_mat_inv_sqrt = tf.linalg.diag(d_inv_sqrt)   # (B, N, N)\n",
    "        adj_norm = tf.matmul(tf.matmul(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n",
    "        return adj_norm  # (B, N, N)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        \"\"\"\n",
    "        x: (B, F, C, T)  — frequency x channel x time\n",
    "        \"\"\"\n",
    "        # --- Temporal blocks ---\n",
    "        y1 = self.Tception1(x, training=training)\n",
    "        out = y1\n",
    "        y2 = self.Tception2(x, training=training)\n",
    "        out = tf.concat([out, y2], axis=-1)   # concat on time axis\n",
    "        y3 = self.Tception3(x, training=training)\n",
    "        out = tf.concat([out, y3], axis=-1)\n",
    "\n",
    "        out = self.BN_t(out, training=training)\n",
    "        out = self.OneXOneConv(out, training=training)\n",
    "        out = self.BN_t_(out, training=training)\n",
    "\n",
    "        # Permute (B, Tchan, C, W) -> (B, C, Tchan, W)\n",
    "        out = tf.transpose(out, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # Flatten last two dims: (B, C, features)\n",
    "        B = tf.shape(out)[0]\n",
    "        C = tf.shape(out)[1]\n",
    "        Fdim = tf.shape(out)[2] * tf.shape(out)[3]\n",
    "        out = tf.reshape(out, (B, C, Fdim))\n",
    "\n",
    "        # Local filter\n",
    "        out = self.local_filter(out)\n",
    "\n",
    "        # Aggregate into brain areas: (B, brain_area, features)\n",
    "        out = self.aggregate(out)\n",
    "\n",
    "        # Build adjacency\n",
    "        adj = self._get_adj(out)\n",
    "\n",
    "        # Global graph conv\n",
    "        out = self.bn(out, training=training)\n",
    "        out = self.gcn([out, adj])\n",
    "        out = self.bn_(out, training=training)\n",
    "\n",
    "        # Flatten and classify\n",
    "        out = tf.reshape(out, (B, -1))\n",
    "        logits = self.fc(out, training=training)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ----------------- Helper: build + compile -----------------\n",
    "\n",
    "def build_lggnet_keras(\n",
    "    num_classes,\n",
    "    input_size,        # (F, C, T)\n",
    "    sampling_rate,\n",
    "    num_T,\n",
    "    out_graph,\n",
    "    dropout_rate,\n",
    "    pool,\n",
    "    pool_step_rate,\n",
    "    idx_graph,\n",
    "    lr=1e-3\n",
    "):\n",
    "    model = LGGNetKeras(\n",
    "        num_classes=num_classes,\n",
    "        input_size=input_size,\n",
    "        sampling_rate=sampling_rate,\n",
    "        num_T=num_T,\n",
    "        out_graph=out_graph,\n",
    "        dropout_rate=dropout_rate,\n",
    "        pool=pool,\n",
    "        pool_step_rate=pool_step_rate,\n",
    "        idx_graph=idx_graph\n",
    "    )\n",
    "\n",
    "    # Sparse categorical for multi-class (logits)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=loss,\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# ----------------- Helper: reshape to LGGNet input -----------------\n",
    "\n",
    "def to_lggnet_input(X):\n",
    "    \"\"\"\n",
    "    Convert your data to LGGNet format (B, F, C, T),\n",
    "    here F=1 (single 'frequency' band).\n",
    "\n",
    "    Assumes X is either:\n",
    "      - (B, C, T, 1)  as in your previous pipelines\n",
    "      - or (B, C, T)\n",
    "    and returns (B, 1, C, T)\n",
    "    \"\"\"\n",
    "    # If there's a trailing singleton channel dimension, drop it\n",
    "    if X.ndim == 4 and X.shape[-1] == 1:\n",
    "        X = np.squeeze(X, axis=-1)   # (B, C, T)\n",
    "\n",
    "    # Add fake frequency dimension F=1 at axis=1\n",
    "    X = np.expand_dims(X, axis=1)    # (B, 1, C, T)\n",
    "    return X\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
