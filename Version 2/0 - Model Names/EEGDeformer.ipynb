{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658aa4d9-17f4-421c-8dea-734a48d66cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ----------------- FeedForward (MLP) -----------------\n",
    "def feed_forward_block(x, hidden_dim, dropout=0.0, name=None):\n",
    "    \"\"\"Equivalent to PyTorch FeedForward (without the residual).\"\"\"\n",
    "    with tf.name_scope(name or \"FeedForward\"):\n",
    "        d = tf.keras.backend.int_shape(x)[-1]\n",
    "        y = layers.LayerNormalization()(x)\n",
    "        y = layers.Dense(hidden_dim, activation=\"gelu\")(y)\n",
    "        y = layers.Dropout(dropout)(y)\n",
    "        y = layers.Dense(d)(y)\n",
    "        y = layers.Dropout(dropout)(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "# ----------------- Attention block -----------------\n",
    "def attention_block(x, dim_head=64, heads=8, dropout=0.0, name=None):\n",
    "    \"\"\"\n",
    "    Equivalent to PyTorch Attention(dim, heads, dim_head).\n",
    "    x: (B, N, D) tokens x embedding\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name or \"Attention\"):\n",
    "        D = tf.keras.backend.int_shape(x)[-1]\n",
    "        mha = layers.MultiHeadAttention(\n",
    "            num_heads=heads,\n",
    "            key_dim=dim_head,\n",
    "            dropout=dropout,\n",
    "            output_shape=D  # project back to dim\n",
    "        )\n",
    "        y = mha(x, x)\n",
    "        y = layers.Dropout(dropout)(y)\n",
    "        # residual will be added outside, as in PyTorch (attn(x) + x)\n",
    "    return y\n",
    "\n",
    "\n",
    "# ----------------- 1D CNN block (fine-grained branch) -----------------\n",
    "def cnn_1d_block(x, in_chan, kernel_size, dropout=0.0, name=None):\n",
    "    \"\"\"\n",
    "    PyTorch:\n",
    "      Dropout -> Conv1d(in_chan, in_chan, k, padding) -> BN -> ELU -> MaxPool1d(2)\n",
    "    Input:  (B, in_chan, L)\n",
    "    Output: (B, in_chan, L/2)\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name or \"CNN1D_Block\"):\n",
    "        # Keras Conv1D with data_format='channels_first': (B, C, L)\n",
    "        y = layers.Dropout(dropout)(x)\n",
    "        y = layers.Conv1D(\n",
    "            filters=in_chan,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=\"same\",\n",
    "            data_format=\"channels_first\",\n",
    "            use_bias=True\n",
    "        )(y)\n",
    "        y = layers.BatchNormalization(axis=1)(y)  # axis=channel for channels_first\n",
    "        y = layers.ELU()(y)\n",
    "        y = layers.MaxPooling1D(pool_size=2, strides=2, data_format=\"channels_first\")(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "# ----------------- Helper: get_info (log(mean(x^2))) -----------------\n",
    "def get_info_tensor(x):\n",
    "    \"\"\"\n",
    "    PyTorch get_info:\n",
    "        x: (b, k, l)\n",
    "        x = log(mean(x^2, dim=-1))\n",
    "        return (b, k)\n",
    "    \"\"\"\n",
    "    return tf.math.log(\n",
    "        tf.reduce_mean(tf.square(x), axis=-1) + 1e-12\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------- Main Deformer builder -----------------\n",
    "def build_eeg_deformer_keras(\n",
    "    num_chan,\n",
    "    num_time,\n",
    "    temporal_kernel,\n",
    "    num_kernel=64,\n",
    "    num_classes=2,\n",
    "    depth=4,\n",
    "    heads=16,\n",
    "    mlp_dim=16,\n",
    "    dim_head=16,\n",
    "    dropout=0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Keras implementation of Deformer.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    num_chan        : number of EEG channels (C)\n",
    "    num_time        : number of time samples (T)\n",
    "    temporal_kernel : temporal kernel (same as PyTorch 'temporal_kernel')\n",
    "    num_kernel      : number of CNN kernels in first 2D conv block (64 in original)\n",
    "    num_classes     : output classes\n",
    "    depth           : number of Transformer layers\n",
    "    heads           : attention heads\n",
    "    mlp_dim         : hidden dim for FFN inside transformer\n",
    "    dim_head        : per-head key dimension\n",
    "    dropout         : dropout rate\n",
    "\n",
    "    Input shape\n",
    "    -----------\n",
    "    EEG input is expected as (batch, num_chan, num_time)\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------- Input -----------------\n",
    "    inp = layers.Input(shape=(num_chan, num_time), name=\"eeg_input\")\n",
    "    # PyTorch unsqueezes along dim=1 => (B, 1, chan, time)\n",
    "    x = layers.Lambda(lambda t: tf.expand_dims(t, axis=1))(inp)  # (B, 1, C, T)\n",
    "\n",
    "    # ----------------- CNN Encoder (2D) -----------------\n",
    "    # PyTorch:\n",
    "    #   Conv2dWithConstraint(1, num_kernel, (1, temporal_kernel), padding=(0, pad))\n",
    "    #   Conv2dWithConstraint(num_kernel, num_kernel, (num_chan, 1), padding=0)\n",
    "    #   BN, ELU, MaxPool2d((1,2))\n",
    "    pad_t = temporal_kernel // 2  # same as get_padding(kernel) in time dim\n",
    "\n",
    "    # First conv: kernel (1, temporal_kernel)\n",
    "    x = layers.Conv2D(\n",
    "        filters=num_kernel,\n",
    "        kernel_size=(1, temporal_kernel),\n",
    "        padding=\"same\",   # temporal padding; equivalent behavior\n",
    "        data_format=\"channels_first\",  # (B, 1, C, T)\n",
    "        use_bias=True,\n",
    "        kernel_constraint=max_norm(2.0)\n",
    "    )(x)\n",
    "\n",
    "    # Second conv: kernel (num_chan, 1) collapses spatial/channel dim\n",
    "    x = layers.Conv2D(\n",
    "        filters=num_kernel,\n",
    "        kernel_size=(num_chan, 1),\n",
    "        padding=\"valid\",\n",
    "        data_format=\"channels_first\",   # (B, num_kernel, 1, T)\n",
    "        use_bias=True,\n",
    "        kernel_constraint=max_norm(2.0)\n",
    "    )(x)\n",
    "\n",
    "    x = layers.BatchNormalization(axis=1)(x)\n",
    "    x = layers.ELU()(x)\n",
    "    x = layers.MaxPool2D(\n",
    "        pool_size=(1, 2),\n",
    "        strides=(1, 2),\n",
    "        data_format=\"channels_first\"\n",
    "    )(x)  # (B, num_kernel, 1, 0.5*num_time)\n",
    "\n",
    "    # After this, in PyTorch: dim = int(0.5*num_time)\n",
    "    dim = int(0.5 * num_time)\n",
    "\n",
    "    # ----------------- To patch embedding: Rearrange 'b k c f -> b k (c f)' -----------------\n",
    "    # Current shape: (B, num_kernel, 1, dim)\n",
    "    x = layers.Permute((1, 3, 2))(x)        # (B, num_kernel, dim, 1)\n",
    "    x = layers.Reshape((num_kernel, dim))(x)  # (B, num_kernel, dim) = (B, k, d)\n",
    "\n",
    "    # x will now be passed to \"Transformer\" equivalent\n",
    "    # We treat shape as (B, in_chan=num_kernel, L=dim) for CNN, and (B, tokens=num_kernel, D=dim) for attention.\n",
    "\n",
    "    dense_features = []\n",
    "    # We'll emulate the iterative Transformer.forward:\n",
    "    #   for attn, ff, cnn in layers:\n",
    "    #       x_cg = pool(x)\n",
    "    #       x_cg = attn(x_cg) + x_cg\n",
    "    #       x_fg = cnn(x)\n",
    "    #       x_info = get_info(x_fg)\n",
    "    #       dense_feature.append(x_info)\n",
    "    #       x = ff(x_cg) + x_fg\n",
    "\n",
    "    # Shared coarse MaxPool1D (same as self.pool in PyTorch)\n",
    "    pool_coarse = layers.MaxPooling1D(\n",
    "        pool_size=2, strides=2,\n",
    "        data_format=\"channels_first\"  # treats shape as (B, C, L)\n",
    "    )\n",
    "\n",
    "    # We'll keep x as (B, num_kernel, current_dim) throughout.\n",
    "    for i in range(depth):\n",
    "        # Coarse-grained branch (pool then attention)\n",
    "        x_cg = pool_coarse(x)          # (B, num_kernel, dim/2)\n",
    "        # Attention expects (B, tokens, features) = same shape.\n",
    "        attn_out = attention_block(\n",
    "            x_cg,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            dropout=dropout,\n",
    "            name=f\"attn_block_{i}\"\n",
    "        )\n",
    "        x_cg = layers.Add()([attn_out, x_cg])  # attn(x_cg) + x_cg\n",
    "\n",
    "        # Fine-grained CNN branch\n",
    "        x_fg = cnn_1d_block(\n",
    "            x,\n",
    "            in_chan=num_kernel,\n",
    "            kernel_size=temporal_kernel,\n",
    "            dropout=dropout,\n",
    "            name=f\"cnn1d_block_{i}\"\n",
    "        )  # (B, num_kernel, dim/2) same length as x_cg\n",
    "\n",
    "        # Info feature from x_fg: (B, num_kernel)\n",
    "        x_info = layers.Lambda(get_info_tensor, name=f\"info_{i}\")(x_fg)\n",
    "        dense_features.append(x_info)\n",
    "\n",
    "        # FeedForward on coarse branch, then combine with fine-grained\n",
    "        ff_out = feed_forward_block(\n",
    "            x_cg,\n",
    "            hidden_dim=mlp_dim,\n",
    "            dropout=dropout,\n",
    "            name=f\"ff_block_{i}\"\n",
    "        )\n",
    "        x = layers.Add()([ff_out, x_fg])   # ff(x_cg) + x_fg\n",
    "\n",
    "    # Concatenate dense features: list of (B, num_kernel) -> (B, num_kernel * depth)\n",
    "    if len(dense_features) > 1:\n",
    "        x_dense = layers.Concatenate(axis=-1)(dense_features)\n",
    "    else:\n",
    "        x_dense = dense_features[0]\n",
    "\n",
    "    # Flatten x from last layer: (B, num_kernel, d_last) -> (B, num_kernel * d_last)\n",
    "    x_flat = layers.Flatten()(x)\n",
    "\n",
    "    # Final embedding: concat [x_flat, x_dense]\n",
    "    emd = layers.Concatenate(axis=-1, name=\"embedding_concat\")([x_flat, x_dense])\n",
    "\n",
    "    # ----------------- MLP Head -----------------\n",
    "    out = layers.Dense(num_classes, activation=\"softmax\", name=\"logits\")(emd)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out, name=\"EEG_Deformer_Keras\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# # ----------------- Example usage -----------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Match the PyTorch example:\n",
    "#     # data = torch.ones((16, 32, 1000))\n",
    "#     batch_size = 16\n",
    "#     num_chan = 32\n",
    "#     num_time = 1000\n",
    "\n",
    "#     model = build_eeg_deformer_keras(\n",
    "#         num_chan=num_chan,\n",
    "#         num_time=num_time,\n",
    "#         temporal_kernel=11,\n",
    "#         num_kernel=64,\n",
    "#         num_classes=2,\n",
    "#         depth=4,\n",
    "#         heads=16,\n",
    "#         mlp_dim=16,\n",
    "#         dim_head=16,\n",
    "#         dropout=0.5,\n",
    "#     )\n",
    "\n",
    "#     model.summary()\n",
    "\n",
    "#     # Dummy forward\n",
    "#     x = np.ones((batch_size, num_chan, num_time), dtype=np.float32)\n",
    "#     y = model(x)\n",
    "#     print(\"Output shape:\", y.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
