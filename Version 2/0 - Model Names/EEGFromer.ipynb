{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6790dad-25a1-4bbe-828d-85d33b6c9197",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### AFNet (accepts (K, M, S, 1); architecture unchanged) ###############\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, DepthwiseConv2D, BatchNormalization, Activation,\n",
    "    AveragePooling2D, Dropout, Dense, SeparableConv2D, Add,\n",
    "    GlobalAveragePooling2D, GlobalAveragePooling1D,\n",
    "    MultiHeadAttention, Reshape, LayerNormalization, Multiply\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "# -------- Utilities: safe temporal parameters (no architecture change) --------\n",
    "def _cap_kernel(k: int, S: int) -> int:\n",
    "    \"\"\"Ensure temporal kernel length does not exceed segment length S.\"\"\"\n",
    "    return max(1, min(int(k), int(S)))\n",
    "\n",
    "def _pick_safe_temporal_pools(S: int,\n",
    "                              preferred=(4, 4)):\n",
    "    \"\"\"\n",
    "    Keep the two AveragePooling2D layers but choose pool sizes so that after\n",
    "    two temporal pools the time axis remains >= 1.\n",
    "      L1 = floor(S / p1), L2 = floor(L1 / p2) >= 1  â‡’  S >= p1 * p2\n",
    "    Returns ((1, p1), (1, p2)).\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        preferred, (4, 4), (4, 3), (3, 3), (3, 2), (2, 2), (2, 1), (1, 1)\n",
    "    ]\n",
    "    seen, uniq = set(), []\n",
    "    for c in candidates:\n",
    "        if c not in seen:\n",
    "            uniq.append(c); seen.add(c)\n",
    "    for p1, p2 in uniq:\n",
    "        if (S // p1) // p2 >= 1:\n",
    "            return (1, p1), (1, p2)\n",
    "    return (1, 1), (1, 1)\n",
    "\n",
    "\n",
    "# -------- Spatial Attention (unchanged) --------\n",
    "def SpatialAttention(x, name=\"spatial_attn\"):\n",
    "    \"\"\"\n",
    "    Spatial attention to focus on the most relevant EEG electrodes.\n",
    "    Returns x * attn where attn is (B, M, 1, 1). We also name the attn tensor.\n",
    "    \"\"\"\n",
    "    attn = GlobalAveragePooling2D(name=f\"{name}_gap\")(x)          # (B, C)\n",
    "    attn = Dense(64, activation='relu', name=f\"{name}_fc1\")(attn)\n",
    "\n",
    "    M_elec = K.int_shape(x)[1]\n",
    "    if M_elec is None:\n",
    "        raise ValueError(\"Electrode dimension must be known. Pass input_shape=(M, S, 1).\")\n",
    "\n",
    "    attn = Dense(int(M_elec), activation='sigmoid', name=f\"{name}_weights\")(attn)  # (B, M)\n",
    "    attn_map = Reshape((int(M_elec), 1, 1), name=f\"{name}_map\")(attn)              # (B, M, 1, 1)\n",
    "\n",
    "    out = Multiply(name=f\"{name}_apply\")([x, attn_map])\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------- Transformer Block (unchanged) --------\n",
    "def TransformerBlock(x, num_heads=4, key_dim=64, ff_dim=128, dropout_rate=0.1):\n",
    "    \"\"\"\n",
    "    Standard Transformer encoder block for sequences (B, L, D).\n",
    "    \"\"\"\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)\n",
    "    attn_output = Dropout(dropout_rate)(attn_output)\n",
    "    attn_output = Add()([x, attn_output])\n",
    "    attn_output = LayerNormalization()(attn_output)\n",
    "\n",
    "    ff = Dense(ff_dim, activation='relu')(attn_output)\n",
    "    ff = Dropout(dropout_rate)(ff)\n",
    "    ff = Dense(K.int_shape(x)[-1])(ff)\n",
    "    x = Add()([attn_output, ff])\n",
    "    x = LayerNormalization()(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# -------- Model (architecture unchanged; parameters adapted to M,S) --------\n",
    "def EEGNet_SpatialTransformer(input_shape=(22, 100, 1),\n",
    "                              dropout_rate=0.5,\n",
    "                              num_heads=4, ff_dim=128,\n",
    "                              num_classes=4):\n",
    "    \"\"\"\n",
    "    AFNet-style EEG model with Spatial Attention + Transformer encoder.\n",
    "    Accepts input_shape=(M, S, 1) with known M and S.\n",
    "    \"\"\"\n",
    "    M, S, C = input_shape\n",
    "    if C != 1:\n",
    "        raise ValueError(\"This model expects a single input channel. Use input_shape=(M, S, 1).\")\n",
    "\n",
    "    # Safe temporal hyperparameters (no architecture change)\n",
    "    k1 = _cap_kernel(5, S)     # for SeparableConv2D (1, 5)\n",
    "    k2 = _cap_kernel(3, S)     # for SeparableConv2D (1, 3)\n",
    "    pool1, pool2 = _pick_safe_temporal_pools(S, preferred=(8, 4))\n",
    "\n",
    "    inputs = Input(shape=input_shape)  # (B, M, S, 1)\n",
    "\n",
    "    # 1) Temporal-focused SeparableConv2D\n",
    "    x = SeparableConv2D(32, (1, k1), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 2) Spatial Attention\n",
    "    x = SpatialAttention(x)\n",
    "\n",
    "    # 3) Spatial filtering across electrodes; keep M dimension\n",
    "    x = DepthwiseConv2D((M, 1), use_bias=False, depth_multiplier=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 4) Residual projection to match channels for Add()\n",
    "    ch_out = K.int_shape(x)[-1] or 64  # should be 32*depth_multiplier = 64\n",
    "    proj = Conv2D(filters=ch_out, kernel_size=(1, 1), padding='same', use_bias=False)(inputs)\n",
    "    proj = BatchNormalization()(proj)\n",
    "    x = Add()([x, proj])\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # 5) Downsample (temporal)\n",
    "    x = AveragePooling2D(pool_size=pool1)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # 6) Separable conv\n",
    "    x = SeparableConv2D(64, (1, k2), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 7) Downsample again (temporal)\n",
    "    x = AveragePooling2D(pool_size=pool2)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # 8) Prepare for Transformer: reshape to (B, M, S'*C')\n",
    "    _, M_eff, S_eff, C_eff = K.int_shape(x)\n",
    "    if None in (M_eff, S_eff, C_eff):\n",
    "        raise ValueError(\"Static shapes must be known. Pass concrete input_shape=(M, S, 1).\")\n",
    "    x = Reshape((M_eff, S_eff * C_eff))(x)\n",
    "\n",
    "    # 9) Transformer encoder\n",
    "    x = TransformerBlock(x, num_heads=num_heads, key_dim=32, ff_dim=ff_dim, dropout_rate=0.1)\n",
    "\n",
    "    # 10) Pool over sequence (electrodes)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # 11) MLP head\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # 12) Output\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs, outputs, name=\"EEGNet_SpatialTransformer_refit\")\n",
    "\n",
    "\n",
    "# -------- Instantiate & Compile (no normalisation of inputs here) --------\n",
    "# Example:\n",
    "# M = X_train.shape[1]; S = X_train.shape[2]; C = X_train.shape[3]  # must be 1\n",
    "# n_classes = int(max(y_train.max(), y_val.max())) + 1\n",
    "# model = EEGNet_SpatialTransformer(input_shape=(M, S, 1),\n",
    "#                                   dropout_rate=0.5,\n",
    "#                                   num_heads=4, ff_dim=128,\n",
    "#                                   num_classes=n_classes)\n",
    "# model.compile(optimizer=Adam(learning_rate=1e-3),\n",
    "#               loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "#               metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')])\n",
    "\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=15,\n",
    "#                                              restore_best_weights=True, verbose=1)\n",
    "# history = model.fit(X_train, y_train,\n",
    "#                     validation_data=(X_val, y_val),\n",
    "#                     batch_size=50, epochs=150,\n",
    "#                     callbacks=[earlystop], verbose=1)\n",
    "# model.summary()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
