{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48d3cb4b-2b03-4283-87bf-b807e7c10d23",
   "metadata": {},
   "source": [
    "step 1- Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac08b3e9-3cf8-4469-bf1e-f8a3a6884e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.20.0\n",
      "GPUs: []\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 1: Imports, seeds, and GPU setup =====\n",
    "\n",
    "# Standard libraries\n",
    "import os, random                              # OS utilities and reproducibility\n",
    "import numpy as np                             # Numerical arrays\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf                        # Main DL framework\n",
    "\n",
    "# Keras layers & utilities\n",
    "from tensorflow.keras.layers import (Input, SeparableConv2D, DepthwiseConv2D, BatchNormalization,\n",
    "                                     Activation, AveragePooling2D, Dropout, Dense, Add, Lambda,\n",
    "                                     GlobalAveragePooling2D, GlobalAveragePooling1D,\n",
    "                                     MultiHeadAttention, Reshape, LayerNormalization, Conv2D)\n",
    "from tensorflow.keras.models import Model      # Functional model API\n",
    "from tensorflow.keras.optimizers import Adam   # Optimizer\n",
    "from tensorflow.keras.losses import BinaryCrossentropy  # Binary classification loss\n",
    "from tensorflow.keras.metrics import Accuracy  # Accuracy metric\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint  # Training callbacks\n",
    "from tensorflow.keras import backend as K      # Keras backend for static shape queries\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import StratifiedKFold                  # Stratified 5-fold CV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score  # Metrics\n",
    "\n",
    "# Plotting (optional)\n",
    "import matplotlib.pyplot as plt               # For charts (optional)\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42                                     # Fixed seed\n",
    "np.random.seed(SEED)                          # Seed NumPy\n",
    "tf.random.set_seed(SEED)                      # Seed TensorFlow\n",
    "random.seed(SEED)                             # Seed Python\n",
    "\n",
    "# Optional: make TensorFlow gentle with GPU memory\n",
    "gpus = tf.config.list_physical_devices('GPU') # Detect GPUs\n",
    "print(\"TensorFlow:\", tf.__version__)          # Show TF version\n",
    "print(\"GPUs:\", gpus)                          # Show detected GPUs\n",
    "if gpus:                                      # If GPU(s) are present\n",
    "    for g in gpus:                            # Iterate each GPU\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(g, True)  # Enable memory growth\n",
    "        except Exception as e:\n",
    "            print(\"GPU memory growth not set:\", e)             # Warn if not possible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e07e80a-cf10-4e44-8609-3a8acc6c6a34",
   "metadata": {},
   "source": [
    "Step 2- Load arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0f8364b-fe04-40e2-b139-7275af0eaf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EEG data...\n",
      "Loaded.\n",
      "Raw X shape: (4770, 14, 200) | dtype: float32\n",
      "Raw y shape: (4770,) | dtype: int64\n",
      "y squeezed shape: (4770,) | unique labels: [0 1]\n",
      "X after adding channel dim: (4770, 14, 200, 1)\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 2: Load the arrays in the given path (no normalization) =====\n",
    "\n",
    "# Exact Windows paths you provided (raw strings to avoid backslash escapes)\n",
    "eeg_path    = r\"C:\\Self Learning\\Research Papers\\UOW Research Papers\\eSport Players with EEG Data\\EEG Data\\final eeg data\\final_eeg.npy\"\n",
    "labels_path = r\"C:\\Self Learning\\Research Papers\\UOW Research Papers\\eSport Players with EEG Data\\EEG Data\\final eeg data\\final_eeg_labels.npy\"\n",
    "\n",
    "print(\"Loading EEG data...\")                 # Status print\n",
    "X = np.load(eeg_path)                        # Load EEG data, expected shape (N, 14, 200)\n",
    "y = np.load(labels_path)                     # Load labels, expected shape (N,) or (N,1)\n",
    "print(\"Loaded.\")                             # Confirm loaded\n",
    "\n",
    "print(\"Raw X shape:\", X.shape, \"| dtype:\", X.dtype)  # Inspect EEG tensor\n",
    "print(\"Raw y shape:\", y.shape, \"| dtype:\", y.dtype)  # Inspect labels\n",
    "\n",
    "y = np.squeeze(y).astype(int)                # Ensure labels are 1D int array\n",
    "print(\"y squeezed shape:\", y.shape, \"| unique labels:\", np.unique(y))  # Show label distribution\n",
    "\n",
    "# Sanity checks on shapes and labels\n",
    "assert X.ndim == 3, f\"Expected X to be (N,14,200). Got {X.shape}\"      # Check rank\n",
    "assert X.shape[1] == 14 and X.shape[2] == 200, \"Expected 14 electrodes and 200 samples per segment.\"  # Check dims\n",
    "assert set(np.unique(y)).issubset({0,1}), \"Labels must be {0,1}.\"      # Binary check\n",
    "\n",
    "# Add channel dimension for Keras Conv2D: (N, 14, 200, 1)\n",
    "X = X[..., np.newaxis]                         # Append channel dim\n",
    "print(\"X after adding channel dim:\", X.shape)  # Verify final input shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1573ae-b7d7-46c2-b447-948c63bd620b",
   "metadata": {},
   "source": [
    "Step 3: Normalising Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da717924-cc93-47c0-b426-a429d850f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 3: Normalization helper (per-fold; compute stats on train only) =====\n",
    "\n",
    "def normalize_per_fold(X_train, X_val, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Normalizes X_train and X_val using mean/std computed from X_train only.\n",
    "    Broadcasting shape is (1, 14, 1, 1), i.e., per-electrode normalization.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute mean & std from the training set only\n",
    "    train_mean = np.mean(X_train, axis=(0, 2, 3), keepdims=True)   # -> (1, 14, 1, 1)\n",
    "    train_std  = np.std( X_train, axis=(0, 2, 3), keepdims=True)   # -> (1, 14, 1, 1)\n",
    "\n",
    "    # Safety: avoid division by (near) zero\n",
    "    train_std = np.where(train_std < eps, 1.0, train_std)\n",
    "\n",
    "    # Step 2: Apply normalization using training mean & std\n",
    "    X_train_norm = ((X_train - train_mean) / train_std).astype(np.float32)\n",
    "    X_val_norm   = ((X_val   - train_mean) / train_std).astype(np.float32)\n",
    "\n",
    "    print(\"Per-fold normalization done.\",\n",
    "          \"train_mean shape:\", train_mean.shape, \"train_std shape:\", train_std.shape)\n",
    "\n",
    "    return X_train_norm, X_val_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd2af7b-1685-4573-b263-604591d7918e",
   "metadata": {},
   "source": [
    "Step 4: Prepare 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7727ac88-2626-48a3-ad00-30da848d423e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared StratifiedKFold with 5 splits.\n",
      "Total samples: 4770\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 4: Prepare the data to perform 5-fold cross validation =====\n",
    "\n",
    "N_SPLITS = 5                                   # Number of CV folds\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS,       # Stratified (preserves class ratios)\n",
    "                      shuffle=True,            # Shuffle before splitting\n",
    "                      random_state=SEED)       # Reproducible splits\n",
    "\n",
    "print(f\"Prepared StratifiedKFold with {N_SPLITS} splits.\")  # Status\n",
    "print(\"Total samples:\", X.shape[0])                         # Dataset size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e50d7-72cd-41ee-94b4-207be8e301f7",
   "metadata": {},
   "source": [
    "Step 5: Model, compile, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82374f63-b924-4a09-837f-d6dbbbf5c241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 5: Define AFNet (your code, only shape edits + Keras-safe ops) =====\n",
    "\n",
    "def SpatialAttention(x):\n",
    "    \"\"\"Spatial attention to weight electrodes; uses Keras Reshape/Multiply (no raw tf ops on KerasTensors).\"\"\"\n",
    "    num_electrodes = K.int_shape(x)[1]                        # Static electrode count (14)\n",
    "    attn = GlobalAveragePooling2D()(x)                        # Global pool -> (batch, channels)\n",
    "    attn = Dense(64, activation='relu')(attn)                 # Dense to learn attention features\n",
    "    attn = Dense(num_electrodes, activation='sigmoid')(attn)  # One weight per electrode\n",
    "    attn = Reshape((num_electrodes, 1, 1))(attn)              # Reshape for broadcast across time & channels\n",
    "    x = tf.keras.layers.Multiply()([x, attn])                 # Apply attention weights\n",
    "    return x                                                  # Return attended feature maps\n",
    "\n",
    "def TransformerBlock(x, num_heads=4, key_dim=64, ff_dim=128, dropout_rate=0.1):\n",
    "    \"\"\"Transformer encoder block: MHA + FFN with residuals and layer norms.\"\"\"\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)  # Self-attention\n",
    "    attn_output = Dropout(dropout_rate)(attn_output)                               # Dropout\n",
    "    x = Add()([x, attn_output])                                                   # Residual\n",
    "    x = LayerNormalization()(x)                                                   # LayerNorm\n",
    "\n",
    "    ff = Dense(ff_dim, activation='relu')(x)                                      # Feed-forward (expand)\n",
    "    ff = Dropout(dropout_rate)(ff)                                                # Dropout\n",
    "    ff = Dense(x.shape[-1])(ff)                                                   # Project back\n",
    "    x = Add()([x, ff])                                                            # Residual\n",
    "    x = LayerNormalization()(x)                                                   # LayerNorm\n",
    "    return x                                                                      # Return block output\n",
    "\n",
    "def EEGNet_SpatialTransformer(input_shape=(14, 200, 1), dropout_rate=0.5, num_heads=4, ff_dim=128):\n",
    "    \"\"\"AFNet adapted for (14,200,1); keeps your structure, minimal shape handling for residual add.\"\"\"\n",
    "    print(\"Building model with input_shape:\", input_shape)                         # Trace\n",
    "\n",
    "    inputs = Input(shape=input_shape)                                              # Input tensor\n",
    "\n",
    "    # Depthwise Separable Convolution (temporal)\n",
    "    x = SeparableConv2D(32, (1, 5), padding='same', use_bias=False)(inputs)       # First sep conv\n",
    "    x = BatchNormalization()(x)                                                    # BN\n",
    "    x = Activation('relu')(x)                                                      # ReLU\n",
    "    print(\"After sepconv1:\", x.shape)                                              # Shape trace\n",
    "\n",
    "    # Spatial Attention\n",
    "    x = SpatialAttention(x)                                                        # Attention over electrodes\n",
    "    print(\"After SpatialAttention:\", x.shape)                                      # Shape trace\n",
    "\n",
    "    # Depthwise Spatial Convolution across electrodes (change 61->14)\n",
    "    x = DepthwiseConv2D((14, 1), use_bias=False, depth_multiplier=2, padding='valid')(x)  # Collapse electrode dim\n",
    "    x = BatchNormalization()(x)                                                    # BN\n",
    "    x = Activation('relu')(x)                                                      # ReLU\n",
    "    print(\"After depthwise spatial:\", x.shape)                                     # Shape trace (electrode dim -> 1)\n",
    "\n",
    "    # Residual branch from inputs via the same first sepconv path (to get 32 channels & same HxW)\n",
    "    res = SeparableConv2D(32, (1, 5), padding='same', use_bias=False)(inputs)      # Residual features (14x200x32)\n",
    "    res = BatchNormalization()(res)                                                # BN\n",
    "    res = Activation('relu')(res)                                                  # ReLU\n",
    "    print(\"Residual branch shape:\", res.shape)                                     # Shape trace\n",
    "\n",
    "    # Project x back to 32 channels so we can add with residual\n",
    "    x = Conv2D(32, (1,1), padding='same', use_bias=False)(x)                       # 1x1 to 32 channels\n",
    "    x = BatchNormalization()(x)                                                    # BN\n",
    "    x = Activation('relu')(x)                                                      # ReLU\n",
    "    print(\"After 1x1 proj to 32:\", x.shape)                                        # Shape trace (1x200x32)\n",
    "\n",
    "    # Tile the electrode axis from 1 -> 14 to match residual spatial dims for Add()\n",
    "    x = Lambda(lambda t: tf.repeat(t, repeats=input_shape[0], axis=1))(x)          # Repeat along axis=1\n",
    "    print(\"After tiling to electrodes=14:\", x.shape)                                # Shape trace (14x200x32)\n",
    "\n",
    "    # Add residual and continue\n",
    "    x = Add()([x, res])                                                            # Residual add\n",
    "    x = BatchNormalization()(x)                                                    # BN\n",
    "    print(\"After residual add:\", x.shape)                                          # Shape trace\n",
    "\n",
    "    # First Average Pooling (temporal downsampling: 200 -> 50)\n",
    "    x = AveragePooling2D((1, 4))(x)                                                # Pool time axis\n",
    "    x = Dropout(dropout_rate)(x)                                                   # Dropout\n",
    "    print(\"After avgpool1:\", x.shape)                                              # Shape trace\n",
    "\n",
    "    # Second separable conv block\n",
    "    x = SeparableConv2D(64, (1, 3), padding='same', use_bias=False)(x)             # More features\n",
    "    x = BatchNormalization()(x)                                                    # BN\n",
    "    x = Activation('relu')(x)                                                      # ReLU\n",
    "    print(\"After sepconv2:\", x.shape)                                              # Shape trace\n",
    "\n",
    "    # Second Average Pooling (temporal downsampling: ~50 -> ~12/13)\n",
    "    x = AveragePooling2D((1, 4))(x)                                                # Pool time again\n",
    "    x = Dropout(dropout_rate)(x)                                                   # Dropout\n",
    "    print(\"After avgpool2:\", x.shape)                                              # Shape trace\n",
    "\n",
    "    # Transformer over electrodes (tokens = 14, features = T' * 64)\n",
    "    x_shape = K.int_shape(x)                                                       # Static shape\n",
    "    seq_len = x_shape[1]                                                           # Electrode tokens (14)\n",
    "    feat_dim = x_shape[2] * x_shape[3]                                             # Feature dim per token\n",
    "    x = Reshape((seq_len, feat_dim))(x)                                            # (batch, 14, T'*64)\n",
    "    print(\"Before Transformer, reshaped to:\", x.shape)                              # Trace shape\n",
    "\n",
    "    x = TransformerBlock(x, num_heads=num_heads, key_dim=64, ff_dim=ff_dim, dropout_rate=0.1)  # Transformer\n",
    "\n",
    "    # Classifier head\n",
    "    x = GlobalAveragePooling1D()(x)                                                # Pool across tokens\n",
    "    x = BatchNormalization()(x)                                                    # BN\n",
    "    print(\"After GAP1D:\", x.shape)                                                 # Shape trace\n",
    "\n",
    "    x = Dense(128, activation='relu')(x)                                           # Dense layer\n",
    "    x = BatchNormalization()(x)                                                    # BN\n",
    "    x = Dropout(dropout_rate)(x)                                                   # Dropout\n",
    "    outputs = Dense(1, activation='sigmoid')(x)                                    # Sigmoid for binary classification\n",
    "\n",
    "    model = Model(inputs, outputs)                                                 # Build model\n",
    "    return model                                                                   # Return uncompiled model\n",
    "\n",
    "\n",
    "# ===== Helpers: compile model and callbacks =====\n",
    "\n",
    "def compile_model():\n",
    "    \"\"\"Create and compile a fresh AFNet model instance.\"\"\"\n",
    "    model = EEGNet_SpatialTransformer(input_shape=(14, 200, 1))             # Build with correct input shape\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3),                       # Adam optimizer (lr=1e-3)\n",
    "                  loss=BinaryCrossentropy(),                                # Binary cross-entropy loss\n",
    "                  metrics=[Accuracy()])                                     # Track Accuracy metric\n",
    "    print(model.summary(line_length=120))                                   # Print model summary\n",
    "    return model                                                            # Return compiled model\n",
    "\n",
    "def make_callbacks(fold_idx):\n",
    "    \"\"\"Construct callbacks for a given fold: EarlyStopping + ReduceLROnPlateau + ModelCheckpoint(.h5).\"\"\"\n",
    "    print(f\"Setting up callbacks for fold {fold_idx}...\")                   # Status print\n",
    "\n",
    "    es = EarlyStopping(                                                     # Early stopping\n",
    "        monitor='val_loss',                                                 # Monitor validation loss\n",
    "        patience=15,                                                        # Wait 15 epochs without improvement\n",
    "        restore_best_weights=True,                                          # Restore best weights\n",
    "        verbose=1                                                           # Verbose messages\n",
    "    )\n",
    "\n",
    "    rlrop = ReduceLROnPlateau(                                              # Reduce LR on plateau\n",
    "        monitor='val_loss',                                                 # Monitor validation loss\n",
    "        factor=0.5,                                                         # Halve the LR\n",
    "        patience=4,                                                         # After 4 stagnant epochs\n",
    "        min_lr=1e-6,                                                        # Do not go below 1e-6\n",
    "        verbose=1                                                           # Verbose messages\n",
    "    )\n",
    "\n",
    "    ckpt_path = f\"best_fold_{fold_idx}.h5\"                                  # Save best model as .h5 (HDF5)\n",
    "    ckpt = ModelCheckpoint(                                                 # Checkpoint callback\n",
    "        filepath=ckpt_path,                                                 # Output file path\n",
    "        monitor='val_loss',                                                 # Select best by val_loss\n",
    "        save_best_only=True,                                                # Only save when improved\n",
    "        verbose=1                                                           # Verbose messages\n",
    "        # Note: .h5 requires h5py installed; if missing, run: pip install h5py\n",
    "    )\n",
    "\n",
    "    return [es, rlrop, ckpt]                                                # Return callback list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f00bd69-f2b6-46ad-bbdf-8ef39459547e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "Fold 1 — Train/Val split\n",
      "Train idx: (3816,) Val idx: (954,)\n",
      "X_train: (3816, 14, 200, 1) | y_train: (3816,)\n",
      "X_val  : (954, 14, 200, 1) | y_val  : (954,)\n",
      "Per-fold normalization done. train_mean shape: (1, 14, 1, 1) train_std shape: (1, 14, 1, 1)\n",
      "Normalization check — Train(mean,std)=(-0.0014, 1.0000) | Val(mean,std)=(-0.0077, 1.0277) | Train per-electrode std range: [1.000, 1.000]\n",
      "Building model with input_shape: (14, 200, 1)\n",
      "After sepconv1: (None, 14, 200, 32)\n",
      "After SpatialAttention: (None, 14, 200, 32)\n",
      "After depthwise spatial: (None, 1, 200, 64)\n",
      "Residual branch shape: (None, 14, 200, 32)\n",
      "After 1x1 proj to 32: (None, 1, 200, 32)\n",
      "After tiling to electrodes=14: (None, 14, 200, 32)\n",
      "After residual add: (None, 14, 200, 32)\n",
      "After avgpool1: (None, 14, 50, 32)\n",
      "After sepconv2: (None, 14, 50, 64)\n",
      "After avgpool2: (None, 14, 12, 64)\n",
      "Before Transformer, reshaped to: (None, 14, 768)\n",
      "After GAP1D: (None, 768)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                      </span>┃<span style=\"font-weight: bold\"> Output Shape                 </span>┃<span style=\"font-weight: bold\">           Param # </span>┃<span style=\"font-weight: bold\"> Connected to              </span>\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                         \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_8             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ separable_conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ global_average_pooling2d_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                   │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)          │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │ global_average_pooling2d_1\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">910</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ reshape_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ multiply_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
       "│                                   │                              │                   │ reshape_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ depthwise_conv2d_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ multiply_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_9             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ depthwise_conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ activation_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_11            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_4                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_10            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ separable_conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           \n",
       "│                                   │                              │                   │ activation_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_12            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ average_pooling2d_2               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ average_pooling2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_5                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">2,144</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_13            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ separable_conv2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ average_pooling2d_3               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ average_pooling2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ reshape_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ multi_head_attention_1            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">787,968</span> │ reshape_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)              │                              │                   │ reshape_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ reshape_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          \n",
       "│                                   │                              │                   │ dropout_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ layer_normalization_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │ layer_normalization_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │ dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>\n",
       "│                                   │                              │                   │ dense_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ layer_normalization_3             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ global_average_pooling1d_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                  │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)          │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_14            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> │ global_average_pooling1d_1\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  │            <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │ batch_normalization_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_15            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                    │               <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "└───────────────────────────────────┴──────────────────────────────┴───────────────────┴───────────────────────────\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m          Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ -                         \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                \u001b[38;5;34m37\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_8             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m128\u001b[0m │ separable_conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ global_average_pooling2d_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                   │                 \u001b[38;5;34m0\u001b[0m │ activation_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)          │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   │             \u001b[38;5;34m2,112\u001b[0m │ global_average_pooling2d_1\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)                   │               \u001b[38;5;34m910\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ reshape_2 (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)             │                 \u001b[38;5;34m0\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ multiply_1 (\u001b[38;5;33mMultiply\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ activation_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
       "│                                   │                              │                   │ reshape_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ depthwise_conv2d_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │               \u001b[38;5;34m896\u001b[0m │ multiply_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "│ (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_9             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │               \u001b[38;5;34m256\u001b[0m │ depthwise_conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_6 (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │             \u001b[38;5;34m2,048\u001b[0m │ activation_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_11            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │               \u001b[38;5;34m128\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_4                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                \u001b[38;5;34m37\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_8 (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_11[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_10            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m128\u001b[0m │ separable_conv2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ activation_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_7 (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_10[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_3 (\u001b[38;5;33mAdd\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           \n",
       "│                                   │                              │                   │ activation_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_12            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m128\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ average_pooling2d_2               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_12[\u001b[38;5;34m0\u001b[0m][\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ average_pooling2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_5                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │             \u001b[38;5;34m2,144\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
       "│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_13            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │               \u001b[38;5;34m256\u001b[0m │ separable_conv2d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_9 (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_13[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ average_pooling2d_3               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ activation_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ average_pooling2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ reshape_3 (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ multi_head_attention_1            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │           \u001b[38;5;34m787,968\u001b[0m │ reshape_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          \n",
       "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)              │                              │                   │ reshape_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ multi_head_attention_1[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_4 (\u001b[38;5;33mAdd\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ reshape_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          \n",
       "│                                   │                              │                   │ dropout_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ layer_normalization_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │             \u001b[38;5;34m1,536\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               \n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │            \u001b[38;5;34m98,432\u001b[0m │ layer_normalization_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │            \u001b[38;5;34m99,072\u001b[0m │ dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_5 (\u001b[38;5;33mAdd\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ layer_normalization_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m\n",
       "│                                   │                              │                   │ dense_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ layer_normalization_3             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │             \u001b[38;5;34m1,536\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               \n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ global_average_pooling1d_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                  │                 \u001b[38;5;34m0\u001b[0m │ layer_normalization_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)          │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_14            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                  │             \u001b[38;5;34m3,072\u001b[0m │ global_average_pooling1d_1\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  │            \u001b[38;5;34m98,432\u001b[0m │ batch_normalization_14[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_15            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  │               \u001b[38;5;34m512\u001b[0m │ dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_15[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                    │               \u001b[38;5;34m129\u001b[0m │ dropout_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "└───────────────────────────────────┴──────────────────────────────┴───────────────────┴───────────────────────────\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,099,897</span> (4.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,099,897\u001b[0m (4.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,097,593</span> (4.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,097,593\u001b[0m (4.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,304</span> (9.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,304\u001b[0m (9.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Setting up callbacks for fold 1...\n",
      "Training with EarlyStopping...\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 1: val_loss improved from None to 4.73693, saving model to best_fold_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 31s - 257ms/step - accuracy: 0.0000e+00 - loss: 0.4107 - val_accuracy: 0.0000e+00 - val_loss: 4.7369 - learning_rate: 1.0000e-03\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 2: val_loss improved from 4.73693 to 3.61261, saving model to best_fold_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 175ms/step - accuracy: 2.6205e-04 - loss: 0.2077 - val_accuracy: 0.0000e+00 - val_loss: 3.6126 - learning_rate: 1.0000e-03\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 3: val_loss improved from 3.61261 to 1.21336, saving model to best_fold_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 177ms/step - accuracy: 7.8616e-04 - loss: 0.1555 - val_accuracy: 0.0000e+00 - val_loss: 1.2134 - learning_rate: 1.0000e-03\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 4: val_loss improved from 1.21336 to 0.41552, saving model to best_fold_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 179ms/step - accuracy: 7.8616e-04 - loss: 0.1273 - val_accuracy: 0.0000e+00 - val_loss: 0.4155 - learning_rate: 1.0000e-03\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 5: val_loss improved from 0.41552 to 0.13825, saving model to best_fold_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 175ms/step - accuracy: 0.0018 - loss: 0.1069 - val_accuracy: 0.0000e+00 - val_loss: 0.1383 - learning_rate: 1.0000e-03\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 6: val_loss improved from 0.13825 to 0.11221, saving model to best_fold_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 22s - 180ms/step - accuracy: 0.0021 - loss: 0.0905 - val_accuracy: 0.0000e+00 - val_loss: 0.1122 - learning_rate: 1.0000e-03\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 7: val_loss improved from 0.11221 to 0.10903, saving model to best_fold_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 173ms/step - accuracy: 7.8616e-04 - loss: 0.0767 - val_accuracy: 0.0031 - val_loss: 0.1090 - learning_rate: 1.0000e-03\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 8: val_loss improved from 0.10903 to 0.10052, saving model to best_fold_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 23s - 189ms/step - accuracy: 0.0024 - loss: 0.0795 - val_accuracy: 0.0451 - val_loss: 0.1005 - learning_rate: 1.0000e-03\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.10052\n",
      "120/120 - 21s - 173ms/step - accuracy: 0.0029 - loss: 0.0685 - val_accuracy: 0.0472 - val_loss: 0.1246 - learning_rate: 1.0000e-03\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 10: val_loss improved from 0.10052 to 0.09302, saving model to best_fold_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 20s - 170ms/step - accuracy: 0.0055 - loss: 0.0599 - val_accuracy: 0.0052 - val_loss: 0.0930 - learning_rate: 1.0000e-03\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.09302\n",
      "120/120 - 22s - 182ms/step - accuracy: 0.0031 - loss: 0.0525 - val_accuracy: 0.0168 - val_loss: 0.1376 - learning_rate: 1.0000e-03\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.09302\n",
      "120/120 - 22s - 182ms/step - accuracy: 0.0021 - loss: 0.0559 - val_accuracy: 0.0178 - val_loss: 0.1447 - learning_rate: 1.0000e-03\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.09302\n",
      "120/120 - 21s - 179ms/step - accuracy: 0.0013 - loss: 0.0534 - val_accuracy: 0.0094 - val_loss: 0.1537 - learning_rate: 1.0000e-03\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 14: val_loss improved from 0.09302 to 0.07419, saving model to best_fold_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 22s - 181ms/step - accuracy: 0.0026 - loss: 0.0375 - val_accuracy: 0.0084 - val_loss: 0.0742 - learning_rate: 1.0000e-03\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 15: val_loss improved from 0.07419 to 0.07230, saving model to best_fold_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 22s - 179ms/step - accuracy: 0.0031 - loss: 0.0356 - val_accuracy: 0.0021 - val_loss: 0.0723 - learning_rate: 1.0000e-03\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.07230\n",
      "120/120 - 21s - 179ms/step - accuracy: 0.0024 - loss: 0.0372 - val_accuracy: 0.0084 - val_loss: 0.1034 - learning_rate: 1.0000e-03\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.07230\n",
      "120/120 - 21s - 172ms/step - accuracy: 0.0050 - loss: 0.0336 - val_accuracy: 0.0430 - val_loss: 0.1768 - learning_rate: 1.0000e-03\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.07230\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0071 - loss: 0.0277 - val_accuracy: 0.0744 - val_loss: 0.1161 - learning_rate: 1.0000e-03\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.07230\n",
      "120/120 - 21s - 177ms/step - accuracy: 0.0060 - loss: 0.0226 - val_accuracy: 0.0084 - val_loss: 0.1098 - learning_rate: 1.0000e-03\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 20: val_loss improved from 0.07230 to 0.06330, saving model to best_fold_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 22s - 187ms/step - accuracy: 0.0058 - loss: 0.0208 - val_accuracy: 0.0073 - val_loss: 0.0633 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.06330\n",
      "120/120 - 22s - 184ms/step - accuracy: 0.0076 - loss: 0.0128 - val_accuracy: 0.1048 - val_loss: 0.0774 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.06330\n",
      "120/120 - 40s - 337ms/step - accuracy: 0.0079 - loss: 0.0080 - val_accuracy: 0.1530 - val_loss: 0.0662 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.06330\n",
      "120/120 - 20s - 168ms/step - accuracy: 0.0102 - loss: 0.0052 - val_accuracy: 0.1740 - val_loss: 0.0926 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.06330\n",
      "120/120 - 20s - 168ms/step - accuracy: 0.0173 - loss: 0.0073 - val_accuracy: 0.1803 - val_loss: 0.1004 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.06330\n",
      "120/120 - 21s - 173ms/step - accuracy: 0.0191 - loss: 0.0091 - val_accuracy: 0.1363 - val_loss: 0.0939 - learning_rate: 2.5000e-04\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.06330\n",
      "120/120 - 21s - 172ms/step - accuracy: 0.0202 - loss: 0.0049 - val_accuracy: 0.1205 - val_loss: 0.0937 - learning_rate: 2.5000e-04\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.06330\n",
      "120/120 - 22s - 179ms/step - accuracy: 0.0183 - loss: 0.0021 - val_accuracy: 0.1300 - val_loss: 0.0923 - learning_rate: 2.5000e-04\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.06330\n",
      "120/120 - 23s - 191ms/step - accuracy: 0.0231 - loss: 0.0023 - val_accuracy: 0.1855 - val_loss: 0.0847 - learning_rate: 2.5000e-04\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.06330\n",
      "120/120 - 22s - 186ms/step - accuracy: 0.0246 - loss: 0.0024 - val_accuracy: 0.1499 - val_loss: 0.0941 - learning_rate: 1.2500e-04\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.06330\n",
      "120/120 - 20s - 171ms/step - accuracy: 0.0267 - loss: 0.0023 - val_accuracy: 0.1604 - val_loss: 0.0837 - learning_rate: 1.2500e-04\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.06330\n",
      "120/120 - 21s - 175ms/step - accuracy: 0.0238 - loss: 0.0026 - val_accuracy: 0.1321 - val_loss: 0.0798 - learning_rate: 1.2500e-04\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.06330\n",
      "120/120 - 21s - 173ms/step - accuracy: 0.0257 - loss: 0.0014 - val_accuracy: 0.1562 - val_loss: 0.0840 - learning_rate: 1.2500e-04\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.06330\n",
      "120/120 - 21s - 173ms/step - accuracy: 0.0270 - loss: 0.0017 - val_accuracy: 0.1488 - val_loss: 0.0831 - learning_rate: 6.2500e-05\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.06330\n",
      "120/120 - 21s - 176ms/step - accuracy: 0.0288 - loss: 0.0013 - val_accuracy: 0.1509 - val_loss: 0.0852 - learning_rate: 6.2500e-05\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.06330\n",
      "120/120 - 21s - 173ms/step - accuracy: 0.0286 - loss: 0.0011 - val_accuracy: 0.1363 - val_loss: 0.0896 - learning_rate: 6.2500e-05\n",
      "Epoch 35: early stopping\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "Predicting on validation fold...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
      "\n",
      "Fold 1 Confusion Matrix (rows=true, cols=pred) [0=Amateur, 1=Professional]:\n",
      "[[465  12]\n",
      " [  2 475]]\n",
      "TN=465, FP=12, FN=2, TP=475\n",
      "\n",
      "Fold 1 Metrics (threshold=0.5):\n",
      "Accuracy    : 0.9853\n",
      "Precision   : 0.9754  (class 1: Professional)\n",
      "Sensitivity : 0.9958     (Recall for class 1)\n",
      "Specificity : 0.9748 (for class 0: Amateur)\n",
      "F1-score    : 0.9855\n",
      "\n",
      "==========================================================================================\n",
      "Fold 2 — Train/Val split\n",
      "Train idx: (3816,) Val idx: (954,)\n",
      "X_train: (3816, 14, 200, 1) | y_train: (3816,)\n",
      "X_val  : (954, 14, 200, 1) | y_val  : (954,)\n",
      "Per-fold normalization done. train_mean shape: (1, 14, 1, 1) train_std shape: (1, 14, 1, 1)\n",
      "Normalization check — Train(mean,std)=(-0.0014, 1.0000) | Val(mean,std)=(0.0090, 1.3690) | Train per-electrode std range: [1.000, 1.000]\n",
      "Building model with input_shape: (14, 200, 1)\n",
      "After sepconv1: (None, 14, 200, 32)\n",
      "After SpatialAttention: (None, 14, 200, 32)\n",
      "After depthwise spatial: (None, 1, 200, 64)\n",
      "Residual branch shape: (None, 14, 200, 32)\n",
      "After 1x1 proj to 32: (None, 1, 200, 32)\n",
      "After tiling to electrodes=14: (None, 14, 200, 32)\n",
      "After residual add: (None, 14, 200, 32)\n",
      "After avgpool1: (None, 14, 50, 32)\n",
      "After sepconv2: (None, 14, 50, 64)\n",
      "After avgpool2: (None, 14, 12, 64)\n",
      "Before Transformer, reshaped to: (None, 14, 768)\n",
      "After GAP1D: (None, 768)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                      </span>┃<span style=\"font-weight: bold\"> Output Shape                 </span>┃<span style=\"font-weight: bold\">           Param # </span>┃<span style=\"font-weight: bold\"> Connected to              </span>\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                         \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_6                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_16            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ separable_conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ global_average_pooling2d_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                   │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)          │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │ global_average_pooling2d_2\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">910</span> │ dense_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ reshape_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ multiply_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
       "│                                   │                              │                   │ reshape_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ depthwise_conv2d_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ multiply_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_17            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ depthwise_conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ activation_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_19            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_7                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_18            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ separable_conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           \n",
       "│                                   │                              │                   │ activation_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_20            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ average_pooling2d_4               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ average_pooling2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_8                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">2,144</span> │ dropout_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_21            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ separable_conv2d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ average_pooling2d_5               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ average_pooling2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ reshape_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ multi_head_attention_2            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">787,968</span> │ reshape_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)              │                              │                   │ reshape_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ reshape_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          \n",
       "│                                   │                              │                   │ dropout_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ layer_normalization_4             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │ layer_normalization_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │ dropout_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>\n",
       "│                                   │                              │                   │ dense_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ layer_normalization_5             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ global_average_pooling1d_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                  │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)          │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_22            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> │ global_average_pooling1d_2\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  │            <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │ batch_normalization_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_23            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                    │               <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "└───────────────────────────────────┴──────────────────────────────┴───────────────────┴───────────────────────────\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m          Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ -                         \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                \u001b[38;5;34m37\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_16            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m128\u001b[0m │ separable_conv2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_10 (\u001b[38;5;33mActivation\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_16[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ global_average_pooling2d_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                   │                 \u001b[38;5;34m0\u001b[0m │ activation_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)          │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   │             \u001b[38;5;34m2,112\u001b[0m │ global_average_pooling2d_2\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)                   │               \u001b[38;5;34m910\u001b[0m │ dense_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ reshape_4 (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)             │                 \u001b[38;5;34m0\u001b[0m │ dense_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ multiply_2 (\u001b[38;5;33mMultiply\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ activation_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
       "│                                   │                              │                   │ reshape_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ depthwise_conv2d_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │               \u001b[38;5;34m896\u001b[0m │ multiply_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "│ (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_17            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │               \u001b[38;5;34m256\u001b[0m │ depthwise_conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_11 (\u001b[38;5;33mActivation\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_17[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │             \u001b[38;5;34m2,048\u001b[0m │ activation_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_19            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │               \u001b[38;5;34m128\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                \u001b[38;5;34m37\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_13 (\u001b[38;5;33mActivation\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_19[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_18            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m128\u001b[0m │ separable_conv2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ lambda_2 (\u001b[38;5;33mLambda\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ activation_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_12 (\u001b[38;5;33mActivation\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_18[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_6 (\u001b[38;5;33mAdd\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ lambda_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           \n",
       "│                                   │                              │                   │ activation_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_20            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m128\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ average_pooling2d_4               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_20[\u001b[38;5;34m0\u001b[0m][\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ average_pooling2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_8                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │             \u001b[38;5;34m2,144\u001b[0m │ dropout_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_21            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │               \u001b[38;5;34m256\u001b[0m │ separable_conv2d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_14 (\u001b[38;5;33mActivation\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_21[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ average_pooling2d_5               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ activation_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ average_pooling2d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ reshape_5 (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ dropout_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ multi_head_attention_2            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │           \u001b[38;5;34m787,968\u001b[0m │ reshape_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          \n",
       "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)              │                              │                   │ reshape_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ multi_head_attention_2[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_7 (\u001b[38;5;33mAdd\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ reshape_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          \n",
       "│                                   │                              │                   │ dropout_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ layer_normalization_4             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │             \u001b[38;5;34m1,536\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               \n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │            \u001b[38;5;34m98,432\u001b[0m │ layer_normalization_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ dense_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │            \u001b[38;5;34m99,072\u001b[0m │ dropout_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_8 (\u001b[38;5;33mAdd\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ layer_normalization_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m\n",
       "│                                   │                              │                   │ dense_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ layer_normalization_5             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │             \u001b[38;5;34m1,536\u001b[0m │ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               \n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ global_average_pooling1d_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                  │                 \u001b[38;5;34m0\u001b[0m │ layer_normalization_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)          │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_22            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                  │             \u001b[38;5;34m3,072\u001b[0m │ global_average_pooling1d_2\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  │            \u001b[38;5;34m98,432\u001b[0m │ batch_normalization_22[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_23            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  │               \u001b[38;5;34m512\u001b[0m │ dense_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_17 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_23[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                    │               \u001b[38;5;34m129\u001b[0m │ dropout_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "└───────────────────────────────────┴──────────────────────────────┴───────────────────┴───────────────────────────\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,099,897</span> (4.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,099,897\u001b[0m (4.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,097,593</span> (4.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,097,593\u001b[0m (4.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,304</span> (9.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,304\u001b[0m (9.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Setting up callbacks for fold 2...\n",
      "Training with EarlyStopping...\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.47718, saving model to best_fold_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 30s - 253ms/step - accuracy: 0.0000e+00 - loss: 0.3702 - val_accuracy: 0.0000e+00 - val_loss: 1.4772 - learning_rate: 1.0000e-03\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 2: val_loss improved from 1.47718 to 0.43255, saving model to best_fold_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 177ms/step - accuracy: 0.0000e+00 - loss: 0.1544 - val_accuracy: 0.0000e+00 - val_loss: 0.4326 - learning_rate: 1.0000e-03\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 3: val_loss improved from 0.43255 to 0.25624, saving model to best_fold_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 22s - 183ms/step - accuracy: 7.8616e-04 - loss: 0.1188 - val_accuracy: 0.0000e+00 - val_loss: 0.2562 - learning_rate: 1.0000e-03\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 4: val_loss improved from 0.25624 to 0.11696, saving model to best_fold_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 22s - 181ms/step - accuracy: 7.8616e-04 - loss: 0.0924 - val_accuracy: 0.0000e+00 - val_loss: 0.1170 - learning_rate: 1.0000e-03\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.11696\n",
      "120/120 - 21s - 173ms/step - accuracy: 0.0016 - loss: 0.0780 - val_accuracy: 0.0042 - val_loss: 0.1182 - learning_rate: 1.0000e-03\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.11696\n",
      "120/120 - 21s - 173ms/step - accuracy: 0.0031 - loss: 0.0550 - val_accuracy: 0.0094 - val_loss: 0.1402 - learning_rate: 1.0000e-03\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.11696\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0029 - loss: 0.0527 - val_accuracy: 0.0178 - val_loss: 0.1931 - learning_rate: 1.0000e-03\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 8: val_loss improved from 0.11696 to 0.11171, saving model to best_fold_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 173ms/step - accuracy: 7.8616e-04 - loss: 0.0470 - val_accuracy: 0.0000e+00 - val_loss: 0.1117 - learning_rate: 1.0000e-03\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.11171\n",
      "120/120 - 21s - 173ms/step - accuracy: 0.0047 - loss: 0.0423 - val_accuracy: 0.0000e+00 - val_loss: 0.1373 - learning_rate: 1.0000e-03\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.11171\n",
      "120/120 - 21s - 176ms/step - accuracy: 0.0058 - loss: 0.0382 - val_accuracy: 0.0010 - val_loss: 0.1281 - learning_rate: 1.0000e-03\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 11: val_loss improved from 0.11171 to 0.09007, saving model to best_fold_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 175ms/step - accuracy: 0.0071 - loss: 0.0423 - val_accuracy: 0.0000e+00 - val_loss: 0.0901 - learning_rate: 1.0000e-03\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 12: val_loss improved from 0.09007 to 0.07026, saving model to best_fold_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 22s - 180ms/step - accuracy: 0.0105 - loss: 0.0364 - val_accuracy: 0.0000e+00 - val_loss: 0.0703 - learning_rate: 1.0000e-03\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.07026\n",
      "120/120 - 21s - 173ms/step - accuracy: 0.0086 - loss: 0.0310 - val_accuracy: 0.0461 - val_loss: 0.2396 - learning_rate: 1.0000e-03\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.07026\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0149 - loss: 0.0316 - val_accuracy: 0.0000e+00 - val_loss: 0.0891 - learning_rate: 1.0000e-03\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.07026\n",
      "120/120 - 21s - 175ms/step - accuracy: 0.0128 - loss: 0.0390 - val_accuracy: 0.1625 - val_loss: 0.2014 - learning_rate: 1.0000e-03\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.07026\n",
      "120/120 - 22s - 181ms/step - accuracy: 0.0157 - loss: 0.0271 - val_accuracy: 0.0105 - val_loss: 0.1062 - learning_rate: 1.0000e-03\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 17: val_loss improved from 0.07026 to 0.06583, saving model to best_fold_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 23s - 188ms/step - accuracy: 0.0147 - loss: 0.0240 - val_accuracy: 0.0388 - val_loss: 0.0658 - learning_rate: 5.0000e-04\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 18: val_loss improved from 0.06583 to 0.06578, saving model to best_fold_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 176ms/step - accuracy: 0.0173 - loss: 0.0146 - val_accuracy: 0.0304 - val_loss: 0.0658 - learning_rate: 5.0000e-04\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.06578\n",
      "120/120 - 21s - 177ms/step - accuracy: 0.0170 - loss: 0.0101 - val_accuracy: 0.0199 - val_loss: 0.0754 - learning_rate: 5.0000e-04\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.06578\n",
      "120/120 - 22s - 187ms/step - accuracy: 0.0202 - loss: 0.0084 - val_accuracy: 0.0304 - val_loss: 0.0772 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.06578\n",
      "120/120 - 21s - 177ms/step - accuracy: 0.0236 - loss: 0.0081 - val_accuracy: 0.0199 - val_loss: 0.0743 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.06578\n",
      "120/120 - 20s - 171ms/step - accuracy: 0.0189 - loss: 0.0069 - val_accuracy: 0.0388 - val_loss: 0.0740 - learning_rate: 2.5000e-04\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.06578\n",
      "120/120 - 22s - 182ms/step - accuracy: 0.0199 - loss: 0.0069 - val_accuracy: 0.0231 - val_loss: 0.0796 - learning_rate: 2.5000e-04\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.06578\n",
      "120/120 - 21s - 172ms/step - accuracy: 0.0165 - loss: 0.0051 - val_accuracy: 0.0566 - val_loss: 0.0706 - learning_rate: 2.5000e-04\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 25: val_loss improved from 0.06578 to 0.06044, saving model to best_fold_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 171ms/step - accuracy: 0.0189 - loss: 0.0036 - val_accuracy: 0.0503 - val_loss: 0.0604 - learning_rate: 2.5000e-04\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.06044\n",
      "120/120 - 21s - 173ms/step - accuracy: 0.0244 - loss: 0.0040 - val_accuracy: 0.0451 - val_loss: 0.0615 - learning_rate: 2.5000e-04\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.06044\n",
      "120/120 - 21s - 173ms/step - accuracy: 0.0259 - loss: 0.0025 - val_accuracy: 0.0587 - val_loss: 0.0683 - learning_rate: 2.5000e-04\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.06044\n",
      "120/120 - 20s - 171ms/step - accuracy: 0.0273 - loss: 0.0030 - val_accuracy: 0.0786 - val_loss: 0.0685 - learning_rate: 2.5000e-04\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.06044\n",
      "120/120 - 21s - 176ms/step - accuracy: 0.0301 - loss: 0.0023 - val_accuracy: 0.0828 - val_loss: 0.0797 - learning_rate: 2.5000e-04\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.06044\n",
      "120/120 - 22s - 187ms/step - accuracy: 0.0349 - loss: 0.0029 - val_accuracy: 0.0702 - val_loss: 0.0729 - learning_rate: 1.2500e-04\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.06044\n",
      "120/120 - 22s - 186ms/step - accuracy: 0.0343 - loss: 0.0014 - val_accuracy: 0.0765 - val_loss: 0.0737 - learning_rate: 1.2500e-04\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.06044\n",
      "120/120 - 21s - 179ms/step - accuracy: 0.0333 - loss: 0.0016 - val_accuracy: 0.0744 - val_loss: 0.0733 - learning_rate: 1.2500e-04\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.06044\n",
      "120/120 - 22s - 183ms/step - accuracy: 0.0393 - loss: 0.0019 - val_accuracy: 0.1153 - val_loss: 0.0681 - learning_rate: 1.2500e-04\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.06044\n",
      "120/120 - 21s - 175ms/step - accuracy: 0.0393 - loss: 0.0021 - val_accuracy: 0.0608 - val_loss: 0.0704 - learning_rate: 6.2500e-05\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.06044\n",
      "120/120 - 21s - 178ms/step - accuracy: 0.0380 - loss: 7.0018e-04 - val_accuracy: 0.0650 - val_loss: 0.0702 - learning_rate: 6.2500e-05\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.06044\n",
      "120/120 - 22s - 179ms/step - accuracy: 0.0393 - loss: 8.2907e-04 - val_accuracy: 0.0608 - val_loss: 0.0725 - learning_rate: 6.2500e-05\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.06044\n",
      "120/120 - 21s - 177ms/step - accuracy: 0.0390 - loss: 8.9047e-04 - val_accuracy: 0.0482 - val_loss: 0.0734 - learning_rate: 6.2500e-05\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.06044\n",
      "120/120 - 22s - 181ms/step - accuracy: 0.0409 - loss: 7.5426e-04 - val_accuracy: 0.0535 - val_loss: 0.0749 - learning_rate: 3.1250e-05\n",
      "Epoch 39/200\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.06044\n",
      "120/120 - 21s - 177ms/step - accuracy: 0.0425 - loss: 6.8150e-04 - val_accuracy: 0.0618 - val_loss: 0.0746 - learning_rate: 3.1250e-05\n",
      "Epoch 40/200\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.06044\n",
      "120/120 - 21s - 171ms/step - accuracy: 0.0448 - loss: 7.7069e-04 - val_accuracy: 0.0597 - val_loss: 0.0750 - learning_rate: 3.1250e-05\n",
      "Epoch 40: early stopping\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "Predicting on validation fold...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
      "\n",
      "Fold 2 Confusion Matrix (rows=true, cols=pred) [0=Amateur, 1=Professional]:\n",
      "[[469   8]\n",
      " [  6 471]]\n",
      "TN=469, FP=8, FN=6, TP=471\n",
      "\n",
      "Fold 2 Metrics (threshold=0.5):\n",
      "Accuracy    : 0.9853\n",
      "Precision   : 0.9833  (class 1: Professional)\n",
      "Sensitivity : 0.9874     (Recall for class 1)\n",
      "Specificity : 0.9832 (for class 0: Amateur)\n",
      "F1-score    : 0.9854\n",
      "\n",
      "==========================================================================================\n",
      "Fold 3 — Train/Val split\n",
      "Train idx: (3816,) Val idx: (954,)\n",
      "X_train: (3816, 14, 200, 1) | y_train: (3816,)\n",
      "X_val  : (954, 14, 200, 1) | y_val  : (954,)\n",
      "Per-fold normalization done. train_mean shape: (1, 14, 1, 1) train_std shape: (1, 14, 1, 1)\n",
      "Normalization check — Train(mean,std)=(-0.0011, 1.0000) | Val(mean,std)=(0.0256, 0.8467) | Train per-electrode std range: [1.000, 1.000]\n",
      "Building model with input_shape: (14, 200, 1)\n",
      "After sepconv1: (None, 14, 200, 32)\n",
      "After SpatialAttention: (None, 14, 200, 32)\n",
      "After depthwise spatial: (None, 1, 200, 64)\n",
      "Residual branch shape: (None, 14, 200, 32)\n",
      "After 1x1 proj to 32: (None, 1, 200, 32)\n",
      "After tiling to electrodes=14: (None, 14, 200, 32)\n",
      "After residual add: (None, 14, 200, 32)\n",
      "After avgpool1: (None, 14, 50, 32)\n",
      "After sepconv2: (None, 14, 50, 64)\n",
      "After avgpool2: (None, 14, 12, 64)\n",
      "Before Transformer, reshaped to: (None, 14, 768)\n",
      "After GAP1D: (None, 768)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                      </span>┃<span style=\"font-weight: bold\"> Output Shape                 </span>┃<span style=\"font-weight: bold\">           Param # </span>┃<span style=\"font-weight: bold\"> Connected to              </span>\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                         \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_9                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_24            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ separable_conv2d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ global_average_pooling2d_3        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                   │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)          │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │ global_average_pooling2d_3\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">910</span> │ dense_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ reshape_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ multiply_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
       "│                                   │                              │                   │ reshape_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ depthwise_conv2d_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ multiply_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_25            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ depthwise_conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ activation_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_27            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_10               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_26            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ separable_conv2d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ lambda_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           \n",
       "│                                   │                              │                   │ activation_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_28            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ average_pooling2d_6               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ average_pooling2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_11               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">2,144</span> │ dropout_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_29            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ separable_conv2d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ average_pooling2d_7               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ average_pooling2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ reshape_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ multi_head_attention_3            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">787,968</span> │ reshape_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)              │                              │                   │ reshape_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ reshape_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          \n",
       "│                                   │                              │                   │ dropout_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ layer_normalization_6             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │ layer_normalization_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │ dropout_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>\n",
       "│                                   │                              │                   │ dense_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ layer_normalization_7             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ global_average_pooling1d_3        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                  │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)          │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_30            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> │ global_average_pooling1d_3\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  │            <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │ batch_normalization_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_31            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                    │               <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "└───────────────────────────────────┴──────────────────────────────┴───────────────────┴───────────────────────────\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m          Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ -                         \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_9                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                \u001b[38;5;34m37\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_24            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m128\u001b[0m │ separable_conv2d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_15 (\u001b[38;5;33mActivation\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_24[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ global_average_pooling2d_3        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                   │                 \u001b[38;5;34m0\u001b[0m │ activation_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)          │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   │             \u001b[38;5;34m2,112\u001b[0m │ global_average_pooling2d_3\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)                   │               \u001b[38;5;34m910\u001b[0m │ dense_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ reshape_6 (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)             │                 \u001b[38;5;34m0\u001b[0m │ dense_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ multiply_3 (\u001b[38;5;33mMultiply\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ activation_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
       "│                                   │                              │                   │ reshape_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ depthwise_conv2d_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │               \u001b[38;5;34m896\u001b[0m │ multiply_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "│ (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_25            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │               \u001b[38;5;34m256\u001b[0m │ depthwise_conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_16 (\u001b[38;5;33mActivation\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_25[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │             \u001b[38;5;34m2,048\u001b[0m │ activation_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_27            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │               \u001b[38;5;34m128\u001b[0m │ conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_10               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                \u001b[38;5;34m37\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_18 (\u001b[38;5;33mActivation\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_27[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_26            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m128\u001b[0m │ separable_conv2d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ lambda_3 (\u001b[38;5;33mLambda\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ activation_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_17 (\u001b[38;5;33mActivation\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_26[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_9 (\u001b[38;5;33mAdd\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ lambda_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           \n",
       "│                                   │                              │                   │ activation_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_28            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m128\u001b[0m │ add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ average_pooling2d_6               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_28[\u001b[38;5;34m0\u001b[0m][\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_18 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ average_pooling2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_11               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │             \u001b[38;5;34m2,144\u001b[0m │ dropout_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_29            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │               \u001b[38;5;34m256\u001b[0m │ separable_conv2d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_19 (\u001b[38;5;33mActivation\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_29[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ average_pooling2d_7               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ activation_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ average_pooling2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ reshape_7 (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ dropout_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ multi_head_attention_3            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │           \u001b[38;5;34m787,968\u001b[0m │ reshape_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          \n",
       "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)              │                              │                   │ reshape_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_21 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ multi_head_attention_3[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_10 (\u001b[38;5;33mAdd\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ reshape_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          \n",
       "│                                   │                              │                   │ dropout_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ layer_normalization_6             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │             \u001b[38;5;34m1,536\u001b[0m │ add_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              \n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │            \u001b[38;5;34m98,432\u001b[0m │ layer_normalization_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_22 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ dense_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │            \u001b[38;5;34m99,072\u001b[0m │ dropout_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_11 (\u001b[38;5;33mAdd\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ layer_normalization_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m\n",
       "│                                   │                              │                   │ dense_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ layer_normalization_7             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │             \u001b[38;5;34m1,536\u001b[0m │ add_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              \n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ global_average_pooling1d_3        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                  │                 \u001b[38;5;34m0\u001b[0m │ layer_normalization_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)          │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_30            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                  │             \u001b[38;5;34m3,072\u001b[0m │ global_average_pooling1d_3\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  │            \u001b[38;5;34m98,432\u001b[0m │ batch_normalization_30[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_31            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  │               \u001b[38;5;34m512\u001b[0m │ dense_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_23 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_31[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                    │               \u001b[38;5;34m129\u001b[0m │ dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "└───────────────────────────────────┴──────────────────────────────┴───────────────────┴───────────────────────────\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,099,897</span> (4.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,099,897\u001b[0m (4.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,097,593</span> (4.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,097,593\u001b[0m (4.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,304</span> (9.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,304\u001b[0m (9.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Setting up callbacks for fold 3...\n",
      "Training with EarlyStopping...\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.65841, saving model to best_fold_3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 30s - 251ms/step - accuracy: 0.0000e+00 - loss: 0.3738 - val_accuracy: 0.0000e+00 - val_loss: 0.6584 - learning_rate: 1.0000e-03\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 2: val_loss improved from 0.65841 to 0.58285, saving model to best_fold_3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 22s - 185ms/step - accuracy: 0.0018 - loss: 0.1808 - val_accuracy: 0.0000e+00 - val_loss: 0.5828 - learning_rate: 1.0000e-03\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 3: val_loss improved from 0.58285 to 0.52373, saving model to best_fold_3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 22s - 183ms/step - accuracy: 0.0010 - loss: 0.1380 - val_accuracy: 0.0000e+00 - val_loss: 0.5237 - learning_rate: 1.0000e-03\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 4: val_loss improved from 0.52373 to 0.41382, saving model to best_fold_3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 22s - 181ms/step - accuracy: 0.0018 - loss: 0.1100 - val_accuracy: 0.0000e+00 - val_loss: 0.4138 - learning_rate: 1.0000e-03\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 5: val_loss improved from 0.41382 to 0.10506, saving model to best_fold_3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 22s - 182ms/step - accuracy: 0.0024 - loss: 0.0903 - val_accuracy: 0.0000e+00 - val_loss: 0.1051 - learning_rate: 1.0000e-03\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 6: val_loss improved from 0.10506 to 0.09499, saving model to best_fold_3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 22s - 187ms/step - accuracy: 0.0037 - loss: 0.0884 - val_accuracy: 0.0052 - val_loss: 0.0950 - learning_rate: 1.0000e-03\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 7: val_loss improved from 0.09499 to 0.07029, saving model to best_fold_3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 22s - 180ms/step - accuracy: 0.0052 - loss: 0.0755 - val_accuracy: 0.0451 - val_loss: 0.0703 - learning_rate: 1.0000e-03\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.07029\n",
      "120/120 - 22s - 182ms/step - accuracy: 0.0068 - loss: 0.0666 - val_accuracy: 0.0073 - val_loss: 0.1079 - learning_rate: 1.0000e-03\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.07029\n",
      "120/120 - 22s - 184ms/step - accuracy: 0.0071 - loss: 0.0546 - val_accuracy: 0.0220 - val_loss: 0.0750 - learning_rate: 1.0000e-03\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.07029\n",
      "120/120 - 22s - 180ms/step - accuracy: 0.0071 - loss: 0.0557 - val_accuracy: 0.1509 - val_loss: 0.0877 - learning_rate: 1.0000e-03\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 11: val_loss improved from 0.07029 to 0.06951, saving model to best_fold_3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 175ms/step - accuracy: 0.0066 - loss: 0.0509 - val_accuracy: 0.0346 - val_loss: 0.0695 - learning_rate: 1.0000e-03\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.06951\n",
      "120/120 - 21s - 176ms/step - accuracy: 0.0092 - loss: 0.0477 - val_accuracy: 0.0052 - val_loss: 0.1132 - learning_rate: 1.0000e-03\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 13: val_loss improved from 0.06951 to 0.06369, saving model to best_fold_3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 23s - 188ms/step - accuracy: 0.0079 - loss: 0.0471 - val_accuracy: 0.0094 - val_loss: 0.0637 - learning_rate: 1.0000e-03\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 14: val_loss improved from 0.06369 to 0.06059, saving model to best_fold_3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 179ms/step - accuracy: 0.0066 - loss: 0.0415 - val_accuracy: 0.0566 - val_loss: 0.0606 - learning_rate: 1.0000e-03\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.06059\n",
      "120/120 - 22s - 184ms/step - accuracy: 0.0060 - loss: 0.0402 - val_accuracy: 0.1090 - val_loss: 0.0911 - learning_rate: 1.0000e-03\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.06059\n",
      "120/120 - 22s - 182ms/step - accuracy: 0.0152 - loss: 0.0393 - val_accuracy: 0.0021 - val_loss: 0.0713 - learning_rate: 1.0000e-03\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 17: val_loss improved from 0.06059 to 0.05775, saving model to best_fold_3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 22s - 184ms/step - accuracy: 0.0089 - loss: 0.0355 - val_accuracy: 0.0713 - val_loss: 0.0577 - learning_rate: 1.0000e-03\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.05775\n",
      "120/120 - 23s - 189ms/step - accuracy: 0.0110 - loss: 0.0369 - val_accuracy: 0.0084 - val_loss: 0.0701 - learning_rate: 1.0000e-03\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.05775\n",
      "120/120 - 21s - 177ms/step - accuracy: 0.0181 - loss: 0.0241 - val_accuracy: 0.0241 - val_loss: 0.1118 - learning_rate: 1.0000e-03\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.05775\n",
      "120/120 - 22s - 182ms/step - accuracy: 0.0215 - loss: 0.0226 - val_accuracy: 0.0073 - val_loss: 0.0613 - learning_rate: 1.0000e-03\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.05775\n",
      "120/120 - 22s - 181ms/step - accuracy: 0.0362 - loss: 0.0274 - val_accuracy: 0.0430 - val_loss: 0.1025 - learning_rate: 1.0000e-03\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 22: val_loss improved from 0.05775 to 0.05267, saving model to best_fold_3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 22s - 187ms/step - accuracy: 0.0231 - loss: 0.0182 - val_accuracy: 0.1279 - val_loss: 0.0527 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.05267\n",
      "120/120 - 21s - 178ms/step - accuracy: 0.0265 - loss: 0.0118 - val_accuracy: 0.0451 - val_loss: 0.0583 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.05267\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0343 - loss: 0.0143 - val_accuracy: 0.1080 - val_loss: 0.0625 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 25: val_loss improved from 0.05267 to 0.05164, saving model to best_fold_3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 172ms/step - accuracy: 0.0322 - loss: 0.0068 - val_accuracy: 0.1373 - val_loss: 0.0516 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.05164\n",
      "120/120 - 21s - 175ms/step - accuracy: 0.0335 - loss: 0.0058 - val_accuracy: 0.2201 - val_loss: 0.0536 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.05164\n",
      "120/120 - 20s - 170ms/step - accuracy: 0.0445 - loss: 0.0045 - val_accuracy: 0.1247 - val_loss: 0.0578 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.05164\n",
      "120/120 - 21s - 176ms/step - accuracy: 0.0524 - loss: 0.0047 - val_accuracy: 0.2264 - val_loss: 0.0575 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.05164\n",
      "120/120 - 21s - 176ms/step - accuracy: 0.0490 - loss: 0.0066 - val_accuracy: 0.1237 - val_loss: 0.0521 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.05164\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0506 - loss: 0.0032 - val_accuracy: 0.2065 - val_loss: 0.0540 - learning_rate: 2.5000e-04\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 31: val_loss improved from 0.05164 to 0.04773, saving model to best_fold_3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 174ms/step - accuracy: 0.0608 - loss: 0.0066 - val_accuracy: 0.1803 - val_loss: 0.0477 - learning_rate: 2.5000e-04\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.04773\n",
      "120/120 - 21s - 172ms/step - accuracy: 0.0624 - loss: 0.0021 - val_accuracy: 0.2809 - val_loss: 0.0519 - learning_rate: 2.5000e-04\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.04773\n",
      "120/120 - 21s - 171ms/step - accuracy: 0.0621 - loss: 0.0027 - val_accuracy: 0.2778 - val_loss: 0.0528 - learning_rate: 2.5000e-04\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.04773\n",
      "120/120 - 21s - 172ms/step - accuracy: 0.0768 - loss: 0.0016 - val_accuracy: 0.3071 - val_loss: 0.0567 - learning_rate: 2.5000e-04\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.04773\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0710 - loss: 0.0014 - val_accuracy: 0.2610 - val_loss: 0.0564 - learning_rate: 2.5000e-04\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.04773\n",
      "120/120 - 21s - 175ms/step - accuracy: 0.0786 - loss: 0.0013 - val_accuracy: 0.3092 - val_loss: 0.0595 - learning_rate: 1.2500e-04\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.04773\n",
      "120/120 - 21s - 172ms/step - accuracy: 0.0789 - loss: 0.0012 - val_accuracy: 0.2799 - val_loss: 0.0580 - learning_rate: 1.2500e-04\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.04773\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0755 - loss: 6.9468e-04 - val_accuracy: 0.2925 - val_loss: 0.0592 - learning_rate: 1.2500e-04\n",
      "Epoch 39/200\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.04773\n",
      "120/120 - 21s - 172ms/step - accuracy: 0.0833 - loss: 0.0010 - val_accuracy: 0.2778 - val_loss: 0.0581 - learning_rate: 1.2500e-04\n",
      "Epoch 40/200\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.04773\n",
      "120/120 - 21s - 173ms/step - accuracy: 0.0839 - loss: 8.9844e-04 - val_accuracy: 0.3040 - val_loss: 0.0581 - learning_rate: 6.2500e-05\n",
      "Epoch 41/200\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.04773\n",
      "120/120 - 21s - 176ms/step - accuracy: 0.0873 - loss: 7.6936e-04 - val_accuracy: 0.3071 - val_loss: 0.0580 - learning_rate: 6.2500e-05\n",
      "Epoch 42/200\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.04773\n",
      "120/120 - 25s - 210ms/step - accuracy: 0.0797 - loss: 6.2654e-04 - val_accuracy: 0.3124 - val_loss: 0.0589 - learning_rate: 6.2500e-05\n",
      "Epoch 43/200\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.04773\n",
      "120/120 - 24s - 197ms/step - accuracy: 0.0870 - loss: 5.3211e-04 - val_accuracy: 0.3134 - val_loss: 0.0589 - learning_rate: 6.2500e-05\n",
      "Epoch 44/200\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.04773\n",
      "120/120 - 21s - 172ms/step - accuracy: 0.0828 - loss: 4.8969e-04 - val_accuracy: 0.3134 - val_loss: 0.0588 - learning_rate: 3.1250e-05\n",
      "Epoch 45/200\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.04773\n",
      "120/120 - 20s - 168ms/step - accuracy: 0.0891 - loss: 7.7013e-04 - val_accuracy: 0.3197 - val_loss: 0.0586 - learning_rate: 3.1250e-05\n",
      "Epoch 46/200\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.04773\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0933 - loss: 6.2935e-04 - val_accuracy: 0.3166 - val_loss: 0.0587 - learning_rate: 3.1250e-05\n",
      "Epoch 46: early stopping\n",
      "Restoring model weights from the end of the best epoch: 31.\n",
      "Predicting on validation fold...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step\n",
      "\n",
      "Fold 3 Confusion Matrix (rows=true, cols=pred) [0=Amateur, 1=Professional]:\n",
      "[[472   5]\n",
      " [  2 475]]\n",
      "TN=472, FP=5, FN=2, TP=475\n",
      "\n",
      "Fold 3 Metrics (threshold=0.5):\n",
      "Accuracy    : 0.9927\n",
      "Precision   : 0.9896  (class 1: Professional)\n",
      "Sensitivity : 0.9958     (Recall for class 1)\n",
      "Specificity : 0.9895 (for class 0: Amateur)\n",
      "F1-score    : 0.9927\n",
      "\n",
      "==========================================================================================\n",
      "Fold 4 — Train/Val split\n",
      "Train idx: (3816,) Val idx: (954,)\n",
      "X_train: (3816, 14, 200, 1) | y_train: (3816,)\n",
      "X_val  : (954, 14, 200, 1) | y_val  : (954,)\n",
      "Per-fold normalization done. train_mean shape: (1, 14, 1, 1) train_std shape: (1, 14, 1, 1)\n",
      "Normalization check — Train(mean,std)=(-0.0012, 1.0000) | Val(mean,std)=(-0.0057, 0.8997) | Train per-electrode std range: [1.000, 1.000]\n",
      "Building model with input_shape: (14, 200, 1)\n",
      "After sepconv1: (None, 14, 200, 32)\n",
      "After SpatialAttention: (None, 14, 200, 32)\n",
      "After depthwise spatial: (None, 1, 200, 64)\n",
      "Residual branch shape: (None, 14, 200, 32)\n",
      "After 1x1 proj to 32: (None, 1, 200, 32)\n",
      "After tiling to electrodes=14: (None, 14, 200, 32)\n",
      "After residual add: (None, 14, 200, 32)\n",
      "After avgpool1: (None, 14, 50, 32)\n",
      "After sepconv2: (None, 14, 50, 64)\n",
      "After avgpool2: (None, 14, 12, 64)\n",
      "Before Transformer, reshaped to: (None, 14, 768)\n",
      "After GAP1D: (None, 768)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                      </span>┃<span style=\"font-weight: bold\"> Output Shape                 </span>┃<span style=\"font-weight: bold\">           Param # </span>┃<span style=\"font-weight: bold\"> Connected to              </span>\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                         \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_12               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_32            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ separable_conv2d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ global_average_pooling2d_4        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                   │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)          │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │ global_average_pooling2d_4\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">910</span> │ dense_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ reshape_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ multiply_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
       "│                                   │                              │                   │ reshape_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ depthwise_conv2d_4                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ multiply_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_33            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ depthwise_conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ activation_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_35            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_13               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_34            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ separable_conv2d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ lambda_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           \n",
       "│                                   │                              │                   │ activation_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_36            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ average_pooling2d_8               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ average_pooling2d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_14               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">2,144</span> │ dropout_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_37            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ separable_conv2d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ average_pooling2d_9               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ average_pooling2d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ reshape_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ multi_head_attention_4            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">787,968</span> │ reshape_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)              │                              │                   │ reshape_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ reshape_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          \n",
       "│                                   │                              │                   │ dropout_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ layer_normalization_8             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │ layer_normalization_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │ dropout_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>\n",
       "│                                   │                              │                   │ dense_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ layer_normalization_9             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ global_average_pooling1d_4        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                  │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)          │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_38            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> │ global_average_pooling1d_4\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  │            <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │ batch_normalization_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_39            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                    │               <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "└───────────────────────────────────┴──────────────────────────────┴───────────────────┴───────────────────────────\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m          Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ -                         \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_12               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                \u001b[38;5;34m37\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_32            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m128\u001b[0m │ separable_conv2d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_20 (\u001b[38;5;33mActivation\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_32[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ global_average_pooling2d_4        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                   │                 \u001b[38;5;34m0\u001b[0m │ activation_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)          │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   │             \u001b[38;5;34m2,112\u001b[0m │ global_average_pooling2d_4\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_25 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)                   │               \u001b[38;5;34m910\u001b[0m │ dense_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ reshape_8 (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)             │                 \u001b[38;5;34m0\u001b[0m │ dense_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ multiply_4 (\u001b[38;5;33mMultiply\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ activation_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
       "│                                   │                              │                   │ reshape_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ depthwise_conv2d_4                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │               \u001b[38;5;34m896\u001b[0m │ multiply_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "│ (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_33            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │               \u001b[38;5;34m256\u001b[0m │ depthwise_conv2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_21 (\u001b[38;5;33mActivation\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_33[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │             \u001b[38;5;34m2,048\u001b[0m │ activation_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_35            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │               \u001b[38;5;34m128\u001b[0m │ conv2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_13               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                \u001b[38;5;34m37\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_23 (\u001b[38;5;33mActivation\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_35[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_34            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m128\u001b[0m │ separable_conv2d_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ lambda_4 (\u001b[38;5;33mLambda\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ activation_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_22 (\u001b[38;5;33mActivation\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_34[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_12 (\u001b[38;5;33mAdd\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ lambda_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           \n",
       "│                                   │                              │                   │ activation_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_36            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m128\u001b[0m │ add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ average_pooling2d_8               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_36[\u001b[38;5;34m0\u001b[0m][\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ average_pooling2d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_14               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │             \u001b[38;5;34m2,144\u001b[0m │ dropout_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_37            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │               \u001b[38;5;34m256\u001b[0m │ separable_conv2d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_24 (\u001b[38;5;33mActivation\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_37[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ average_pooling2d_9               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ activation_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ average_pooling2d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ reshape_9 (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ multi_head_attention_4            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │           \u001b[38;5;34m787,968\u001b[0m │ reshape_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          \n",
       "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)              │                              │                   │ reshape_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_27 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ multi_head_attention_4[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_13 (\u001b[38;5;33mAdd\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ reshape_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          \n",
       "│                                   │                              │                   │ dropout_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ layer_normalization_8             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │             \u001b[38;5;34m1,536\u001b[0m │ add_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              \n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │            \u001b[38;5;34m98,432\u001b[0m │ layer_normalization_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_28 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ dense_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │            \u001b[38;5;34m99,072\u001b[0m │ dropout_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_14 (\u001b[38;5;33mAdd\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ layer_normalization_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m\n",
       "│                                   │                              │                   │ dense_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ layer_normalization_9             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │             \u001b[38;5;34m1,536\u001b[0m │ add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              \n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ global_average_pooling1d_4        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                  │                 \u001b[38;5;34m0\u001b[0m │ layer_normalization_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)          │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_38            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                  │             \u001b[38;5;34m3,072\u001b[0m │ global_average_pooling1d_4\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  │            \u001b[38;5;34m98,432\u001b[0m │ batch_normalization_38[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_39            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  │               \u001b[38;5;34m512\u001b[0m │ dense_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_29 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_39[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                    │               \u001b[38;5;34m129\u001b[0m │ dropout_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "└───────────────────────────────────┴──────────────────────────────┴───────────────────┴───────────────────────────\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,099,897</span> (4.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,099,897\u001b[0m (4.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,097,593</span> (4.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,097,593\u001b[0m (4.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,304</span> (9.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,304\u001b[0m (9.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Setting up callbacks for fold 4...\n",
      "Training with EarlyStopping...\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.85088, saving model to best_fold_4.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 31s - 259ms/step - accuracy: 0.0000e+00 - loss: 0.3924 - val_accuracy: 0.0000e+00 - val_loss: 1.8509 - learning_rate: 1.0000e-03\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 2: val_loss improved from 1.85088 to 0.57389, saving model to best_fold_4.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 22s - 185ms/step - accuracy: 0.0000e+00 - loss: 0.1867 - val_accuracy: 0.0000e+00 - val_loss: 0.5739 - learning_rate: 1.0000e-03\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 3: val_loss improved from 0.57389 to 0.32001, saving model to best_fold_4.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 177ms/step - accuracy: 7.8616e-04 - loss: 0.1253 - val_accuracy: 0.0000e+00 - val_loss: 0.3200 - learning_rate: 1.0000e-03\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 4: val_loss improved from 0.32001 to 0.14276, saving model to best_fold_4.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 176ms/step - accuracy: 0.0021 - loss: 0.1034 - val_accuracy: 0.0000e+00 - val_loss: 0.1428 - learning_rate: 1.0000e-03\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 5: val_loss improved from 0.14276 to 0.09061, saving model to best_fold_4.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 176ms/step - accuracy: 0.0026 - loss: 0.0796 - val_accuracy: 0.0000e+00 - val_loss: 0.0906 - learning_rate: 1.0000e-03\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.09061\n",
      "120/120 - 21s - 177ms/step - accuracy: 7.8616e-04 - loss: 0.0834 - val_accuracy: 0.0042 - val_loss: 0.0908 - learning_rate: 1.0000e-03\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.09061\n",
      "120/120 - 21s - 175ms/step - accuracy: 0.0034 - loss: 0.0735 - val_accuracy: 0.0000e+00 - val_loss: 0.1323 - learning_rate: 1.0000e-03\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.09061\n",
      "120/120 - 21s - 175ms/step - accuracy: 0.0026 - loss: 0.0605 - val_accuracy: 0.0000e+00 - val_loss: 0.0945 - learning_rate: 1.0000e-03\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.09061\n",
      "120/120 - 21s - 177ms/step - accuracy: 0.0060 - loss: 0.0588 - val_accuracy: 0.0000e+00 - val_loss: 0.1107 - learning_rate: 1.0000e-03\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 10: val_loss improved from 0.09061 to 0.05293, saving model to best_fold_4.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 175ms/step - accuracy: 0.0034 - loss: 0.0426 - val_accuracy: 0.0273 - val_loss: 0.0529 - learning_rate: 5.0000e-04\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.05293\n",
      "120/120 - 21s - 172ms/step - accuracy: 0.0052 - loss: 0.0295 - val_accuracy: 0.0136 - val_loss: 0.0654 - learning_rate: 5.0000e-04\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.05293\n",
      "120/120 - 21s - 173ms/step - accuracy: 0.0052 - loss: 0.0240 - val_accuracy: 0.0482 - val_loss: 0.0678 - learning_rate: 5.0000e-04\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.05293\n",
      "120/120 - 21s - 177ms/step - accuracy: 0.0084 - loss: 0.0241 - val_accuracy: 0.0535 - val_loss: 0.0678 - learning_rate: 5.0000e-04\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.05293\n",
      "120/120 - 21s - 178ms/step - accuracy: 0.0094 - loss: 0.0164 - val_accuracy: 0.1394 - val_loss: 0.0599 - learning_rate: 5.0000e-04\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.05293\n",
      "120/120 - 21s - 173ms/step - accuracy: 0.0097 - loss: 0.0165 - val_accuracy: 0.0650 - val_loss: 0.0545 - learning_rate: 2.5000e-04\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.05293\n",
      "120/120 - 21s - 173ms/step - accuracy: 0.0089 - loss: 0.0123 - val_accuracy: 0.0618 - val_loss: 0.0560 - learning_rate: 2.5000e-04\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.05293\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0126 - loss: 0.0105 - val_accuracy: 0.0922 - val_loss: 0.0773 - learning_rate: 2.5000e-04\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.05293\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0147 - loss: 0.0096 - val_accuracy: 0.0608 - val_loss: 0.0583 - learning_rate: 2.5000e-04\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.05293\n",
      "120/120 - 21s - 175ms/step - accuracy: 0.0131 - loss: 0.0065 - val_accuracy: 0.0786 - val_loss: 0.0632 - learning_rate: 1.2500e-04\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.05293\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0110 - loss: 0.0063 - val_accuracy: 0.0618 - val_loss: 0.0570 - learning_rate: 1.2500e-04\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.05293\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0121 - loss: 0.0042 - val_accuracy: 0.0482 - val_loss: 0.0586 - learning_rate: 1.2500e-04\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.05293\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0100 - loss: 0.0053 - val_accuracy: 0.0440 - val_loss: 0.0655 - learning_rate: 1.2500e-04\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.05293\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0181 - loss: 0.0040 - val_accuracy: 0.0377 - val_loss: 0.0548 - learning_rate: 6.2500e-05\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 24: val_loss improved from 0.05293 to 0.05186, saving model to best_fold_4.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 175ms/step - accuracy: 0.0121 - loss: 0.0047 - val_accuracy: 0.0377 - val_loss: 0.0519 - learning_rate: 6.2500e-05\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.05186\n",
      "120/120 - 21s - 175ms/step - accuracy: 0.0128 - loss: 0.0039 - val_accuracy: 0.0325 - val_loss: 0.0545 - learning_rate: 6.2500e-05\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.05186\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0126 - loss: 0.0037 - val_accuracy: 0.0325 - val_loss: 0.0530 - learning_rate: 6.2500e-05\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.05186\n",
      "120/120 - 21s - 175ms/step - accuracy: 0.0107 - loss: 0.0025 - val_accuracy: 0.0157 - val_loss: 0.0579 - learning_rate: 6.2500e-05\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.05186\n",
      "120/120 - 22s - 181ms/step - accuracy: 0.0123 - loss: 0.0030 - val_accuracy: 0.0262 - val_loss: 0.0649 - learning_rate: 6.2500e-05\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.05186\n",
      "120/120 - 21s - 176ms/step - accuracy: 0.0144 - loss: 0.0020 - val_accuracy: 0.0199 - val_loss: 0.0624 - learning_rate: 3.1250e-05\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.05186\n",
      "120/120 - 21s - 176ms/step - accuracy: 0.0113 - loss: 0.0019 - val_accuracy: 0.0147 - val_loss: 0.0606 - learning_rate: 3.1250e-05\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.05186\n",
      "120/120 - 21s - 175ms/step - accuracy: 0.0126 - loss: 0.0026 - val_accuracy: 0.0126 - val_loss: 0.0628 - learning_rate: 3.1250e-05\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.05186\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0149 - loss: 0.0017 - val_accuracy: 0.0094 - val_loss: 0.0637 - learning_rate: 3.1250e-05\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.05186\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0123 - loss: 0.0023 - val_accuracy: 0.0094 - val_loss: 0.0634 - learning_rate: 1.5625e-05\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.05186\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0170 - loss: 0.0018 - val_accuracy: 0.0094 - val_loss: 0.0652 - learning_rate: 1.5625e-05\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.05186\n",
      "120/120 - 21s - 176ms/step - accuracy: 0.0157 - loss: 0.0025 - val_accuracy: 0.0094 - val_loss: 0.0635 - learning_rate: 1.5625e-05\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.05186\n",
      "120/120 - 21s - 173ms/step - accuracy: 0.0147 - loss: 0.0016 - val_accuracy: 0.0094 - val_loss: 0.0636 - learning_rate: 1.5625e-05\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.05186\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0139 - loss: 0.0018 - val_accuracy: 0.0073 - val_loss: 0.0635 - learning_rate: 7.8125e-06\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.05186\n",
      "120/120 - 22s - 182ms/step - accuracy: 0.0121 - loss: 0.0021 - val_accuracy: 0.0084 - val_loss: 0.0658 - learning_rate: 7.8125e-06\n",
      "Epoch 39/200\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.05186\n",
      "120/120 - 21s - 179ms/step - accuracy: 0.0149 - loss: 0.0014 - val_accuracy: 0.0073 - val_loss: 0.0660 - learning_rate: 7.8125e-06\n",
      "Epoch 39: early stopping\n",
      "Restoring model weights from the end of the best epoch: 24.\n",
      "Predicting on validation fold...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step\n",
      "\n",
      "Fold 4 Confusion Matrix (rows=true, cols=pred) [0=Amateur, 1=Professional]:\n",
      "[[475   2]\n",
      " [  7 470]]\n",
      "TN=475, FP=2, FN=7, TP=470\n",
      "\n",
      "Fold 4 Metrics (threshold=0.5):\n",
      "Accuracy    : 0.9906\n",
      "Precision   : 0.9958  (class 1: Professional)\n",
      "Sensitivity : 0.9853     (Recall for class 1)\n",
      "Specificity : 0.9958 (for class 0: Amateur)\n",
      "F1-score    : 0.9905\n",
      "\n",
      "==========================================================================================\n",
      "Fold 5 — Train/Val split\n",
      "Train idx: (3816,) Val idx: (954,)\n",
      "X_train: (3816, 14, 200, 1) | y_train: (3816,)\n",
      "X_val  : (954, 14, 200, 1) | y_val  : (954,)\n",
      "Per-fold normalization done. train_mean shape: (1, 14, 1, 1) train_std shape: (1, 14, 1, 1)\n",
      "Normalization check — Train(mean,std)=(-0.0012, 1.0000) | Val(mean,std)=(-0.0286, 0.9224) | Train per-electrode std range: [1.000, 1.000]\n",
      "Building model with input_shape: (14, 200, 1)\n",
      "After sepconv1: (None, 14, 200, 32)\n",
      "After SpatialAttention: (None, 14, 200, 32)\n",
      "After depthwise spatial: (None, 1, 200, 64)\n",
      "Residual branch shape: (None, 14, 200, 32)\n",
      "After 1x1 proj to 32: (None, 1, 200, 32)\n",
      "After tiling to electrodes=14: (None, 14, 200, 32)\n",
      "After residual add: (None, 14, 200, 32)\n",
      "After avgpool1: (None, 14, 50, 32)\n",
      "After sepconv2: (None, 14, 50, 64)\n",
      "After avgpool2: (None, 14, 12, 64)\n",
      "Before Transformer, reshaped to: (None, 14, 768)\n",
      "After GAP1D: (None, 768)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                      </span>┃<span style=\"font-weight: bold\"> Output Shape                 </span>┃<span style=\"font-weight: bold\">           Param # </span>┃<span style=\"font-weight: bold\"> Connected to              </span>\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
       "│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                         \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_15               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span> │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_40            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ separable_conv2d_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ global_average_pooling2d_5        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                   │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)          │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │ global_average_pooling2d_5\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">910</span> │ dense_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ reshape_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ multiply_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
       "│                                   │                              │                   │ reshape_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ depthwise_conv2d_5                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ multiply_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_41            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ depthwise_conv2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ activation_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_43            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_16               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span> │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_42            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ separable_conv2d_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ lambda_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           \n",
       "│                                   │                              │                   │ activation_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_44            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ average_pooling2d_10              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ average_pooling2d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_17               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">2,144</span> │ dropout_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_45            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ separable_conv2d_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ average_pooling2d_11              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ average_pooling2d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ reshape_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ multi_head_attention_5            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">787,968</span> │ reshape_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)              │                              │                   │ reshape_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ reshape_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         \n",
       "│                                   │                              │                   │ dropout_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ layer_normalization_10            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │ layer_normalization_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │ dropout_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "│                                   │                              │                   │ dense_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ layer_normalization_11            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ global_average_pooling1d_5        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                  │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)          │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_46            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> │ global_average_pooling1d_5\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  │            <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │ batch_normalization_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_47            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                    │               <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "└───────────────────────────────────┴──────────────────────────────┴───────────────────┴───────────────────────────\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m          Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
       "│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ -                         \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_15               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                \u001b[38;5;34m37\u001b[0m │ input_layer_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_40            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m128\u001b[0m │ separable_conv2d_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_25 (\u001b[38;5;33mActivation\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_40[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ global_average_pooling2d_5        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                   │                 \u001b[38;5;34m0\u001b[0m │ activation_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)          │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   │             \u001b[38;5;34m2,112\u001b[0m │ global_average_pooling2d_5\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)                   │               \u001b[38;5;34m910\u001b[0m │ dense_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ reshape_10 (\u001b[38;5;33mReshape\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)             │                 \u001b[38;5;34m0\u001b[0m │ dense_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ multiply_5 (\u001b[38;5;33mMultiply\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ activation_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
       "│                                   │                              │                   │ reshape_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ depthwise_conv2d_5                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │               \u001b[38;5;34m896\u001b[0m │ multiply_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "│ (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_41            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │               \u001b[38;5;34m256\u001b[0m │ depthwise_conv2d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_26 (\u001b[38;5;33mActivation\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_41[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │             \u001b[38;5;34m2,048\u001b[0m │ activation_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_43            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │               \u001b[38;5;34m128\u001b[0m │ conv2d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_16               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                \u001b[38;5;34m37\u001b[0m │ input_layer_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_28 (\u001b[38;5;33mActivation\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_43[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_42            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m128\u001b[0m │ separable_conv2d_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ lambda_5 (\u001b[38;5;33mLambda\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ activation_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_27 (\u001b[38;5;33mActivation\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_42[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_15 (\u001b[38;5;33mAdd\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │                 \u001b[38;5;34m0\u001b[0m │ lambda_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           \n",
       "│                                   │                              │                   │ activation_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_44            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m128\u001b[0m │ add_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ average_pooling2d_10              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_44[\u001b[38;5;34m0\u001b[0m][\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_30 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ average_pooling2d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ separable_conv2d_17               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │             \u001b[38;5;34m2,144\u001b[0m │ dropout_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)                 │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_45            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │               \u001b[38;5;34m256\u001b[0m │ separable_conv2d_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ activation_29 (\u001b[38;5;33mActivation\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_45[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ average_pooling2d_11              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ activation_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_31 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │                 \u001b[38;5;34m0\u001b[0m │ average_pooling2d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ reshape_11 (\u001b[38;5;33mReshape\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ dropout_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ multi_head_attention_5            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │           \u001b[38;5;34m787,968\u001b[0m │ reshape_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         \n",
       "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)              │                              │                   │ reshape_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_33 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ multi_head_attention_5[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_16 (\u001b[38;5;33mAdd\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ reshape_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         \n",
       "│                                   │                              │                   │ dropout_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ layer_normalization_10            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │             \u001b[38;5;34m1,536\u001b[0m │ add_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              \n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_32 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │            \u001b[38;5;34m98,432\u001b[0m │ layer_normalization_10[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_34 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ dense_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_33 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │            \u001b[38;5;34m99,072\u001b[0m │ dropout_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ add_17 (\u001b[38;5;33mAdd\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │                 \u001b[38;5;34m0\u001b[0m │ layer_normalization_10[\u001b[38;5;34m0\u001b[0m][\n",
       "│                                   │                              │                   │ dense_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ layer_normalization_11            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)              │             \u001b[38;5;34m1,536\u001b[0m │ add_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              \n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ global_average_pooling1d_5        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                  │                 \u001b[38;5;34m0\u001b[0m │ layer_normalization_11[\u001b[38;5;34m0\u001b[0m][\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)          │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_46            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                  │             \u001b[38;5;34m3,072\u001b[0m │ global_average_pooling1d_5\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_34 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  │            \u001b[38;5;34m98,432\u001b[0m │ batch_normalization_46[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ batch_normalization_47            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  │               \u001b[38;5;34m512\u001b[0m │ dense_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)              │                              │                   │                           \n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dropout_35 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  │                 \u001b[38;5;34m0\u001b[0m │ batch_normalization_47[\u001b[38;5;34m0\u001b[0m][\n",
       "├───────────────────────────────────┼──────────────────────────────┼───────────────────┼───────────────────────────\n",
       "│ dense_35 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                    │               \u001b[38;5;34m129\u001b[0m │ dropout_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "└───────────────────────────────────┴──────────────────────────────┴───────────────────┴───────────────────────────\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,099,897</span> (4.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,099,897\u001b[0m (4.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,097,593</span> (4.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,097,593\u001b[0m (4.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,304</span> (9.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,304\u001b[0m (9.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Setting up callbacks for fold 5...\n",
      "Training with EarlyStopping...\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 1: val_loss improved from None to 3.25698, saving model to best_fold_5.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 28s - 231ms/step - accuracy: 0.0000e+00 - loss: 0.3576 - val_accuracy: 0.0000e+00 - val_loss: 3.2570 - learning_rate: 1.0000e-03\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 2: val_loss improved from 3.25698 to 1.57647, saving model to best_fold_5.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 174ms/step - accuracy: 2.6205e-04 - loss: 0.1666 - val_accuracy: 0.0000e+00 - val_loss: 1.5765 - learning_rate: 1.0000e-03\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 3: val_loss improved from 1.57647 to 0.43208, saving model to best_fold_5.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 173ms/step - accuracy: 5.2411e-04 - loss: 0.1157 - val_accuracy: 0.0000e+00 - val_loss: 0.4321 - learning_rate: 1.0000e-03\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 4: val_loss improved from 0.43208 to 0.07358, saving model to best_fold_5.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 174ms/step - accuracy: 7.8616e-04 - loss: 0.0946 - val_accuracy: 0.0000e+00 - val_loss: 0.0736 - learning_rate: 1.0000e-03\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.07358\n",
      "120/120 - 21s - 172ms/step - accuracy: 0.0029 - loss: 0.0851 - val_accuracy: 0.0000e+00 - val_loss: 0.1165 - learning_rate: 1.0000e-03\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 6: val_loss improved from 0.07358 to 0.06515, saving model to best_fold_5.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 175ms/step - accuracy: 0.0026 - loss: 0.0742 - val_accuracy: 0.0010 - val_loss: 0.0651 - learning_rate: 1.0000e-03\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.06515\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0018 - loss: 0.0650 - val_accuracy: 0.0199 - val_loss: 0.1760 - learning_rate: 1.0000e-03\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.06515\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0031 - loss: 0.0676 - val_accuracy: 0.0294 - val_loss: 0.1342 - learning_rate: 1.0000e-03\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.06515\n",
      "120/120 - 21s - 175ms/step - accuracy: 0.0037 - loss: 0.0558 - val_accuracy: 0.0524 - val_loss: 0.1070 - learning_rate: 1.0000e-03\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.06515\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0016 - loss: 0.0526 - val_accuracy: 0.0241 - val_loss: 0.0732 - learning_rate: 1.0000e-03\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 11: val_loss improved from 0.06515 to 0.05524, saving model to best_fold_5.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 - 21s - 174ms/step - accuracy: 0.0071 - loss: 0.0347 - val_accuracy: 0.0356 - val_loss: 0.0552 - learning_rate: 5.0000e-04\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.05524\n",
      "120/120 - 21s - 172ms/step - accuracy: 0.0084 - loss: 0.0271 - val_accuracy: 0.0860 - val_loss: 0.0569 - learning_rate: 5.0000e-04\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.05524\n",
      "120/120 - 21s - 173ms/step - accuracy: 0.0110 - loss: 0.0223 - val_accuracy: 0.0482 - val_loss: 0.0658 - learning_rate: 5.0000e-04\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.05524\n",
      "120/120 - 21s - 173ms/step - accuracy: 0.0118 - loss: 0.0203 - val_accuracy: 0.0765 - val_loss: 0.0827 - learning_rate: 5.0000e-04\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.05524\n",
      "120/120 - 21s - 172ms/step - accuracy: 0.0168 - loss: 0.0167 - val_accuracy: 0.1593 - val_loss: 0.0795 - learning_rate: 5.0000e-04\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.05524\n",
      "120/120 - 21s - 172ms/step - accuracy: 0.0173 - loss: 0.0136 - val_accuracy: 0.1468 - val_loss: 0.0796 - learning_rate: 2.5000e-04\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.05524\n",
      "120/120 - 21s - 177ms/step - accuracy: 0.0176 - loss: 0.0099 - val_accuracy: 0.0975 - val_loss: 0.0716 - learning_rate: 2.5000e-04\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.05524\n",
      "120/120 - 21s - 174ms/step - accuracy: 0.0181 - loss: 0.0094 - val_accuracy: 0.1331 - val_loss: 0.0691 - learning_rate: 2.5000e-04\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.05524\n",
      "120/120 - 21s - 173ms/step - accuracy: 0.0223 - loss: 0.0074 - val_accuracy: 0.1174 - val_loss: 0.0720 - learning_rate: 2.5000e-04\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.05524\n",
      "120/120 - 21s - 173ms/step - accuracy: 0.0218 - loss: 0.0064 - val_accuracy: 0.1583 - val_loss: 0.0727 - learning_rate: 1.2500e-04\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.05524\n",
      "120/120 - 21s - 172ms/step - accuracy: 0.0267 - loss: 0.0060 - val_accuracy: 0.1855 - val_loss: 0.0736 - learning_rate: 1.2500e-04\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.05524\n",
      "120/120 - 21s - 172ms/step - accuracy: 0.0296 - loss: 0.0056 - val_accuracy: 0.1698 - val_loss: 0.0739 - learning_rate: 1.2500e-04\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.05524\n",
      "120/120 - 21s - 172ms/step - accuracy: 0.0312 - loss: 0.0031 - val_accuracy: 0.1834 - val_loss: 0.0788 - learning_rate: 1.2500e-04\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.05524\n",
      "120/120 - 21s - 172ms/step - accuracy: 0.0291 - loss: 0.0047 - val_accuracy: 0.1541 - val_loss: 0.0770 - learning_rate: 6.2500e-05\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.05524\n",
      "120/120 - 21s - 173ms/step - accuracy: 0.0286 - loss: 0.0032 - val_accuracy: 0.1488 - val_loss: 0.0746 - learning_rate: 6.2500e-05\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.05524\n",
      "120/120 - 21s - 172ms/step - accuracy: 0.0241 - loss: 0.0028 - val_accuracy: 0.1226 - val_loss: 0.0793 - learning_rate: 6.2500e-05\n",
      "Epoch 26: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Predicting on validation fold...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step\n",
      "\n",
      "Fold 5 Confusion Matrix (rows=true, cols=pred) [0=Amateur, 1=Professional]:\n",
      "[[470   7]\n",
      " [  6 471]]\n",
      "TN=470, FP=7, FN=6, TP=471\n",
      "\n",
      "Fold 5 Metrics (threshold=0.5):\n",
      "Accuracy    : 0.9864\n",
      "Precision   : 0.9854  (class 1: Professional)\n",
      "Sensitivity : 0.9874     (Recall for class 1)\n",
      "Specificity : 0.9853 (for class 0: Amateur)\n",
      "F1-score    : 0.9864\n",
      "\n",
      "=== Per-Fold Metrics Summary ===\n",
      "Fold 1: Acc=0.9853, Prec=0.9754, Sens=0.9958, Spec=0.9748, F1=0.9855 | TN=465 FP=12 FN=2 TP=475\n",
      "Fold 2: Acc=0.9853, Prec=0.9833, Sens=0.9874, Spec=0.9832, F1=0.9854 | TN=469 FP=8 FN=6 TP=471\n",
      "Fold 3: Acc=0.9927, Prec=0.9896, Sens=0.9958, Spec=0.9895, F1=0.9927 | TN=472 FP=5 FN=2 TP=475\n",
      "Fold 4: Acc=0.9906, Prec=0.9958, Sens=0.9853, Spec=0.9958, F1=0.9905 | TN=475 FP=2 FN=7 TP=470\n",
      "Fold 5: Acc=0.9864, Prec=0.9854, Sens=0.9874, Spec=0.9853, F1=0.9864 | TN=470 FP=7 FN=6 TP=471\n",
      "\n",
      "=== Mean ± Std over 5 folds ===\n",
      "Accuracy    : 0.9881 ± 0.0030\n",
      "Precision   : 0.9859 ± 0.0068\n",
      "Sensitivity : 0.9904 ± 0.0045\n",
      "Specificity : 0.9857 ± 0.0069\n",
      "F1-score    : 0.9881 ± 0.0030\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 3 (training): 5-fold CV with per-fold confusion matrix + metrics =====\n",
    "\n",
    "EPOCHS = 200                                                                   # Upper bound; ES will cut earlier\n",
    "BATCH_SIZE = 32                                                                # Batch size\n",
    "\n",
    "per_fold_metrics = []                                                          # List to store per-fold metric dicts\n",
    "histories = []                                                                 # Store Keras history dicts per fold\n",
    "\n",
    "fold = 0                                                                       # Initialize fold counter\n",
    "for train_idx, val_idx in skf.split(X, y):                                     # Iterate stratified folds\n",
    "    fold += 1                                                                  # Increment fold number\n",
    "\n",
    "    print(\"\\n\" + \"=\"*90)                                                       # Visual separator\n",
    "    print(f\"Fold {fold} — Train/Val split\")                                    # Announce fold\n",
    "    print(\"Train idx:\", train_idx.shape, \"Val idx:\", val_idx.shape)            # Show split sizes\n",
    "\n",
    "    X_train, X_val = X[train_idx], X[val_idx]                                  # Slice EEG training/val data\n",
    "    y_train, y_val = y[train_idx], y[val_idx]                                  # Slice labels training/val\n",
    "\n",
    "    print(\"X_train:\", X_train.shape, \"| y_train:\", y_train.shape)              # Shapes\n",
    "    print(\"X_val  :\", X_val.shape,   \"| y_val  :\", y_val.shape)                # Shapes\n",
    "\n",
    "    # ----- Apply per-fold normalization (uses train-only stats) -----\n",
    "    X_train, X_val = normalize_per_fold(X_train, X_val)                        # Normalize per electrode\n",
    "\n",
    "    # ✅ Quick sanity check: after normalization, TRAIN should be ~mean=0, std=~1\n",
    "    train_global_mean = float(np.mean(X_train))\n",
    "    train_global_std  = float(np.std(X_train))\n",
    "    val_global_mean   = float(np.mean(X_val))\n",
    "    val_global_std    = float(np.std(X_val))\n",
    "    train_per_elec_std = np.std(X_train, axis=(0, 2, 3))  # shape: (14,)\n",
    "\n",
    "    print(\n",
    "        \"Normalization check — \"\n",
    "        f\"Train(mean,std)=({train_global_mean:.4f}, {train_global_std:.4f}) | \"\n",
    "        f\"Val(mean,std)=({val_global_mean:.4f}, {val_global_std:.4f}) | \"\n",
    "        f\"Train per-electrode std range: [{train_per_elec_std.min():.3f}, {train_per_elec_std.max():.3f}]\"\n",
    "    )\n",
    "\n",
    "    model = compile_model()                                                    # Build + compile model\n",
    "    callbacks = make_callbacks(fold)                                           # Create callbacks for this fold\n",
    "\n",
    "    print(\"Training with EarlyStopping...\")                                    # Status\n",
    "    history = model.fit(                                                       # Fit model\n",
    "        X_train, y_train,                                                      # Train data\n",
    "        validation_data=(X_val, y_val),                                        # Validation data\n",
    "        epochs=EPOCHS,                                                         # Max epochs\n",
    "        batch_size=BATCH_SIZE,                                                 # Batch size\n",
    "        callbacks=callbacks,                                                   # Callbacks list\n",
    "        verbose=2                                                              # Per-epoch logs\n",
    "    )\n",
    "\n",
    "    histories.append(history.history)                                          # Store training history dict\n",
    "\n",
    "    print(\"Predicting on validation fold...\")                                  # Status\n",
    "    y_prob = model.predict(X_val, batch_size=BATCH_SIZE).ravel()               # Predict probabilities\n",
    "    y_pred = (y_prob >= 0.5).astype(int)                                       # Threshold at 0.5 to get classes\n",
    "\n",
    "    # ----- Confusion Matrix & Per-fold Metrics -----\n",
    "    cm = confusion_matrix(y_val, y_pred, labels=[0, 1])                        # Confusion matrix\n",
    "    tn, fp, fn, tp = cm.ravel()                                                # Unpack counts\n",
    "\n",
    "    print(\"\\nFold\", fold, \"Confusion Matrix (rows=true, cols=pred) [0=Amateur, 1=Professional]:\")\n",
    "    print(cm)                                                                  # Print matrix\n",
    "    print(f\"TN={tn}, FP={fp}, FN={fn}, TP={tp}\")                               # Print counts\n",
    "\n",
    "    acc  = accuracy_score(y_val, y_pred)                                       # Accuracy\n",
    "    prec = precision_score(y_val, y_pred, pos_label=1, zero_division=0)        # Precision (class 1)\n",
    "    sens = recall_score(y_val, y_pred,    pos_label=1, zero_division=0)        # Sensitivity/Recall (class 1)\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0                            # Specificity (class 0)\n",
    "    f1   = f1_score(y_val, y_pred, pos_label=1, zero_division=0)               # F1-score (class 1)\n",
    "\n",
    "    print(\"\\nFold\", fold, \"Metrics (threshold=0.5):\")                          # Header\n",
    "    print(f\"Accuracy    : {acc:.4f}\")                                          # Accuracy\n",
    "    print(f\"Precision   : {prec:.4f}  (class 1: Professional)\")                # Precision\n",
    "    print(f\"Sensitivity : {sens:.4f}     (Recall for class 1)\")                # Sensitivity/Recall\n",
    "    print(f\"Specificity : {spec:.4f} (for class 0: Amateur)\")                  # Specificity\n",
    "    print(f\"F1-score    : {f1:.4f}\")                                           # F1\n",
    "\n",
    "    per_fold_metrics.append({                                                  # Save per-fold metrics\n",
    "        \"fold\": fold, \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n",
    "        \"accuracy\": float(acc), \"precision\": float(prec),\n",
    "        \"sensitivity\": float(sens), \"specificity\": float(spec), \"f1\": float(f1)\n",
    "    })\n",
    "\n",
    "# ----- Per-fold summary printout -----\n",
    "print(\"\\n=== Per-Fold Metrics Summary ===\")                                     # Header\n",
    "for m in per_fold_metrics:                                                      # Iterate per-fold metrics\n",
    "    print(f\"Fold {m['fold']}: \"\n",
    "          f\"Acc={m['accuracy']:.4f}, \"\n",
    "          f\"Prec={m['precision']:.4f}, \"\n",
    "          f\"Sens={m['sensitivity']:.4f}, \"\n",
    "          f\"Spec={m['specificity']:.4f}, \"\n",
    "          f\"F1={m['f1']:.4f} | \"\n",
    "          f\"TN={m['tn']} FP={m['fp']} FN={m['fn']} TP={m['tp']}\")               # Pretty per-fold line\n",
    "\n",
    "# ----- Optional overall mean ± std across folds -----\n",
    "accs  = [m[\"accuracy\"]    for m in per_fold_metrics]                            # Collect accuracies\n",
    "precs = [m[\"precision\"]   for m in per_fold_metrics]                            # Collect precisions\n",
    "sens  = [m[\"sensitivity\"] for m in per_fold_metrics]                            # Collect sensitivities\n",
    "specs = [m[\"specificity\"] for m in per_fold_metrics]                            # Collect specificities\n",
    "f1s   = [m[\"f1\"]          for m in per_fold_metrics]                            # Collect F1 scores\n",
    "\n",
    "print(\"\\n=== Mean ± Std over 5 folds ===\")                                      # Header\n",
    "print(f\"Accuracy    : {np.mean(accs):.4f} ± {np.std(accs):.4f}\")                # Mean±Std accuracy\n",
    "print(f\"Precision   : {np.mean(precs):.4f} ± {np.std(precs):.4f}\")              # Mean±Std precision\n",
    "print(f\"Sensitivity : {np.mean(sens):.4f} ± {np.std(sens):.4f}\")                # Mean±Std sensitivity\n",
    "print(f\"Specificity : {np.mean(specs):.4f} ± {np.std(specs):.4f}\")              # Mean±Std specificity\n",
    "print(f\"F1-score    : {np.mean(f1s):.4f} ± {np.std(f1s):.4f}\")                  # Mean±Std F1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87abc00d-73b3-4d50-b24b-97115e3678cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
